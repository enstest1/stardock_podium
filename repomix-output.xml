This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.claude/settings.local.json
audio_pipeline.py
book_style_analysis.py
cli_entrypoint.py
episode_memory.py
episode_metadata.py
epub_processor.py
main.py
mem0_client.py
quality_checker.py
README.md
reference_memory_sync.py
requirements.txt
script_editor.py
SETUP_GUIDE.md
story_structure.py
tests/test_story_structure.py
voice_registry.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "mcp__context7__resolve-library-id",
      "mcp__context7__get-library-docs",
      "Bash(mkdir:*)"
    ],
    "deny": []
  }
}
</file>

<file path="audio_pipeline.py">
#!/usr/bin/env python
"""
Audio Pipeline Module for Stardock Podium.

This module handles the audio generation, processing, and assembly for podcast
episodes, including voice synthesis, sound effects, and mixing.
"""

import os
import json
import logging
import time
import uuid
import tempfile
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, BinaryIO, Tuple
import concurrent.futures
import asyncio
from dataclasses import dataclass

# Try to import required libraries
try:
    from elevenlabs import ElevenLabs, VoiceSettings
    from elevenlabs.client import ElevenLabs as ElevenLabsClient
except ImportError:
    logging.error("ElevenLabs not found. Please install it with: pip install elevenlabs")
    raise

try:
    import ffmpeg
except ImportError:
    logging.error("ffmpeg-python not found. Please install it with: pip install ffmpeg-python")
    raise

# Local imports
from script_editor import load_episode_script
from voice_registry import get_voice_registry, get_voice, map_characters_to_voices
from story_structure import get_episode

# Setup logging
logger = logging.getLogger(__name__)

@dataclass
class AudioClip:
    """Represents an audio clip with metadata."""
    path: str
    type: str
    duration: float
    start_time: float = 0.0
    character: Optional[str] = None
    line_index: Optional[int] = None
    scene_index: Optional[int] = None
    volume: float = 1.0

class AudioPipeline:
    """Audio generation and processing pipeline for podcast episodes."""
    
    def __init__(self, episodes_dir: str = "episodes", assets_dir: str = "assets"):
        """Initialize the audio pipeline.
        
        Args:
            episodes_dir: Directory for episode data
            assets_dir: Directory for audio assets
        """
        self.episodes_dir = Path(episodes_dir)
        self.assets_dir = Path(assets_dir)
        
        # Create asset directories if they don't exist
        self.sound_effects_dir = self.assets_dir / "sound_effects"
        self.music_dir = self.assets_dir / "music"
        self.ambience_dir = self.assets_dir / "ambience"
        
        for directory in [self.sound_effects_dir, self.music_dir, self.ambience_dir]:
            directory.mkdir(exist_ok=True, parents=True)
        
        # Initialize voice registry
        self.voice_registry = get_voice_registry()
        
        # Initialize ElevenLabs API
        self.api_key = os.environ.get("ELEVENLABS_API_KEY")
        if self.api_key:
            self.elevenlabs = ElevenLabs(api_key=self.api_key)
            self.client = ElevenLabsClient(api_key=self.api_key)
        else:
            logger.warning("ELEVENLABS_API_KEY not found in environment variables")
            self.elevenlabs = None
            self.client = None
    
    def generate_episode_audio(self, episode_id: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """Generate audio for a complete episode.
        
        Args:
            episode_id: ID of the episode
            options: Audio generation options
        
        Returns:
            Dictionary with generation results
        """
        # Get episode data
        episode = get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {"error": f"Episode not found: {episode_id}"}
        
        # Get script
        script = load_episode_script(episode_id)
        if not script:
            logger.error(f"Script not found for episode: {episode_id}")
            return {"error": f"Script not found for episode: {episode_id}"}
        
        # Set up audio directory
        episode_dir = self.episodes_dir / episode_id
        audio_dir = episode_dir / "audio"
        audio_dir.mkdir(exist_ok=True)
        
        # Clean up previous audio if present
        if options.get("clean", False):
            for file in audio_dir.glob("*"):
                file.unlink()
        
        # Map characters to voices
        character_voices = map_characters_to_voices(episode.get('characters', []))
        
        # Store voice mapping in the audio directory for reference
        with open(audio_dir / "voice_mapping.json", 'w') as f:
            json.dump(character_voices, f, indent=2)
        
        try:
            # Process each scene
            scene_results = []
            
            scenes = script.get('scenes', [])
            
            # Use concurrent processing to generate audio for all scenes
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                future_to_scene = {
                    executor.submit(self.generate_scene_audio, 
                                  scene, i, character_voices, episode_id, audio_dir): i 
                    for i, scene in enumerate(scenes)
                }
                
                for future in concurrent.futures.as_completed(future_to_scene):
                    scene_idx = future_to_scene[future]
                    try:
                        result = future.result()
                        scene_results.append({
                            "scene_index": scene_idx,
                            "scene_number": scenes[scene_idx].get('scene_number', scene_idx + 1),
                            "success": result.get("success", False),
                            "audio_file": result.get("audio_file"),
                            "duration": result.get("duration", 0)
                        })
                    except Exception as e:
                        logger.error(f"Error generating audio for scene {scene_idx}: {e}")
                        scene_results.append({
                            "scene_index": scene_idx,
                            "scene_number": scenes[scene_idx].get('scene_number', scene_idx + 1),
                            "success": False,
                            "error": str(e)
                        })
            
            # Sort scene results by scene index
            scene_results.sort(key=lambda r: r.get("scene_index", 0))
            
            # Add intro and outro music
            intro_file = self._add_intro_music(episode_id, audio_dir)
            outro_file = self._add_outro_music(episode_id, audio_dir)
            
            # Assemble full episode
            episode_file = self._assemble_episode(
                episode_id, 
                scene_results, 
                intro_file, 
                outro_file, 
                audio_dir
            )
            
            # Generate file with generation metadata
            generation_meta = {
                "generated_at": time.time(),
                "episode_id": episode_id,
                "title": episode.get('title', 'Unknown'),
                "scenes_generated": len(scene_results),
                "scenes_successful": sum(1 for r in scene_results if r.get("success", False)),
                "total_duration": sum(r.get("duration", 0) for r in scene_results),
                "full_episode_file": str(episode_file) if episode_file else None
            }
            
            meta_file = audio_dir / "generation_metadata.json"
            with open(meta_file, 'w') as f:
                json.dump(generation_meta, f, indent=2)
            
            # Update episode with audio info
            episode['audio'] = {
                "generated_at": generation_meta["generated_at"],
                "duration": generation_meta["total_duration"],
                "file_path": str(episode_file) if episode_file else None
            }
            
            with open(episode_dir / "structure.json", 'w') as f:
                json.dump(episode, f, indent=2)
            
            return generation_meta
        
        except Exception as e:
            logger.exception(f"Error generating episode audio: {e}")
            return {"error": f"Error generating episode audio: {str(e)}"}
    
    def generate_scene_audio(self, scene: Dict[str, Any], scene_index: int,
                           character_voices: Dict[str, str], episode_id: str,
                           audio_dir: Path) -> Dict[str, Any]:
        """Generate audio for a single scene.
        
        Args:
            scene: Scene data
            scene_index: Index of the scene
            character_voices: Mapping of character names to voice IDs
            episode_id: ID of the episode
            audio_dir: Directory for audio output
        
        Returns:
            Dictionary with scene audio results
        """
        # Create scene directory
        scene_dir = audio_dir / f"scene_{scene_index:02d}"
        scene_dir.mkdir(exist_ok=True)
        
        # Create temp directory for line audio
        temp_dir = scene_dir / "temp"
        temp_dir.mkdir(exist_ok=True)
        
        # Process each line in the scene
        line_clips = []
        
        try:
            for i, line in enumerate(scene.get('lines', [])):
                clip = self._process_line(line, i, scene_dir, temp_dir, character_voices)
                if clip:
                    line_clips.append(clip)
            
            # Add scene ambience
            ambience_clip = self._add_scene_ambience(scene, scene_dir)
            
            # Mix all clips together
            output_file = scene_dir / "scene_audio.mp3"
            mixed_duration = self._mix_scene_audio(line_clips, ambience_clip, output_file)
            
            return {
                "success": True,
                "audio_file": str(output_file),
                "duration": mixed_duration
            }
        
        except Exception as e:
            logger.error(f"Error generating scene audio: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _process_line(self, line: Dict[str, Any], line_index: int, 
                     scene_dir: Path, temp_dir: Path,
                     character_voices: Dict[str, str]) -> Optional[AudioClip]:
        """Process a single line to generate audio.
        
        Args:
            line: Line data
            line_index: Index of the line
            scene_dir: Directory for scene audio
            temp_dir: Directory for temporary audio files
            character_voices: Mapping of character names to voice IDs
        
        Returns:
            AudioClip or None if processing failed
        """
        line_type = line.get('type', 'unknown')
        content = line.get('content', '')
        
        if not content:
            return None
        
        try:
            if line_type == 'dialogue':
                character = line.get('character', '')
                return self._generate_character_audio(
                    character, content, line_index, temp_dir, character_voices
                )
            
            elif line_type == 'narration':
                return self._generate_narrator_audio(content, line_index, temp_dir)
            
            elif line_type == 'sound_effect':
                return self._get_sound_effect(content, line_index, scene_dir)
            
            elif line_type == 'description':
                # No audio for descriptions unless explicitly requested
                return None
            
            else:
                logger.warning(f"Unknown line type: {line_type}")
                return None
        
        except Exception as e:
            logger.error(f"Error processing line {line_index}: {e}")
            return None
    
    def _generate_character_audio(self, character: str, content: str, 
                                line_index: int, temp_dir: Path,
                                character_voices: Dict[str, str]) -> Optional[AudioClip]:
        """Generate audio for character dialogue.
        
        Args:
            character: Character name
            content: Dialogue content
            line_index: Index of the line
            temp_dir: Directory for temporary audio files
            character_voices: Mapping of character names to voice IDs
        
        Returns:
            AudioClip or None if generation failed
        """
        if not self.elevenlabs:
            logger.error("ElevenLabs client not initialized (no API key)")
            return None
        
        # Get voice ID for character
        voice_identifier = character_voices.get(character)
        
        if not voice_identifier:
            logger.warning(f"No voice mapped for character: {character}")
            # Try to find a suitable voice
            voices = self.voice_registry.list_voices()
            if voices:
                # Use first available voice as fallback
                voice_identifier = voices[0].get('voice_registry_id')
                logger.info(f"Using fallback voice for {character}: {voice_identifier}")
            else:
                logger.error(f"No voices available for character: {character}")
                return None
        
        try:
            # Clean text to avoid synthesis issues
            content = content.replace('"', '').replace('...', '…')
            
            # Generate audio file name
            audio_file = temp_dir / f"line_{line_index:03d}_{character.lower().replace(' ', '_')}.mp3"
            
            # Generate speech
            audio_data = self.voice_registry.generate_speech(
                text=content,
                voice_identifier=voice_identifier,
                output_path=str(audio_file)
            )
            
            # Get audio duration using ffmpeg
            probe = ffmpeg.probe(str(audio_file))
            duration = float(probe['format']['duration'])
            
            return AudioClip(
                path=str(audio_file),
                type='dialogue',
                duration=duration,
                character=character,
                line_index=line_index
            )
        
        except Exception as e:
            logger.error(f"Error generating character audio for {character}: {e}")
            return None
    
    def _generate_narrator_audio(self, content: str, line_index: int, 
                               temp_dir: Path) -> Optional[AudioClip]:
        """Generate audio for narrator lines.
        
        Args:
            content: Narration content
            line_index: Index of the line
            temp_dir: Directory for temporary audio files
        
        Returns:
            AudioClip or None if generation failed
        """
        if not self.elevenlabs:
            logger.error("ElevenLabs client not initialized (no API key)")
            return None
        
        try:
            # Generate audio file name
            audio_file = temp_dir / f"line_{line_index:03d}_narrator.mp3"
            
            # Try to get narrator voice
            voice_id = None
            narrator_voices = self.voice_registry.find_voices_by_description("narrator deep authoritative", limit=1)
            
            if narrator_voices:
                voice_id = narrator_voices[0].get('voice_registry_id')
            else:
                # Fallback to any available voice
                voices = self.voice_registry.list_voices()
                if voices:
                    voice_id = voices[0].get('voice_registry_id')
            
            if not voice_id:
                logger.error("No voice available for narrator")
                return None
            
            # Generate speech
            audio_data = self.voice_registry.generate_speech(
                text=content,
                voice_identifier=voice_id,
                output_path=str(audio_file)
            )
            
            # Get audio duration using ffmpeg
            probe = ffmpeg.probe(str(audio_file))
            duration = float(probe['format']['duration'])
            
            return AudioClip(
                path=str(audio_file),
                type='narration',
                duration=duration,
                line_index=line_index
            )
        
        except Exception as e:
            logger.error(f"Error generating narrator audio: {e}")
            return None
    
    def _get_sound_effect(self, description: str, line_index: int, 
                        scene_dir: Path) -> Optional[AudioClip]:
        """Find or generate a sound effect based on description.
        
        Args:
            description: Sound effect description
            line_index: Index of the line
            scene_dir: Directory for scene audio
        
        Returns:
            AudioClip or None if not found
        """
        # Clean description to create a search key
        search_key = description.lower().replace(' ', '_').replace('.', '').replace(',', '')
        
        # Look for matching sound effect in assets
        for ext in ['mp3', 'wav']:
            matches = list(self.sound_effects_dir.glob(f"*{search_key}*.{ext}"))
            if matches:
                # Use first match
                effect_file = matches[0]
                
                try:
                    # Get audio duration using ffmpeg
                    probe = ffmpeg.probe(str(effect_file))
                    duration = float(probe['format']['duration'])
                    
                    # Copy to scene directory
                    dest_file = scene_dir / f"sfx_{line_index:03d}.{ext}"
                    with open(effect_file, 'rb') as src:
                        with open(dest_file, 'wb') as dst:
                            dst.write(src.read())
                    
                    return AudioClip(
                        path=str(dest_file),
                        type='sound_effect',
                        duration=duration,
                        line_index=line_index,
                        volume=1.2  # Slightly louder than dialogue
                    )
                
                except Exception as e:
                    logger.error(f"Error processing sound effect: {e}")
            
        # If no matching sound effect found
        logger.warning(f"No sound effect found for: {description}")
        
        # TODO: Implement synthesized sound effect option when no match found
        
        return None
    
    def _add_scene_ambience(self, scene: Dict[str, Any], scene_dir: Path) -> Optional[AudioClip]:
        """Add ambient sound for the scene.
        
        Args:
            scene: Scene data
            scene_dir: Directory for scene audio
        
        Returns:
            AudioClip or None if no ambience added
        """
        setting = scene.get('setting', '').lower()
        atmosphere = scene.get('atmosphere', '').lower()
        
        # Setting-based ambience keywords
        ambience_mapping = {
            'bridge': ['bridge', 'starship_bridge', 'command_center'],
            'space': ['space', 'vacuum', 'stars'],
            'planet': ['planet', 'alien_world', 'nature'],
            'engine room': ['engine_room', 'machinery', 'warp_core'],
            'medical': ['sickbay', 'medical', 'hospital'],
            'corridor': ['corridor', 'hallway', 'footsteps'],
            'quarters': ['quarters', 'room', 'living_space'],
            'shuttlecraft': ['shuttle', 'small_ship', 'cockpit'],
            'transporter': ['transporter', 'teleport', 'energy'],
            'battle': ['battle', 'combat', 'weapons'],
            'forest': ['forest', 'woods', 'nature'],
            'city': ['city', 'urban', 'crowd'],
            'underwater': ['underwater', 'ocean', 'bubbles']
        }
        
        # Choose keywords based on setting
        keywords = []
        for key, values in ambience_mapping.items():
            if any(term in setting for term in key.split()):
                keywords.extend(values)
                break
        
        # Add atmosphere-based keywords
        if 'tense' in atmosphere or 'danger' in atmosphere:
            keywords.append('tension')
        elif 'quiet' in atmosphere or 'calm' in atmosphere:
            keywords.append('quiet')
        elif 'busy' in atmosphere or 'active' in atmosphere:
            keywords.append('activity')
        
        # No keywords found
        if not keywords:
            keywords = ['background', 'ambience']
        
        # Look for matching ambience in assets
        for keyword in keywords:
            matches = list(self.ambience_dir.glob(f"*{keyword}*.mp3")) + list(self.ambience_dir.glob(f"*{keyword}*.wav"))
            if matches:
                # Use first match
                ambience_file = matches[0]
                
                try:
                    # Get audio duration using ffmpeg
                    probe = ffmpeg.probe(str(ambience_file))
                    duration = float(probe['format']['duration'])
                    
                    # Copy to scene directory
                    dest_file = scene_dir / f"ambience.{ambience_file.suffix}"
                    with open(ambience_file, 'rb') as src:
                        with open(dest_file, 'wb') as dst:
                            dst.write(src.read())
                    
                    return AudioClip(
                        path=str(dest_file),
                        type='ambience',
                        duration=duration,
                        volume=0.3  # Lower volume for background
                    )
                
                except Exception as e:
                    logger.error(f"Error processing ambience: {e}")
        
        # If no matching ambience found
        logger.warning(f"No ambience found for setting: {setting}")
        return None
    
    def _mix_scene_audio(self, line_clips: List[AudioClip], 
                        ambience_clip: Optional[AudioClip],
                        output_file: Path) -> float:
        """Mix scene audio clips together.
        
        Args:
            line_clips: List of line audio clips
            ambience_clip: Optional ambience audio clip
            output_file: Output file path
        
        Returns:
            Duration of the mixed audio
        """
        if not line_clips:
            logger.warning("No audio clips to mix")
            return 0.0
        
        # Sort clips by line index
        line_clips.sort(key=lambda c: c.line_index if c.line_index is not None else 999)
        
        # Initialize ffmpeg input streams
        inputs = []
        
        # Calculate total duration based on line clips
        total_duration = sum(clip.duration for clip in line_clips) + 1.0  # Add 1 second padding
        
        # Add silence between clips
        silence_duration = 0.5  # Half-second silence between lines
        
        try:
            # Create a silence file for padding
            silence_file = output_file.parent / "silence.mp3"
            input_args = {
                'f': 'lavfi',
                'i': 'anullsrc=r=44100:cl=stereo',
                't': str(silence_duration),
                'c:a': 'libmp3lame',
                'b:a': '128k'
            }
            (
                ffmpeg
                .input(**input_args)
                .output(str(silence_file))
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            # Build concatenation file list
            concat_file = output_file.parent / "concat.txt"
            with open(concat_file, 'w') as f:
                # Add each clip followed by silence
                for clip in line_clips:
                    f.write(f"file '{clip.path}'\n")
                    f.write(f"file '{silence_file}'\n")
            
            # Concatenate clips with silence between
            dialogue_file = output_file.parent / "dialogue.mp3"
            (
                ffmpeg
                .input(str(concat_file), format='concat', safe=0)
                .output(str(dialogue_file), c='copy')
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            # If we have ambience, mix it with the dialogue
            if ambience_clip:
                # If ambience is shorter than total duration, loop it
                if ambience_clip.duration < total_duration:
                    looped_ambience = output_file.parent / "looped_ambience.mp3"
                    loop_count = int(total_duration / ambience_clip.duration) + 1
                    
                    # Create concat file for looping
                    loop_concat = output_file.parent / "loop_concat.txt"
                    with open(loop_concat, 'w') as f:
                        for _ in range(loop_count):
                            f.write(f"file '{ambience_clip.path}'\n")
                    
                    # Generate looped ambience
                    (
                        ffmpeg
                        .input(str(loop_concat), format='concat', safe=0)
                        .output(str(looped_ambience), c='copy', t=str(total_duration))
                        .overwrite_output()
                        .global_args('-loglevel', 'error')
                        .run()
                    )
                    
                    # Mix dialogue and looped ambience
                    (
                        ffmpeg
                        .input(str(dialogue_file))
                        .input(str(looped_ambience))
                        .filter_complex(f'[0:a][1:a]amix=inputs=2:duration=first:weights={1}\\\ {ambience_clip.volume}')
                        .output(str(output_file), ar=44100)
                        .overwrite_output()
                        .global_args('-loglevel', 'error')
                        .run()
                    )
                else:
                    # Mix dialogue and ambience directly
                    (
                        ffmpeg
                        .input(str(dialogue_file))
                        .input(str(ambience_clip.path))
                        .filter_complex(f'[0:a][1:a]amix=inputs=2:duration=first:weights={1}\\\ {ambience_clip.volume}')
                        .output(str(output_file), ar=44100)
                        .overwrite_output()
                        .global_args('-loglevel', 'error')
                        .run()
                    )
            else:
                # Just use the dialogue file as output
                (
                    ffmpeg
                    .input(str(dialogue_file))
                    .output(str(output_file), ar=44100)
                    .overwrite_output()
                    .global_args('-loglevel', 'error')
                    .run()
                )
            
            # Get final output duration
            probe = ffmpeg.probe(str(output_file))
            final_duration = float(probe['format']['duration'])
            
            return final_duration
        
        except Exception as e:
            logger.error(f"Error mixing scene audio: {e}")
            return 0.0
    
    def _add_intro_music(self, episode_id: str, audio_dir: Path) -> Optional[Path]:
        """Add intro music for the episode.
        
        Args:
            episode_id: ID of the episode
            audio_dir: Directory for audio output
        
        Returns:
            Path to the intro music file or None if failed
        """
        # Look for sci-fi intro music
        intro_matches = list(self.music_dir.glob("*intro*.mp3")) + list(self.music_dir.glob("*opening*.mp3"))
        
        if not intro_matches:
            logger.warning("No intro music found")
            return None
        
        # Use first match
        intro_file = intro_matches[0]
        
        try:
            # Copy to audio directory
            dest_file = audio_dir / "intro.mp3"
            with open(intro_file, 'rb') as src:
                with open(dest_file, 'wb') as dst:
                    dst.write(src.read())
            
            # Trim to reasonable length (15 seconds)
            trimmed_file = audio_dir / "intro_trimmed.mp3"
            (
                ffmpeg
                .input(str(dest_file))
                .output(str(trimmed_file), t='15')
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            # Add fade-out
            final_intro = audio_dir / "intro_final.mp3"
            (
                ffmpeg
                .input(str(trimmed_file))
                .filter_('afade', t='out', st='12', d='3')
                .output(str(final_intro))
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            return final_intro
        
        except Exception as e:
            logger.error(f"Error processing intro music: {e}")
            return None
    
    def _add_outro_music(self, episode_id: str, audio_dir: Path) -> Optional[Path]:
        """Add outro music for the episode.
        
        Args:
            episode_id: ID of the episode
            audio_dir: Directory for audio output
        
        Returns:
            Path to the outro music file or None if failed
        """
        # Look for sci-fi outro music
        outro_matches = list(self.music_dir.glob("*outro*.mp3")) + list(self.music_dir.glob("*closing*.mp3"))
        
        if not outro_matches:
            logger.warning("No outro music found")
            return None
        
        # Use first match
        outro_file = outro_matches[0]
        
        try:
            # Copy to audio directory
            dest_file = audio_dir / "outro.mp3"
            with open(outro_file, 'rb') as src:
                with open(dest_file, 'wb') as dst:
                    dst.write(src.read())
            
            # Trim to reasonable length (10 seconds)
            trimmed_file = audio_dir / "outro_trimmed.mp3"
            (
                ffmpeg
                .input(str(dest_file))
                .output(str(trimmed_file), t='10')
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            # Add fade-in
            final_outro = audio_dir / "outro_final.mp3"
            (
                ffmpeg
                .input(str(trimmed_file))
                .filter_('afade', t='in', st='0', d='2')
                .output(str(final_outro))
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            return final_outro
        
        except Exception as e:
            logger.error(f"Error processing outro music: {e}")
            return None
    
    def _assemble_episode(self, episode_id: str, scene_results: List[Dict[str, Any]],
                         intro_file: Optional[Path], outro_file: Optional[Path],
                         audio_dir: Path) -> Optional[Path]:
        """Assemble the full episode audio from scene audio files.
        
        Args:
            episode_id: ID of the episode
            scene_results: List of scene audio generation results
            intro_file: Optional intro music file
            outro_file: Optional outro music file
            audio_dir: Directory for audio output
        
        Returns:
            Path to the full episode audio file or None if failed
        """
        # Get all scene audio files
        valid_scenes = [s for s in scene_results if s.get("success", False) and s.get("audio_file")]
        
        if not valid_scenes:
            logger.error("No valid scene audio files to assemble")
            return None
        
        try:
            # Create concatenation file
            concat_file = audio_dir / "episode_concat.txt"
            with open(concat_file, 'w') as f:
                # Add intro if available
                if intro_file and intro_file.exists():
                    f.write(f"file '{intro_file}'\n")
                
                # Add each scene in order
                for scene in sorted(valid_scenes, key=lambda s: s.get("scene_index", 0)):
                    f.write(f"file '{scene['audio_file']}'\n")
                
                # Add outro if available
                if outro_file and outro_file.exists():
                    f.write(f"file '{outro_file}'\n")
            
            # Concatenate all files
            output_file = audio_dir / "full_episode.mp3"
            (
                ffmpeg
                .input(str(concat_file), format='concat', safe=0)
                .output(str(output_file), c='copy')
                .overwrite_output()
                .global_args('-loglevel', 'error')
                .run()
            )
            
            # Add metadata to the file
            try:
                episode = get_episode(episode_id)
                if episode:
                    title = episode.get('title', f"Episode {episode.get('episode_number', 'Unknown')}")
                    series = episode.get('series', 'Main Series')
                    
                    (
                        ffmpeg
                        .input(str(output_file))
                        .output(
                            str(output_file) + ".temp.mp3",
                            **{
                                'metadata:g:0': f"title={title}",
                                'metadata:g:1': f"album={series}",
                                'metadata:g:2': f"artist=Stardock Podium AI",
                                'metadata:g:3': f"comment=Generated by Stardock Podium"
                            }
                        )
                        .overwrite_output()
                        .global_args('-loglevel', 'error')
                        .run()
                    )
                    
                    # Replace original file
                    os.replace(str(output_file) + ".temp.mp3", str(output_file))
            except Exception as e:
                logger.error(f"Error adding metadata to episode: {e}")
            
            return output_file
        
        except Exception as e:
            logger.error(f"Error assembling episode audio: {e}")
            return None
    
    def generate_single_audio(self, text: str, voice_identifier: str, 
                            output_file: Optional[str] = None) -> Tuple[bytes, float]:
        """Generate audio for a single text passage.
        
        Args:
            text: Text to convert to speech
            voice_identifier: Voice registry ID or character name
            output_file: Optional path to save the audio file
        
        Returns:
            Tuple of (audio data, duration)
        """
        if not self.elevenlabs:
            raise RuntimeError("ElevenLabs client not initialized (no API key)")
        
        # Generate speech
        audio_data = self.voice_registry.generate_speech(
            text=text,
            voice_identifier=voice_identifier,
            output_path=output_file
        )
        
        # Get duration
        if output_file:
            probe = ffmpeg.probe(output_file)
            duration = float(probe['format']['duration'])
        else:
            # Write to a temporary file to get duration
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
                tmp_path = tmp.name
                tmp.write(audio_data)
            
            probe = ffmpeg.probe(tmp_path)
            duration = float(probe['format']['duration'])
            
            # Clean up temporary file
            os.unlink(tmp_path)
        
        return audio_data, duration

# Singleton instance
_audio_pipeline = None

def get_audio_pipeline() -> AudioPipeline:
    """Get the AudioPipeline singleton instance."""
    global _audio_pipeline
    
    if _audio_pipeline is None:
        _audio_pipeline = AudioPipeline()
    
    return _audio_pipeline

def generate_episode_audio(episode_id: str, options: Dict[str, Any] = None) -> Dict[str, Any]:
    """Generate audio for a complete episode.
    
    Args:
        episode_id: ID of the episode
        options: Audio generation options
    
    Returns:
        Dictionary with generation results
    """
    if options is None:
        options = {}
    
    pipeline = get_audio_pipeline()
    return pipeline.generate_episode_audio(episode_id, options)

def generate_audio(text: str, voice_identifier: str, 
                  output_file: Optional[str] = None) -> Tuple[bytes, float]:
    """Generate audio for a text passage.
    
    Args:
        text: Text to convert to speech
        voice_identifier: Voice registry ID or character name
        output_file: Optional path to save the audio file
    
    Returns:
        Tuple of (audio data, duration)
    """
    pipeline = get_audio_pipeline()
    return pipeline.generate_single_audio(text, voice_identifier, output_file)
</file>

<file path="book_style_analysis.py">
#!/usr/bin/env python
"""
Book Style Analysis Module for Stardock Podium.

This module analyzes the style and content of ingested books to extract
writing patterns, character archetypes, dialogue styles, and thematic elements.
The results are used to guide the podcast's storytelling style and content.
"""

import os
import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from collections import Counter, defaultdict
import random
import string
import math

# Try to import required libraries
try:
    from nltk.tokenize import sent_tokenize, word_tokenize
    from nltk.corpus import stopwords
    import nltk
    
    # Ensure necessary NLTK data is available
    for resource in ['punkt', 'stopwords']:
        try:
            nltk.data.find(f'tokenizers/{resource}')
        except LookupError:
            nltk.download(resource, quiet=True)
except ImportError:
    logging.error("NLTK not found. Please install it with: pip install nltk")
    raise

# Local imports
from epub_processor import get_processor, get_book_content

# Setup logging
logger = logging.getLogger(__name__)

class BookStyleAnalyzer:
    """Analyzer for book style and content to extract patterns and insights."""
    
    def __init__(self, analysis_dir: str = "analysis"):
        """Initialize the book style analyzer.
        
        Args:
            analysis_dir: Directory to store analysis results
        """
        self.analysis_dir = Path(analysis_dir)
        self.analysis_dir.mkdir(exist_ok=True)
        
        # Get EPUB processor
        self.epub_processor = get_processor()
        
        # Load stopwords for analysis
        self.stopwords = set(stopwords.words('english'))
        
        # Define patterns
        self.dialogue_pattern = re.compile(r'"([^"]*)"')
        self.thought_pattern = re.compile(r'\'([^\']*)\'')
        self.sentence_end_pattern = re.compile(r'[.!?][\s")]')
    
    def analyze_book_style(self, book_id: str, deep: bool = False) -> Dict[str, Any]:
        """Analyze the style and content of a book.
        
        Args:
            book_id: ID of the book to analyze
            deep: Whether to perform a deep analysis
        
        Returns:
            Dictionary with analysis results
        """
        logger.info(f"Starting style analysis for book {book_id}")
        
        # Check if analysis already exists
        analysis_file = self.analysis_dir / f"{book_id}_style_analysis.json"
        if analysis_file.exists() and not deep:
            try:
                with open(analysis_file, 'r') as f:
                    existing_analysis = json.load(f)
                    logger.info(f"Loaded existing analysis for book {book_id}")
                    return existing_analysis
            except Exception as e:
                logger.error(f"Error loading existing analysis: {e}")
        
        # Get book content
        book_content = get_book_content(book_id)
        
        if not book_content or not book_content.get('metadata'):
            logger.error(f"Book content not found for ID: {book_id}")
            return {}
        
        # Extract metadata
        metadata = book_content['metadata']
        title = metadata.get('title', 'Unknown')
        author = metadata.get('creator', 'Unknown')
        
        logger.info(f"Analyzing book: '{title}' by {author}")
        
        # Initialize analysis result structure
        analysis = {
            "book_id": book_id,
            "title": title,
            "author": author,
            "analyzed_at": time.time(),
            "deep_analysis": deep,
            "statistics": {},
            "style": {},
            "dialogue": {},
            "characters": {},
            "themes": {},
            "settings": {},
            "plot_elements": {},
            "unique_words": {}
        }
        
        # Extract content samples
        sections = book_content.get('sections', {}).get('sections', [])
        if not sections:
            logger.error(f"No sections found for book {book_id}")
            return {}
        
        # For basic analysis, sample the sections
        if not deep:
            sections = self._sample_sections(sections)
        
        # Process the text
        text_samples = [section.get('content', '') for section in sections]
        combined_text = "\n\n".join(text_samples)
        
        # Basic text statistics
        analysis["statistics"] = self._compute_text_statistics(combined_text)
        
        # Style analysis
        analysis["style"] = self._analyze_writing_style(combined_text)
        
        # Dialogue analysis
        analysis["dialogue"] = self._analyze_dialogue(combined_text)
        
        # Character analysis
        analysis["characters"] = self._identify_characters(combined_text, book_id)
        
        # Theme analysis
        analysis["themes"] = self._identify_themes(combined_text)
        
        # Setting analysis
        analysis["settings"] = self._identify_settings(combined_text)
        
        # Plot elements
        analysis["plot_elements"] = self._identify_plot_elements(combined_text)
        
        # Vocabulary analysis
        analysis["unique_words"] = self._analyze_vocabulary(combined_text)
        
        # If deep analysis, add additional in-depth insights
        if deep:
            logger.info(f"Performing deep analysis for book {book_id}")
            
            # Add deep analysis results
            analysis["deep_insights"] = self._perform_deep_analysis(combined_text)
            
            # Add relationship analysis
            analysis["relationships"] = self._analyze_relationships(combined_text, analysis["characters"])
            
            # Add narrative arc analysis
            analysis["narrative_arc"] = self._analyze_narrative_arc(book_content)
        
        # Save analysis
        try:
            with open(analysis_file, 'w') as f:
                json.dump(analysis, f, indent=2)
            logger.info(f"Saved analysis to {analysis_file}")
        except Exception as e:
            logger.error(f"Error saving analysis: {e}")
        
        return analysis
    
    def _sample_sections(self, sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Sample sections for analysis to reduce processing time.
        
        Args:
            sections: List of all sections
        
        Returns:
            List of sampled sections
        """
        num_sections = len(sections)
        
        if num_sections <= 10:
            # For small books, use all sections
            return sections
        elif num_sections <= 30:
            # For medium books, sample beginning, middle, and end
            indices = list(range(0, 3))
            indices.extend(range(num_sections // 2 - 1, num_sections // 2 + 2))
            indices.extend(range(num_sections - 3, num_sections))
        else:
            # For large books, sample proportionally
            indices = [0, 1, 2]  # Start
            indices.extend(random.sample(range(3, num_sections // 3), 3))  # Early
            indices.extend(random.sample(range(num_sections // 3, 2 * num_sections // 3), 4))  # Middle
            indices.extend(random.sample(range(2 * num_sections // 3, num_sections - 3), 3))  # Late
            indices.extend([num_sections - 3, num_sections - 2, num_sections - 1])  # End
        
        # Get unique sorted indices within range
        unique_indices = sorted(set(i for i in indices if 0 <= i < num_sections))
        
        return [sections[i] for i in unique_indices]
    
    def _compute_text_statistics(self, text: str) -> Dict[str, Any]:
        """Compute basic text statistics.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with text statistics
        """
        statistics = {}
        
        # Tokenize
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        
        # Filter out punctuation from words
        words = [word for word in words if word.isalnum()]
        
        # Compute basic statistics
        statistics["total_words"] = len(words)
        statistics["total_sentences"] = len(sentences)
        statistics["unique_words"] = len(set(words))
        statistics["lexical_diversity"] = len(set(words)) / len(words) if words else 0
        
        # Compute sentence length statistics
        sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]
        statistics["avg_sentence_length"] = sum(sentence_lengths) / len(sentences) if sentences else 0
        statistics["min_sentence_length"] = min(sentence_lengths) if sentences else 0
        statistics["max_sentence_length"] = max(sentence_lengths) if sentences else 0
        
        # Compute word length statistics
        word_lengths = [len(word) for word in words]
        statistics["avg_word_length"] = sum(word_lengths) / len(words) if words else 0
        
        # Count paragraphs
        paragraphs = re.split(r'\n{2,}', text)
        statistics["total_paragraphs"] = len(paragraphs)
        
        # Estimate reading time (words per minute)
        statistics["estimated_reading_time_mins"] = len(words) / 250
        
        return statistics
    
    def _analyze_writing_style(self, text: str) -> Dict[str, Any]:
        """Analyze the writing style of the text.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with writing style analysis
        """
        style = {}
        
        # Tokenize
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        
        # Analyze sentence structure
        sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]
        sentence_length_variance = sum((length - sum(sentence_lengths) / len(sentences)) ** 2 
                                   for length in sentence_lengths) / len(sentences)
        
        style["sentence_variety"] = math.sqrt(sentence_length_variance)
        
        # Calculate sentence types
        statement_count = len(re.findall(r'[.]\s', text))
        question_count = len(re.findall(r'[?]\s', text))
        exclamation_count = len(re.findall(r'[!]\s', text))
        
        total_sentence_counts = statement_count + question_count + exclamation_count
        if total_sentence_counts > 0:
            style["statement_ratio"] = statement_count / total_sentence_counts
            style["question_ratio"] = question_count / total_sentence_counts
            style["exclamation_ratio"] = exclamation_count / total_sentence_counts
        
        # Analyze paragraph structure
        paragraphs = re.split(r'\n{2,}', text)
        paragraph_lengths = [len(word_tokenize(paragraph)) for paragraph in paragraphs]
        style["avg_paragraph_length"] = sum(paragraph_lengths) / len(paragraphs) if paragraphs else 0
        
        # Identify adjective usage
        # Simple approximation - exact part-of-speech tagging would be better
        adjective_endings = ['able', 'al', 'ary', 'ent', 'ful', 'ic', 'ical', 'ish', 'less', 'ous']
        adjective_candidates = [word.lower() for word in words if any(word.lower().endswith(ending) for ending in adjective_endings)]
        style["estimated_adjective_ratio"] = len(adjective_candidates) / len(words) if words else 0
        
        # Identify adverb usage
        adverb_candidates = [word.lower() for word in words if word.lower().endswith('ly')]
        style["estimated_adverb_ratio"] = len(adverb_candidates) / len(words) if words else 0
        
        # Analyze voice (active vs. passive)
        # Simple approximation - passive voice often uses "was/were" + past participle
        passive_indicators = ['was', 'were', 'been', 'being', 'be']
        passive_constructs = sum(1 for word in words if word.lower() in passive_indicators)
        style["passive_voice_indicator"] = passive_constructs / len(sentences) if sentences else 0
        
        # Analyze narrative style (estimated 1st vs 3rd person)
        first_person_indicators = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']
        third_person_indicators = ['he', 'she', 'it', 'they', 'him', 'her', 'them', 'his', 'hers', 'their']
        
        first_person_count = sum(1 for word in words if word.lower() in first_person_indicators)
        third_person_count = sum(1 for word in words if word.lower() in third_person_indicators)
        
        # Calculate total indicator count and narratives ratios
        total_indicators = first_person_count + third_person_count
        if total_indicators > 0:
            style["first_person_ratio"] = first_person_count / total_indicators
            style["third_person_ratio"] = third_person_count / total_indicators
            
            # Determine narrative style
            if style["first_person_ratio"] > 0.6:
                style["primary_narrative_style"] = "first_person"
            elif style["third_person_ratio"] > 0.6:
                style["primary_narrative_style"] = "third_person"
            else:
                style["primary_narrative_style"] = "mixed"
        
        # Analyze dialogue density
        dialogue_matches = re.findall(self.dialogue_pattern, text)
        style["dialogue_density"] = len(dialogue_matches) / len(sentences) if sentences else 0
        
        # Analyze descriptive vs. action-oriented content
        # Action words typically feature more verbs
        # This is a rough approximation
        action_verbs = ['run', 'jump', 'move', 'turn', 'walk', 'hit', 'grab', 'take',
                       'push', 'pull', 'throw', 'catch', 'drop', 'lift', 'fight',
                       'attack', 'defend', 'escape', 'chase', 'flee', 'shoot', 'kick']
        
        action_verb_count = sum(1 for word in words if word.lower() in action_verbs)
        style["action_verb_density"] = action_verb_count / len(words) if words else 0
        
        if style["action_verb_density"] > 0.02:
            style["content_type_inference"] = "action_oriented"
        elif style["estimated_adjective_ratio"] > 0.1:
            style["content_type_inference"] = "descriptive"
        else:
            style["content_type_inference"] = "balanced"
        
        return style
    
    def _analyze_dialogue(self, text: str) -> Dict[str, Any]:
        """Analyze dialogue patterns in the text.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with dialogue analysis
        """
        dialogue = {}
        
        # Extract dialogue
        dialogue_matches = re.findall(self.dialogue_pattern, text)
        
        if not dialogue_matches:
            dialogue["total_lines"] = 0
            dialogue["present"] = False
            return dialogue
        
        dialogue["total_lines"] = len(dialogue_matches)
        dialogue["present"] = True
        
        # Analyze dialogue lines
        dialogue_lengths = [len(word_tokenize(line)) for line in dialogue_matches]
        dialogue["avg_line_length"] = sum(dialogue_lengths) / len(dialogue_lengths) if dialogue_lengths else 0
        
        # Identify question frequency in dialogue
        question_dialogues = [line for line in dialogue_matches if '?' in line]
        dialogue["question_ratio"] = len(question_dialogues) / len(dialogue_matches)
        
        # Identify exclamation frequency in dialogue
        exclamation_dialogues = [line for line in dialogue_matches if '!' in line]
        dialogue["exclamation_ratio"] = len(exclamation_dialogues) / len(dialogue_matches)
        
        # Analyze dialogue tags (if available)
        dialogue_tag_pattern = re.compile(r'"[^"]*"(?:\s+)([^,.!?;:]+)(?:ed|s)(?:\s+)')
        dialogue_tags = dialogue_tag_pattern.findall(text)
        
        # Count the most common dialogue tags
        if dialogue_tags:
            common_tags = Counter(dialogue_tags).most_common(10)
            dialogue["common_tags"] = [{"tag": tag, "count": count} for tag, count in common_tags]
        
        # Identify patterns of short vs. long exchanges
        dialogue_paragraphs = re.split(r'\n{2,}', text)
        dialogue_exchanges = [para for para in dialogue_paragraphs if '"' in para]
        
        if dialogue_exchanges:
            exchange_lengths = [para.count('"') // 2 for para in dialogue_exchanges]
            dialogue["avg_exchange_length"] = sum(exchange_lengths) / len(exchange_lengths)
            
            # Count single-line vs multi-line exchanges
            single_exchanges = sum(1 for length in exchange_lengths if length == 1)
            dialogue["single_line_ratio"] = single_exchanges / len(exchange_lengths)
        
        # Sample dialogue for style reference
        if dialogue_matches:
            # Choose some representative samples
            sample_indices = random.sample(range(len(dialogue_matches)), 
                                          min(5, len(dialogue_matches)))
            dialogue["samples"] = [dialogue_matches[i] for i in sample_indices]
        
        return dialogue
    
    def _identify_characters(self, text: str, book_id: str) -> Dict[str, Any]:
        """Identify and analyze characters in the text.
        
        Args:
            text: The text to analyze
            book_id: ID of the book
        
        Returns:
            Dictionary with character analysis
        """
        characters = {}
        
        # Check for existing character analysis
        character_file = self.analysis_dir / f"{book_id}_characters.json"
        if character_file.exists():
            try:
                with open(character_file, 'r') as f:
                    existing_analysis = json.load(f)
                    logger.info(f"Loaded existing character analysis for book {book_id}")
                    return existing_analysis
            except Exception as e:
                logger.error(f"Error loading existing character analysis: {e}")
        
        # Simple name extraction
        # This is a basic approach - more sophisticated NER would be better
        sentences = sent_tokenize(text)
        
        # Look for capitalized words that aren't at the start of sentences
        name_candidates = set()
        for sentence in sentences:
            words = word_tokenize(sentence)
            if len(words) > 1:
                for i in range(1, len(words)):
                    word = words[i]
                    # Check if capitalized and not a common stop word
                    if (word and word[0].isupper() and 
                        word.lower() not in self.stopwords and
                        len(word) > 1 and  # Exclude single letters
                        word.isalpha()):  # Only alphabetic
                        name_candidates.add(word)
        
        # Try to identify main characters by frequency
        words = word_tokenize(text)
        # Exclude words at beginning of sentences
        words = [words[i] for i in range(len(words)) if i > 0 and words[i-1] in ['.', '!', '?']]
        
        # Count name occurrences
        name_counts = Counter(word for word in words if word in name_candidates)
        
        # Get the most common names
        most_common_names = name_counts.most_common(10)
        
        # Extract character mentions with surrounding context
        character_mentions = {}
        for name, _ in most_common_names:
            mentions = []
            for sentence in sentences:
                if re.search(r'\b' + re.escape(name) + r'\b', sentence):
                    mentions.append(sentence)
            
            # Only keep a sample of mentions
            if mentions:
                character_mentions[name] = random.sample(mentions, min(5, len(mentions)))
        
        # Create character profiles
        character_profiles = []
        for name, count in most_common_names:
            if count >= 3:  # Only include characters mentioned multiple times
                profile = {
                    "name": name,
                    "mention_count": count,
                    "sample_mentions": character_mentions.get(name, []),
                    "likely_importance": "high" if count > 10 else "medium" if count > 5 else "low"
                }
                character_profiles.append(profile)
        
        characters["profiles"] = character_profiles
        characters["total_characters"] = len(character_profiles)
        
        # Save character analysis
        try:
            with open(character_file, 'w') as f:
                json.dump(characters, f, indent=2)
            logger.info(f"Saved character analysis to {character_file}")
        except Exception as e:
            logger.error(f"Error saving character analysis: {e}")
        
        return characters
    
    def _identify_themes(self, text: str) -> Dict[str, Any]:
        """Identify thematic elements in the text.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with theme analysis
        """
        themes = {}
        
        # Tokenize and filter out stopwords
        words = word_tokenize(text.lower())
        filtered_words = [word for word in words if word.isalpha() and word not in self.stopwords]
        
        # Count word frequencies
        word_counts = Counter(filtered_words)
        
        # Get most common words
        common_words = word_counts.most_common(100)
        
        # Define common theme categories
        theme_categories = {
            "adventure": ['journey', 'quest', 'adventure', 'discover', 'exploration', 'mission'],
            "romance": ['love', 'heart', 'romance', 'passion', 'kiss', 'embrace'],
            "conflict": ['war', 'battle', 'fight', 'conflict', 'struggle', 'confrontation'],
            "identity": ['self', 'identity', 'discover', 'true', 'become', 'change'],
            "power": ['power', 'control', 'authority', 'command', 'rule', 'dominate'],
            "justice": ['justice', 'fair', 'right', 'law', 'punishment', 'crime'],
            "family": ['family', 'father', 'mother', 'parent', 'child', 'brother', 'sister'],
            "technology": ['machine', 'computer', 'technology', 'device', 'system', 'program'],
            "nature": ['nature', 'tree', 'river', 'mountain', 'forest', 'earth', 'animal'],
            "survival": ['survive', 'escape', 'danger', 'threat', 'risk', 'death', 'alive'],
            "mystery": ['mystery', 'secret', 'hidden', 'discover', 'reveal', 'clue'],
            "leadership": ['leader', 'follow', 'command', 'direct', 'guide', 'decision'],
            "morality": ['moral', 'right', 'wrong', 'good', 'evil', 'ethical', 'choice'],
            "science": ['science', 'experiment', 'research', 'discovery', 'theory', 'laboratory'],
            "space": ['space', 'planet', 'star', 'galaxy', 'universe', 'cosmic', 'orbit']
        }
        
        # Score each theme category
        theme_scores = {}
        for category, keywords in theme_categories.items():
            score = sum(count for word, count in common_words if word in keywords)
            theme_scores[category] = score
        
        # Normalize scores
        max_score = max(theme_scores.values()) if theme_scores else 1
        normalized_scores = {category: score / max_score for category, score in theme_scores.items()}
        
        # Sort themes by score
        sorted_themes = sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Identify primary and secondary themes
        themes["primary"] = [theme for theme, score in sorted_themes[:3] if score > 0.5]
        themes["secondary"] = [theme for theme, score in sorted_themes[3:6] if score > 0.3]
        themes["all_scores"] = normalized_scores
        
        # Include most common thematic words
        themes["common_thematic_words"] = [{"word": word, "count": count} 
                                          for word, count in common_words[:20]]
        
        return themes
    
    def _identify_settings(self, text: str) -> Dict[str, Any]:
        """Identify setting elements in the text.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with setting analysis
        """
        settings = {}
        
        # Define setting categories and keywords
        setting_categories = {
            "urban": ['city', 'street', 'building', 'apartment', 'urban', 'downtown'],
            "rural": ['farm', 'field', 'village', 'countryside', 'rural', 'barn'],
            "futuristic": ['future', 'advanced', 'technology', 'spacecraft', 'robot', 'hologram'],
            "historical": ['ancient', 'medieval', 'century', 'historical', 'kingdom', 'empire'],
            "natural": ['forest', 'mountain', 'river', 'lake', 'ocean', 'nature', 'tree'],
            "space": ['space', 'planet', 'star', 'ship', 'station', 'orbit', 'galaxy'],
            "indoor": ['room', 'house', 'inside', 'interior', 'corridor', 'hall'],
            "underwater": ['ocean', 'underwater', 'sea', 'submarine', 'aquatic', 'marine'],
            "military": ['base', 'outpost', 'command', 'ship', 'bunker', 'fortress']
        }
        
        # Check time period indicators
        time_periods = {
            "prehistoric": ['prehistoric', 'ancient', 'primitive', 'stone age', 'dinosaur'],
            "antiquity": ['ancient', 'roman', 'greek', 'egyptian', 'babylon', 'pharaoh'],
            "medieval": ['medieval', 'castle', 'knight', 'kingdom', 'middle ages', 'sword'],
            "renaissance": ['renaissance', 'reformation', 'tudor', 'elizabethan'],
            "industrial": ['industrial', 'factory', 'steam', 'victorian', '19th century'],
            "modern": ['modern', 'contemporary', 'present day', 'current', 'today'],
            "near_future": ['near future', 'upcoming', 'soon', 'next decade', 'tomorrow'],
            "far_future": ['distant future', 'far future', 'centuries ahead', 'millennia', 'eon']
        }
        
        # Tokenize and analyze
        words = word_tokenize(text.lower())
        text_lower = text.lower()
        
        # Score settings
        setting_scores = {}
        for category, keywords in setting_categories.items():
            score = sum(words.count(keyword) for keyword in keywords)
            setting_scores[category] = score
        
        # Score time periods
        time_scores = {}
        for period, keywords in time_periods.items():
            score = sum(text_lower.count(keyword) for keyword in keywords)
            time_scores[period] = score
        
        # Normalize and sort
        if setting_scores:
            max_setting = max(setting_scores.values())
            if max_setting > 0:
                normalized_settings = {k: v / max_setting for k, v in setting_scores.items()}
                sorted_settings = sorted(normalized_settings.items(), key=lambda x: x[1], reverse=True)
                settings["primary_setting"] = sorted_settings[0][0] if sorted_settings[0][1] > 0.3 else "undefined"
                settings["setting_scores"] = normalized_settings
        
        if time_scores:
            max_time = max(time_scores.values())
            if max_time > 0:
                normalized_times = {k: v / max_time for k, v in time_scores.items()}
                sorted_times = sorted(normalized_times.items(), key=lambda x: x[1], reverse=True)
                settings["time_period"] = sorted_times[0][0] if sorted_times[0][1] > 0.3 else "undefined"
                settings["time_scores"] = normalized_times
        
        # Extract location mentions
        location_indicators = ['at', 'in', 'on', 'near', 'inside', 'outside', 'across', 'beyond']
        location_candidates = []
        
        sentences = sent_tokenize(text)
        for sentence in sentences:
            words = word_tokenize(sentence)
            for i, word in enumerate(words):
                if word.lower() in location_indicators and i + 1 < len(words):
                    if words[i+1][0].isupper():  # Check for proper noun
                        # Capture 1-3 subsequent capitalized words
                        loc = []
                        for j in range(i+1, min(i+4, len(words))):
                            if words[j][0].isupper() and words[j].isalpha():
                                loc.append(words[j])
                            else:
                                break
                        if loc:
                            location_candidates.append(' '.join(loc))
        
        # Count and rank location mentions
        if location_candidates:
            location_counts = Counter(location_candidates)
            common_locations = location_counts.most_common(5)
            settings["mentioned_locations"] = [{"location": loc, "count": count} 
                                             for loc, count in common_locations if count > 1]
        
        return settings
    
    def _identify_plot_elements(self, text: str) -> Dict[str, Any]:
        """Identify potential plot elements in the text.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with plot element analysis
        """
        plot_elements = {}
        
        # Define plot element categories
        plot_categories = {
            "quest": ['search', 'seek', 'find', 'quest', 'journey', 'mission'],
            "conflict": ['battle', 'fight', 'war', 'conflict', 'struggle', 'confront'],
            "mystery": ['mystery', 'secret', 'clue', 'reveal', 'discover', 'unknown'],
            "betrayal": ['betray', 'deceive', 'trick', 'treachery', 'traitor'],
            "rescue": ['save', 'rescue', 'protect', 'defend', 'help'],
            "transformation": ['change', 'transform', 'become', 'evolve', 'grow'],
            "revenge": ['revenge', 'avenge', 'vengeance', 'payback', 'retribution'],
            "discovery": ['discover', 'find', 'uncover', 'learn', 'realize'],
            "escape": ['escape', 'flee', 'run', 'evade', 'avoid', 'breakout'],
            "sacrifice": ['sacrifice', 'give up', 'surrender', 'offer', 'forfeit']
        }
        
        # Tokenize text
        words = word_tokenize(text.lower())
        text_lower = text.lower()
        
        # Score plot elements
        plot_scores = {}
        for category, keywords in plot_categories.items():
            # Check for individual words
            word_score = sum(words.count(keyword) for keyword in keywords)
            # Check for phrases (may contain multiple words)
            phrase_score = sum(text_lower.count(keyword) for keyword in keywords if ' ' in keyword)
            plot_scores[category] = word_score + phrase_score * 2  # Weight phrases higher
        
        # Normalize and sort
        if plot_scores:
            max_score = max(plot_scores.values())
            if max_score > 0:
                normalized_scores = {k: v / max_score for k, v in plot_scores.items()}
                sorted_plots = sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True)
                
                # Extract top plot elements
                plot_elements["primary_elements"] = [plot for plot, score in sorted_plots[:3] 
                                                   if score > 0.4]
                plot_elements["element_scores"] = normalized_scores
        
        # Look for save-the-cat structure elements
        save_cat_elements = {
            "opening_image": "opening|begins|start|first",
            "theme_stated": "theme|message|moral|lesson",
            "setup": "introduce|establish|setting|background",
            "catalyst": "catalyst|event|trigger|inciting|incident",
            "debate": "debate|hesitate|doubt|question|uncertain",
            "break_into_two": "decision|choice|accept|begin journey",
            "b_story": "subplot|secondary|relationship|love interest",
            "fun_and_games": "adventure|exploration|action|discovery",
            "midpoint": "middle|halfway|midpoint|turn|twist",
            "bad_guys_close_in": "threat|danger|enemy|opponent|challenge",
            "all_is_lost": "defeat|failure|loss|despair|lowest point",
            "dark_night_of_soul": "reflection|introspection|doubt|hopeless",
            "break_into_three": "solution|realization|plan|inspiration",
            "finale": "climax|showdown|confrontation|final battle",
            "final_image": "ending|conclusion|resolution|final"
        }
        
        # Check for save-the-cat elements
        save_cat_scores = {}
        for element, patterns in save_cat_elements.items():
            # Create a regex pattern
            pattern = re.compile(r'\b(' + patterns.replace('|', '|') + r')\b', re.IGNORECASE)
            # Count matches
            matches = pattern.findall(text)
            save_cat_scores[element] = len(matches)
        
        # Normalize and include
        if save_cat_scores:
            total_matches = sum(save_cat_scores.values())
            if total_matches > 0:
                # Convert to relative proportions
                normalized_save_cat = {k: v / total_matches for k, v in save_cat_scores.items()}
                plot_elements["save_the_cat_structure"] = normalized_save_cat
        
        return plot_elements
    
    def _analyze_vocabulary(self, text: str) -> Dict[str, Any]:
        """Analyze vocabulary uniqueness and complexity.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with vocabulary analysis
        """
        vocabulary = {}
        
        # Tokenize and clean
        words = word_tokenize(text.lower())
        words = [word for word in words if word.isalpha()]
        
        # Calculate frequency distribution
        word_freq = Counter(words)
        
        # Calculate common words (minus stopwords)
        content_words = [word for word in words if word not in self.stopwords]
        content_freq = Counter(content_words)
        
        # Get most common content words
        most_common = content_freq.most_common(20)
        vocabulary["common_content_words"] = [{"word": word, "count": count} for word, count in most_common]
        
        # Analyze complexity
        word_lengths = [len(word) for word in content_words]
        vocabulary["avg_content_word_length"] = sum(word_lengths) / len(word_lengths) if word_lengths else 0
        
        # Check for advanced vocabulary
        advanced_words = [word for word in content_words if len(word) > 8]
        vocabulary["advanced_word_ratio"] = len(advanced_words) / len(content_words) if content_words else 0
        
        # Sample some unique advanced words
        if advanced_words:
            unique_advanced = list(set(advanced_words))
            if len(unique_advanced) > 10:
                vocabulary["advanced_word_samples"] = random.sample(unique_advanced, 10)
            else:
                vocabulary["advanced_word_samples"] = unique_advanced
        
        return vocabulary
    
    def _perform_deep_analysis(self, text: str) -> Dict[str, Any]:
        """Perform deeper analysis for more sophisticated insights.
        
        Args:
            text: The text to analyze
        
        Returns:
            Dictionary with deep analysis results
        """
        deep_insights = {}
        
        # Analyze emotional tone
        emotion_categories = {
            "joy": ['happy', 'joy', 'delight', 'pleasure', 'thrill', 'smile', 'laugh'],
            "sadness": ['sad', 'grief', 'sorrow', 'mourn', 'depression', 'despair', 'melancholy'],
            "anger": ['angry', 'fury', 'rage', 'hate', 'outrage', 'irritation', 'frustration'],
            "fear": ['fear', 'terror', 'horror', 'dread', 'panic', 'alarm', 'shock'],
            "surprise": ['surprise', 'amazement', 'astonishment', 'wonder', 'shock'],
            "disgust": ['disgust', 'revulsion', 'loathing', 'distaste', 'aversion'],
            "anticipation": ['anticipation', 'expectation', 'prospect', 'suspense', 'tension']
        }
        
        # Score emotions
        words = word_tokenize(text.lower())
        emotion_scores = {}
        for emotion, keywords in emotion_categories.items():
            score = sum(words.count(keyword) for keyword in keywords)
            emotion_scores[emotion] = score
        
        # Normalize and sort
        if emotion_scores:
            max_score = max(emotion_scores.values())
            if max_score > 0:
                normalized_emotions = {k: v / max_score for k, v in emotion_scores.items()}
                deep_insights["emotional_tone"] = normalized_emotions
                
                # Determine primary emotions
                sorted_emotions = sorted(normalized_emotions.items(), key=lambda x: x[1], reverse=True)
                deep_insights["primary_emotions"] = [emotion for emotion, score in sorted_emotions[:2] 
                                                   if score > 0.4]
        
        # Analyze writing complexity
        syllable_pattern = re.compile(r'[aeiouy]+', re.IGNORECASE)
        sentences = sent_tokenize(text)
        word_count = len([word for word in words if word.isalpha()])
        sentence_count = len(sentences)
        
        # Calculate approximate syllables
        syllable_count = 0
        for word in words:
            if word.isalpha():
                syllables = len(syllable_pattern.findall(word))
                # Every word has at least one syllable
                syllable_count += max(1, syllables)
        
        # Calculate Flesch-Kincaid metrics
        if sentence_count > 0 and word_count > 0:
            # Flesch Reading Ease
            reading_ease = 206.835 - 1.015 * (word_count / sentence_count) - 84.6 * (syllable_count / word_count)
            # Flesch-Kincaid Grade Level
            grade_level = 0.39 * (word_count / sentence_count) + 11.8 * (syllable_count / word_count) - 15.59
            
            deep_insights["reading_complexity"] = {
                "reading_ease": reading_ease,
                "grade_level": grade_level,
                "interpretation": self._interpret_reading_complexity(reading_ease)
            }
        
        # Analyze rhythm and pacing
        para_lengths = [len(para.split()) for para in re.split(r'\n{2,}', text)]
        if para_lengths:
            variance = sum((length - sum(para_lengths) / len(para_lengths)) ** 2 for length in para_lengths) / len(para_lengths)
            deep_insights["pacing"] = {
                "paragraph_length_variance": variance,
                "fast_paced_sections_ratio": len([l for l in para_lengths if l < 30]) / len(para_lengths),
                "interpretation": "fast_paced" if variance > 200 else "moderate_paced" if variance > 100 else "steady_paced"
            }
        
        return deep_insights
    
    def _interpret_reading_complexity(self, reading_ease: float) -> str:
        """Interpret the Flesch Reading Ease score.
        
        Args:
            reading_ease: Flesch Reading Ease score
        
        Returns:
            String interpretation of the score
        """
        if reading_ease >= 90:
            return "very_easy"
        elif reading_ease >= 80:
            return "easy"
        elif reading_ease >= 70:
            return "fairly_easy"
        elif reading_ease >= 60:
            return "standard"
        elif reading_ease >= 50:
            return "fairly_difficult"
        elif reading_ease >= 30:
            return "difficult"
        else:
            return "very_difficult"
    
    def _analyze_relationships(self, text: str, characters: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze character relationships.
        
        Args:
            text: The text to analyze
            characters: The character analysis results
        
        Returns:
            Dictionary with relationship analysis
        """
        relationships = {}
        
        character_profiles = characters.get("profiles", [])
        if not character_profiles:
            return {"character_pairs": []}
        
        # Extract character names
        character_names = [profile["name"] for profile in character_profiles]
        
        # Look for co-occurrences in sentences
        sentences = sent_tokenize(text)
        
        # Create a co-occurrence matrix
        character_pairs = []
        for i, name_a in enumerate(character_names):
            for j, name_b in enumerate(character_names):
                if i < j:  # Only process unique pairs
                    # Count sentences where both characters are mentioned
                    co_occurrences = sum(1 for sentence in sentences 
                                        if re.search(r'\b' + re.escape(name_a) + r'\b', sentence) 
                                        and re.search(r'\b' + re.escape(name_b) + r'\b', sentence))
                    
                    if co_occurrences > 0:
                        character_pairs.append({
                            "character_a": name_a,
                            "character_b": name_b,
                            "co_occurrences": co_occurrences,
                            "sample_interaction": self._find_interaction_sample(sentences, name_a, name_b)
                        })
        
        # Sort by co-occurrence count
        character_pairs.sort(key=lambda x: x["co_occurrences"], reverse=True)
        
        # Take top 10 most significant relationships
        relationships["character_pairs"] = character_pairs[:10]
        
        return relationships
    
    def _find_interaction_sample(self, sentences: List[str], name_a: str, name_b: str) -> str:
        """Find a sample sentence showing interaction between characters.
        
        Args:
            sentences: List of sentences from the text
            name_a: First character name
            name_b: Second character name
        
        Returns:
            A sample sentence containing both characters
        """
        # Find sentences with both characters
        both_sentences = [sentence for sentence in sentences 
                        if re.search(r'\b' + re.escape(name_a) + r'\b', sentence) 
                        and re.search(r'\b' + re.escape(name_b) + r'\b', sentence)]
        
        # Prefer sentences with dialogue
        dialogue_sentences = [sentence for sentence in both_sentences if '"' in sentence]
        
        if dialogue_sentences:
            # Return a dialogue sentence
            return random.choice(dialogue_sentences)
        elif both_sentences:
            # Return any co-occurrence sentence
            return random.choice(both_sentences)
        else:
            return ""
    
    def _analyze_narrative_arc(self, book_content: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze the narrative arc of the book.
        
        Args:
            book_content: Book content dictionary
        
        Returns:
            Dictionary with narrative arc analysis
        """
        narrative_arc = {}
        
        sections = book_content.get('sections', {}).get('sections', [])
        if not sections:
            return narrative_arc
        
        num_sections = len(sections)
        
        # Divide the book into beginning, middle, and end
        beginning = sections[:num_sections // 3]
        middle = sections[num_sections // 3: 2 * num_sections // 3]
        end = sections[2 * num_sections // 3:]
        
        # Combine text for each part
        beginning_text = "\n\n".join(section.get('content', '') for section in beginning)
        middle_text = "\n\n".join(section.get('content', '') for section in middle)
        end_text = "\n\n".join(section.get('content', '') for section in end)
        
        # Analyze dialogue density across parts
        dialogue_pattern = re.compile(r'"([^"]*)"')
        beginning_dialogues = dialogue_pattern.findall(beginning_text)
        middle_dialogues = dialogue_pattern.findall(middle_text)
        end_dialogues = dialogue_pattern.findall(end_text)
        
        # Calculate dialogue density
        beginning_density = len(beginning_dialogues) / len(sent_tokenize(beginning_text)) if beginning_text else 0
        middle_density = len(middle_dialogues) / len(sent_tokenize(middle_text)) if middle_text else 0
        end_density = len(end_dialogues) / len(sent_tokenize(end_text)) if end_text else 0
        
        narrative_arc["dialogue_progression"] = {
            "beginning": beginning_density,
            "middle": middle_density,
            "end": end_density
        }
        
        # Analyze emotional tone in each part
        emotion_categories = {
            "joy": ['happy', 'joy', 'delight', 'pleasure', 'thrill', 'smile', 'laugh'],
            "sadness": ['sad', 'grief', 'sorrow', 'mourn', 'depression', 'despair', 'melancholy'],
            "anger": ['angry', 'fury', 'rage', 'hate', 'outrage', 'irritation', 'frustration'],
            "fear": ['fear', 'terror', 'horror', 'dread', 'panic', 'alarm', 'shock'],
            "tension": ['tension', 'suspense', 'anticipation', 'nervous', 'worry', 'concern', 'anxiety']
        }
        
        # Calculate emotional profiles for each part
        beginning_emotions = self._calculate_emotional_profile(beginning_text, emotion_categories)
        middle_emotions = self._calculate_emotional_profile(middle_text, emotion_categories)
        end_emotions = self._calculate_emotional_profile(end_text, emotion_categories)
        
        narrative_arc["emotional_arc"] = {
            "beginning": beginning_emotions,
            "middle": middle_emotions,
            "end": end_emotions
        }
        
        # Infer narrative structure based on emotional arcs
        narrative_arc["structure_inference"] = self._infer_narrative_structure(
            beginning_emotions, middle_emotions, end_emotions)
        
        return narrative_arc
    
    def _calculate_emotional_profile(self, text: str, 
                                    emotion_categories: Dict[str, List[str]]) -> Dict[str, float]:
        """Calculate emotional profile for a text segment.
        
        Args:
            text: The text to analyze
            emotion_categories: Dictionary of emotion categories and keywords
        
        Returns:
            Dictionary with normalized emotion scores
        """
        words = word_tokenize(text.lower())
        emotion_scores = {}
        
        for emotion, keywords in emotion_categories.items():
            score = sum(words.count(keyword) for keyword in keywords)
            emotion_scores[emotion] = score
        
        # Normalize scores
        max_score = max(emotion_scores.values()) if emotion_scores and max(emotion_scores.values()) > 0 else 1
        normalized_emotions = {k: v / max_score for k, v in emotion_scores.items()}
        
        return normalized_emotions
    
    def _infer_narrative_structure(self, beginning: Dict[str, float], 
                                 middle: Dict[str, float], 
                                 end: Dict[str, float]) -> Dict[str, Any]:
        """Infer narrative structure based on emotional arcs.
        
        Args:
            beginning: Emotional profile of the beginning
            middle: Emotional profile of the middle
            end: Emotional profile of the end
        
        Returns:
            Dictionary with narrative structure inference
        """
        # Check for common narrative patterns
        tension_growth = (middle.get('tension', 0) > beginning.get('tension', 0) and 
                         middle.get('tension', 0) > end.get('tension', 0))
        
        joy_increase = end.get('joy', 0) > beginning.get('joy', 0)
        sadness_increase = end.get('sadness', 0) > beginning.get('sadness', 0)
        
        fear_middle_spike = (middle.get('fear', 0) > beginning.get('fear', 0) and 
                           middle.get('fear', 0) > end.get('fear', 0))
        
        # Infer the likely structure
        structure = {}
        
        if tension_growth and fear_middle_spike and joy_increase:
            structure["type"] = "classic_hero_journey"
            structure["description"] = "Classic hero's journey with tension that builds, peaks, and resolves"
        
        elif tension_growth and sadness_increase:
            structure["type"] = "tragedy"
            structure["description"] = "Tragic arc with rising tension and sad conclusion"
        
        elif fear_middle_spike and joy_increase:
            structure["type"] = "challenge_and_triumph"
            structure["description"] = "Characters face challenges which they overcome successfully"
        
        elif middle.get('anger', 0) > beginning.get('anger', 0) and middle.get('anger', 0) > end.get('anger', 0):
            structure["type"] = "conflict_resolution"
            structure["description"] = "Central conflict with eventual resolution"
        
        else:
            structure["type"] = "mixed"
            structure["description"] = "Complex structure with mixed emotional patterns"
        
        return structure

# Function to analyze a book's style
def analyze_book_style(book_id: str, deep: bool = False) -> Dict[str, Any]:
    """Analyze the style and content of a book.
    
    Args:
        book_id: ID of the book to analyze
        deep: Whether to perform a deep analysis
    
    Returns:
        Dictionary with analysis results
    """
    analyzer = BookStyleAnalyzer()
    return analyzer.analyze_book_style(book_id, deep)
</file>

<file path="cli_entrypoint.py">
#!/usr/bin/env python
"""
CLI Entrypoint for the Stardock Podium AI System.

This module serves as the main command-line interface for the Star Trek-style podcast
generation system. It provides commands for:
- Book ingestion and analysis
- Episode generation and management
- Voice registry management
- Audio generation and post-processing
- Quality checking

All functionality is accessible through a unified CLI using argparse.
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from typing import Optional, List, Dict, Any

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("stardock_podium.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Create base directories
def create_directories():
    """Create necessary directories for the application."""
    directories = [
        "books",
        "analysis",
        "episodes",
        "audio",
        "voices",
        "temp",
        "data"
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        logger.debug(f"Created directory: {directory}")

class CommandRegistry:
    """Registry for all available commands."""
    
    def __init__(self):
        self.commands = {}
        
    def register(self, name: str, func: callable, help_text: str, arguments: List[Dict[str, Any]]):
        """Register a command with its function, help text, and arguments."""
        self.commands[name] = {
            'func': func,
            'help': help_text,
            'arguments': arguments
        }
    
    def get_command(self, name: str):
        """Get a registered command by name."""
        return self.commands.get(name)
    
    def get_all_commands(self):
        """Get all registered commands."""
        return self.commands

# Initialize command registry
cmd_registry = CommandRegistry()

# Define argument types for better clarity
STR_ARG = {'type': str}
INT_ARG = {'type': int}
FLOAT_ARG = {'type': float}
BOOL_ARG = {'action': 'store_true'}
FILE_ARG = {'type': str}  # Actually a path, but represented as string
DIR_ARG = {'type': str}   # Actually a path, but represented as string

def register_command(name: str, help_text: str, arguments: List[Dict[str, Any]]):
    """Decorator to register commands with the registry."""
    def decorator(func):
        cmd_registry.register(name, func, help_text, arguments)
        return func
    return decorator

# Command implementations will be imported from respective modules after they are created

@register_command(
    name="ingest",
    help_text="Ingest and process reference books",
    arguments=[
        {'name': 'file_path', **FILE_ARG, 'help': 'Path to EPUB file to ingest'},
        {'name': '--analyze', **BOOL_ARG, 'help': 'Perform style analysis after ingestion'}
    ]
)
def cmd_ingest(args):
    """Ingest an EPUB book and optionally analyze its style."""
    from epub_processor import process_epub
    result = process_epub(args.file_path)
    
    if args.analyze and result:
        from book_style_analysis import analyze_book_style
        analyze_book_style(result['book_id'])
    
    if result:
        logger.info(f"Successfully ingested: {result['title']}")
        return True
    return False

@register_command(
    name="analyze",
    help_text="Analyze style and content of ingested books",
    arguments=[
        {'name': 'book_id', **STR_ARG, 'help': 'ID of the book to analyze'},
        {'name': '--deep', **BOOL_ARG, 'help': 'Perform deep analysis'}
    ]
)
def cmd_analyze(args):
    """Analyze the style and content of an ingested book."""
    from book_style_analysis import analyze_book_style
    return analyze_book_style(args.book_id, deep=args.deep)

@register_command(
    name="sync-memory",
    help_text="Sync reference materials to vector memory",
    arguments=[
        {'name': '--all', **BOOL_ARG, 'help': 'Sync all available books'},
        {'name': '--book-id', **STR_ARG, 'help': 'ID of specific book to sync', 'required': False}
    ]
)
def cmd_sync_memory(args):
    """Sync reference materials to the vector memory database."""
    from reference_memory_sync import sync_references
    
    if args.all:
        return sync_references()
    elif args.book_id:
        return sync_references(book_id=args.book_id)
    else:
        logger.error("Either --all or --book-id must be specified")
        return False

@register_command(
    name="generate-episode",
    help_text="Generate a new podcast episode",
    arguments=[
        {'name': '--title', **STR_ARG, 'help': 'Episode title', 'required': False},
        {'name': '--theme', **STR_ARG, 'help': 'Theme or topic for the episode', 'required': False},
        {'name': '--series', **STR_ARG, 'help': 'Series name', 'default': 'Main Series'},
        {'name': '--episode-number', **INT_ARG, 'help': 'Episode number', 'required': False},
        {'name': '--duration', **INT_ARG, 'help': 'Target duration in minutes', 'default': 30}
    ]
)
def cmd_generate_episode(args):
    """Generate a new podcast episode."""
    from story_structure import generate_episode
    
    episode_data = {
        'title': args.title,
        'theme': args.theme,
        'series': args.series,
        'episode_number': args.episode_number,
        'target_duration': args.duration
    }
    
    return generate_episode(episode_data)

@register_command(
    name="edit-script",
    help_text="Edit an episode script",
    arguments=[
        {'name': 'episode_id', **STR_ARG, 'help': 'ID of the episode to edit'}
    ]
)
def cmd_edit_script(args):
    """Open an episode script for editing."""
    from script_editor import edit_episode_script
    return edit_episode_script(args.episode_id)

@register_command(
    name="regenerate-scene",
    help_text="Regenerate a scene in an episode",
    arguments=[
        {'name': 'episode_id', **STR_ARG, 'help': 'ID of the episode'},
        {'name': 'scene_index', **INT_ARG, 'help': 'Index of the scene to regenerate'},
        {'name': '--instructions', **STR_ARG, 'help': 'Special instructions for regeneration', 'required': False}
    ]
)
def cmd_regenerate_scene(args):
    """Regenerate a specific scene in an episode."""
    from script_editor import regenerate_scene
    return regenerate_scene(args.episode_id, args.scene_index, args.instructions)

@register_command(
    name="register-voice",
    help_text="Register a new voice in the voice registry",
    arguments=[
        {'name': 'name', **STR_ARG, 'help': 'Character name for the voice'},
        {'name': 'voice_id', **STR_ARG, 'help': 'ElevenLabs voice ID'},
        {'name': '--description', **STR_ARG, 'help': 'Voice description', 'required': False},
        {'name': '--character-bio', **STR_ARG, 'help': 'Character biography', 'required': False}
    ]
)
def cmd_register_voice(args):
    """Register a new voice in the voice registry."""
    from voice_registry import register_voice
    
    voice_data = {
        'name': args.name,
        'voice_id': args.voice_id,
        'description': args.description,
        'character_bio': args.character_bio
    }
    
    return register_voice(voice_data)

@register_command(
    name="list-voices",
    help_text="List all registered voices",
    arguments=[]
)
def cmd_list_voices(args):
    """List all registered voices."""
    from voice_registry import list_voices
    return list_voices()

@register_command(
    name="generate-audio",
    help_text="Generate audio for an episode",
    arguments=[
        {'name': 'episode_id', **STR_ARG, 'help': 'ID of the episode'},
        {'name': '--output-dir', **DIR_ARG, 'help': 'Output directory', 'default': 'audio'},
        {'name': '--format', **STR_ARG, 'help': 'Output format', 'default': 'mp3'},
        {'name': '--quality', **STR_ARG, 'help': 'Audio quality', 'default': 'high', 
         'choices': ['low', 'medium', 'high']}
    ]
)
def cmd_generate_audio(args):
    """Generate audio for an episode."""
    from audio_pipeline import generate_episode_audio
    
    options = {
        'output_dir': args.output_dir,
        'format': args.format,
        'quality': args.quality
    }
    
    return generate_episode_audio(args.episode_id, options)

@register_command(
    name="check-quality",
    help_text="Check the quality of an episode",
    arguments=[
        {'name': 'episode_id', **STR_ARG, 'help': 'ID of the episode to check'},
        {'name': '--script-only', **BOOL_ARG, 'help': 'Check only the script quality'},
        {'name': '--audio-only', **BOOL_ARG, 'help': 'Check only the audio quality'}
    ]
)
def cmd_check_quality(args):
    """Check the quality of an episode."""
    from quality_checker import check_episode_quality
    
    check_options = {
        'check_script': not args.audio_only,
        'check_audio': not args.script_only
    }
    
    return check_episode_quality(args.episode_id, check_options)

@register_command(
    name="list-episodes",
    help_text="List all generated episodes",
    arguments=[
        {'name': '--series', **STR_ARG, 'help': 'Filter by series', 'required': False},
        {'name': '--status', **STR_ARG, 'help': 'Filter by status', 'required': False, 
         'choices': ['draft', 'complete', 'published']}
    ]
)
def cmd_list_episodes(args):
    """List all generated episodes."""
    from episode_metadata import list_episodes
    
    filters = {}
    if args.series:
        filters['series'] = args.series
    if args.status:
        filters['status'] = args.status
    
    return list_episodes(filters)

def main():
    """Main entry point for the CLI."""
    # Create required directories
    create_directories()
    
    # Set up the argument parser
    parser = argparse.ArgumentParser(
        description='Stardock Podium - AI Star Trek podcast generator',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Add version
    parser.add_argument('--version', action='version', version='Stardock Podium v0.1.0')
    
    # Create subparsers for each command
    subparsers = parser.add_subparsers(dest='command', help='Command to execute')
    
    # Register all commands from the registry
    for cmd_name, cmd_info in cmd_registry.get_all_commands().items():
        cmd_parser = subparsers.add_parser(cmd_name, help=cmd_info['help'])
        
        for arg in cmd_info['arguments']:
            arg_name = arg.pop('name')
            cmd_parser.add_argument(arg_name, **arg)
    
    # Parse arguments
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    # Get and execute the command
    cmd_info = cmd_registry.get_command(args.command)
    if not cmd_info:
        logger.error(f"Unknown command: {args.command}")
        return 1
    
    try:
        result = cmd_info['func'](args)
        return 0 if result else 1
    except Exception as e:
        logger.exception(f"Error executing command {args.command}: {e}")
        return 1

if __name__ == '__main__':
    sys.exit(main())
</file>

<file path="episode_memory.py">
#!/usr/bin/env python
"""
Episode Memory Module for Stardock Podium.

This module handles the storage and retrieval of episode memories,
including plot points, character developments, and continuity information.
"""

import os
import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union

# Local imports
from mem0_client import get_mem0_client
from story_structure import get_episode

# Setup logging
logger = logging.getLogger(__name__)

class EpisodeMemory:
    """Manages episode memory and continuity."""
    
    # Constants for memory categories
    PLOT_POINT = "plot_point"
    CHARACTER_DEVELOPMENT = "character_development"
    WORLD_BUILDING = "world_building"
    CONTINUITY = "continuity"
    RELATIONSHIP = "relationship"
    
    def __init__(self):
        """Initialize the episode memory manager."""
        self.mem0_client = get_mem0_client()
    
    def extract_memories_from_episode(self, episode_id: str) -> Dict[str, List[Dict[str, Any]]]:
        """Extract memory entries from an episode.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Dictionary of memory entries by category
        """
        # Get episode data
        episode = get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {}
        
        # Extract memories by category
        memories = {
            self.PLOT_POINT: self._extract_plot_points(episode),
            self.CHARACTER_DEVELOPMENT: self._extract_character_developments(episode),
            self.WORLD_BUILDING: self._extract_world_building(episode),
            self.CONTINUITY: self._extract_continuity_points(episode),
            self.RELATIONSHIP: self._extract_relationships(episode)
        }
        
        # Save memories to database
        for category, entries in memories.items():
            for entry in entries:
                self.add_memory(
                    content=entry["content"],
                    category=category,
                    episode_id=episode_id,
                    metadata=entry.get("metadata", {})
                )
        
        return memories
    
    def _extract_plot_points(self, episode: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract plot points from an episode.
        
        Args:
            episode: Episode data
        
        Returns:
            List of plot point memory entries
        """
        plot_points = []
        
        # Extract from beats
        if "beats" in episode:
            for beat in episode["beats"]:
                plot_points.append({
                    "content": f"In episode '{episode.get('title')}', during the '{beat.get('name')}' beat: {beat.get('description')}",
                    "metadata": {
                        "beat": beat.get("name"),
                        "episode_title": episode.get("title"),
                        "episode_number": episode.get("episode_number")
                    }
                })
        
        # Extract from scenes
        if "scenes" in episode:
            for scene in episode["scenes"]:
                if "plot" in scene:
                    plot_points.append({
                        "content": f"In episode '{episode.get('title')}', scene {scene.get('scene_number', 0)}: {scene.get('plot')}",
                        "metadata": {
                            "scene_number": scene.get("scene_number", 0),
                            "beat": scene.get("beat"),
                            "episode_title": episode.get("title"),
                            "episode_number": episode.get("episode_number")
                        }
                    })
        
        return plot_points
    
    def _extract_character_developments(self, episode: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract character developments from an episode.
        
        Args:
            episode: Episode data
        
        Returns:
            List of character development memory entries
        """
        developments = []
        
        # First, get character information
        characters = {char.get("name"): char for char in episode.get("characters", [])}
        
        # Extract from script if available
        if episode.get("script") and episode["script"].get("scenes"):
            for scene in episode["script"]["scenes"]:
                # Get all dialogue lines for each character
                character_lines = {}
                for line in scene.get("lines", []):
                    if line.get("type") == "dialogue":
                        char_name = line.get("character")
                        if char_name not in character_lines:
                            character_lines[char_name] = []
                        character_lines[char_name].append(line.get("content", ""))
                
                # Look for character development in dialogue
                for char_name, lines in character_lines.items():
                    # Join lines for this character in this scene
                    char_dialogue = " ".join(lines)
                    
                    # Look for signs of character development in dialogue
                    dev_indicators = ["I've never", "I've learned", "I realize", "I understand",
                                    "I feel", "I've changed", "I used to", "I think", "I believe"]
                    
                    for indicator in dev_indicators:
                        if indicator.lower() in char_dialogue.lower():
                            # Extract the sentence containing the indicator
                            sentences = re.split(r'[.!?]+', char_dialogue)
                            relevant_sentence = next((s for s in sentences 
                                                  if indicator.lower() in s.lower()), "")
                            
                            if relevant_sentence:
                                developments.append({
                                    "content": f"Character Development for {char_name} in episode '{episode.get('title')}': {relevant_sentence.strip()}",
                                    "metadata": {
                                        "character": char_name,
                                        "episode_title": episode.get("title"),
                                        "episode_number": episode.get("episode_number"),
                                        "scene_number": scene.get("scene_number", 0)
                                    }
                                })
        
        # Add basic character introductions if this is their first appearance
        for char_name, char_data in characters.items():
            developments.append({
                "content": f"Character Introduction: {char_name} is a {char_data.get('species', 'unknown')} {char_data.get('role', 'crew member')} who appears in episode '{episode.get('title')}'. {char_data.get('personality', '')}",
                "metadata": {
                    "character": char_name,
                    "episode_title": episode.get("title"),
                    "episode_number": episode.get("episode_number"),
                    "character_role": char_data.get("role"),
                    "character_species": char_data.get("species")
                }
            })
        
        return developments
    
    def _extract_world_building(self, episode: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract world-building elements from an episode.
        
        Args:
            episode: Episode data
        
        Returns:
            List of world-building memory entries
        """
        world_building = []
        
        # Extract from scenes
        if "scenes" in episode:
            for scene in episode["scenes"]:
                if "setting" in scene:
                    world_building.append({
                        "content": f"Setting in episode '{episode.get('title')}': {scene.get('setting')}",
                        "metadata": {
                            "type": "setting",
                            "episode_title": episode.get("title"),
                            "episode_number": episode.get("episode_number"),
                            "scene_number": scene.get("scene_number", 0)
                        }
                    })
        
        # Extract from script descriptions
        if episode.get("script") and episode["script"].get("scenes"):
            for scene in episode["script"]["scenes"]:
                for line in scene.get("lines", []):
                    if line.get("type") == "description":
                        # Look for setting descriptions
                        content = line.get("content", "")
                        
                        # Only include substantial descriptions
                        if len(content) > 40 and re.search(r'(starship|planet|space|station|base|world|alien|technology)', 
                                                         content, re.IGNORECASE):
                            world_building.append({
                                "content": f"World detail from episode '{episode.get('title')}': {content}",
                                "metadata": {
                                    "type": "description",
                                    "episode_title": episode.get("title"),
                                    "episode_number": episode.get("episode_number"),
                                    "scene_number": scene.get("scene_number", 0)
                                }
                            })
        
        return world_building
    
    def _extract_continuity_points(self, episode: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract continuity points from an episode.
        
        Args:
            episode: Episode data
        
        Returns:
            List of continuity memory entries
        """
        continuity = []
        
        # Add basic episode information for continuity
        continuity.append({
            "content": f"Episode '{episode.get('title')}' (#{episode.get('episode_number')}) in series '{episode.get('series')}' deals with the theme of {episode.get('theme', 'space exploration')}.",
            "metadata": {
                "type": "episode_summary",
                "episode_title": episode.get("title"),
                "episode_number": episode.get("episode_number"),
                "series": episode.get("series")
            }
        })
        
        # Extract from script dialogue references to past events
        if episode.get("script") and episode["script"].get("scenes"):
            for scene in episode["script"]["scenes"]:
                for line in scene.get("lines", []):
                    if line.get("type") == "dialogue":
                        content = line.get("content", "")
                        
                        # Look for references to past events
                        past_indicators = ["remember when", "last time", "previously", "before",
                                          "used to", "back when", "last mission", "last episode"]
                        
                        for indicator in past_indicators:
                            if indicator.lower() in content.lower():
                                # Extract the sentence containing the indicator
                                sentences = re.split(r'[.!?]+', content)
                                relevant_sentence = next((s for s in sentences 
                                                      if indicator.lower() in s.lower()), "")
                                
                                if relevant_sentence:
                                    continuity.append({
                                        "content": f"Continuity reference from {line.get('character')} in episode '{episode.get('title')}': {relevant_sentence.strip()}",
                                        "metadata": {
                                            "type": "dialogue_reference",
                                            "character": line.get("character"),
                                            "episode_title": episode.get("title"),
                                            "episode_number": episode.get("episode_number"),
                                            "scene_number": scene.get("scene_number", 0)
                                        }
                                    })
        
        return continuity
    
    def _extract_relationships(self, episode: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract relationship developments from an episode.
        
        Args:
            episode: Episode data
        
        Returns:
            List of relationship memory entries
        """
        relationships = []
        
        # Extract from script interactions
        if episode.get("script") and episode["script"].get("scenes"):
            # Track character interactions by scene
            scene_interactions = {}
            
            for scene in episode["script"]["scenes"]:
                scene_number = scene.get("scene_number", 0)
                scene_interactions[scene_number] = {}
                
                # Track speaking characters
                speaking_chars = set()
                
                for line in scene.get("lines", []):
                    if line.get("type") == "dialogue":
                        char_name = line.get("character")
                        speaking_chars.add(char_name)
                        
                        # Analyze dialogue for relationship indicators
                        content = line.get("content", "")
                        
                        # Check if addressing another character
                        for other_char in speaking_chars:
                            if other_char != char_name and other_char in content:
                                # Store the interaction
                                pair_key = tuple(sorted([char_name, other_char]))
                                if pair_key not in scene_interactions[scene_number]:
                                    scene_interactions[scene_number][pair_key] = []
                                
                                scene_interactions[scene_number][pair_key].append(content)
            
            # Generate relationship memories from interactions
            for scene_number, interactions in scene_interactions.items():
                for (char1, char2), dialogues in interactions.items():
                    # Only consider substantial interactions
                    if len(dialogues) >= 2:
                        relationships.append({
                            "content": f"Relationship between {char1} and {char2} in episode '{episode.get('title')}': They interact in scene {scene_number} with dialogue including: '{dialogues[0][:100]}...'",
                            "metadata": {
                                "characters": [char1, char2],
                                "episode_title": episode.get("title"),
                                "episode_number": episode.get("episode_number"),
                                "scene_number": scene_number
                            }
                        })
        
        return relationships
    
    def add_memory(self, content: str, category: str, episode_id: str, 
                  metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Add an episode memory entry.
        
        Args:
            content: Text content for the memory
            category: Category of memory (plot_point, character_development, etc.)
            episode_id: ID of the related episode
            metadata: Additional metadata
        
        Returns:
            Result of the memory addition
        """
        if not metadata:
            metadata = {}
        
        # Add required fields to metadata
        metadata.update({
            "category": category,
            "episode_id": episode_id,
            "created_at": time.time()
        })
        
        # Add to memory
        return self.mem0_client.add_episode_memory(
            content=content,
            episode_id=episode_id,
            metadata=metadata
        )
    
    def search_memories(self, query: str, category: Optional[str] = None, 
                       episode_id: Optional[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
        """Search episode memories for relevant information.
        
        Args:
            query: Search query
            category: Optional memory category to filter by
            episode_id: Optional episode ID to filter by
            limit: Maximum number of results to return
        
        Returns:
            List of matching memory entries
        """
        memories = self.mem0_client.search_episode_memories(
            query=query,
            episode_id=episode_id,
            limit=limit
        )
        
        # Filter by category if specified
        if category and memories:
            memories = [m for m in memories 
                       if m.get('metadata', {}).get('category') == category]
        
        return memories
    
    def get_all_memories(self, episode_id: Optional[str] = None, 
                        category: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get all episode memories, optionally filtered.
        
        Args:
            episode_id: Optional episode ID to filter by
            category: Optional memory category to filter by
        
        Returns:
            List of memory entries
        """
        # Get all episode memories
        memories = self.mem0_client.get_all_memories(
            user_id="episodes",
            memory_type=self.mem0_client.EPISODE_MEMORY
        )
        
        # Filter by episode ID if specified
        if episode_id:
            memories = [m for m in memories 
                       if m.get('metadata', {}).get('episode_id') == episode_id]
        
        # Filter by category if specified
        if category:
            memories = [m for m in memories 
                       if m.get('metadata', {}).get('category') == category]
        
        return memories
    
    def get_character_memories(self, character_name: str) -> List[Dict[str, Any]]:
        """Get all memories related to a specific character.
        
        Args:
            character_name: Name of the character
        
        Returns:
            List of memory entries about the character
        """
        # Search for character-specific memories
        memories = self.mem0_client.search_episode_memories(
            query=character_name,
            limit=50  # Get a large number of results
        )
        
        # Filter to only include memories explicitly about this character
        filtered_memories = []
        for memory in memories:
            metadata = memory.get('metadata', {})
            
            # Include character development memories for this character
            if (metadata.get('category') == self.CHARACTER_DEVELOPMENT and 
                metadata.get('character') == character_name):
                filtered_memories.append(memory)
            
            # Include relationship memories involving this character
            elif (metadata.get('category') == self.RELATIONSHIP and 
                 character_name in metadata.get('characters', [])):
                filtered_memories.append(memory)
            
            # For other memory types, check if the character is mentioned prominently
            elif (character_name.lower() in memory.get('memory', '').lower() and
                 re.search(r'\b' + re.escape(character_name) + r'\b', 
                          memory.get('memory', ''), re.IGNORECASE)):
                filtered_memories.append(memory)
        
        return filtered_memories
    
    def get_timeline(self) -> Dict[str, List[Dict[str, Any]]]:
        """Get a chronological timeline of key events.
        
        Returns:
            Dictionary of episode IDs mapped to key plot points
        """
        # Get all continuity and plot point memories
        continuity_memories = self.get_all_memories(category=self.CONTINUITY)
        plot_memories = self.get_all_memories(category=self.PLOT_POINT)
        
        # Organize by episode
        timeline = {}
        
        for memory in continuity_memories + plot_memories:
            metadata = memory.get('metadata', {})
            episode_id = metadata.get('episode_id')
            
            if not episode_id:
                continue
            
            if episode_id not in timeline:
                timeline[episode_id] = []
            
            # Extract key information
            episode_title = metadata.get('episode_title', 'Unknown Episode')
            episode_number = metadata.get('episode_number', 0)
            
            # Add to timeline with sorting metadata
            timeline[episode_id].append({
                "memory_id": memory.get('id'),
                "content": memory.get('memory', ''),
                "category": metadata.get('category'),
                "episode_title": episode_title,
                "episode_number": episode_number,
                "scene_number": metadata.get('scene_number', 0) if 'scene_number' in metadata else 0,
                "type": metadata.get('type', 'general')
            })
        
        # Sort each episode's events by scene number
        for episode_id in timeline:
            timeline[episode_id].sort(key=lambda x: (x.get('scene_number', 0), x.get('memory_id', '')))
        
        return timeline

# Singleton instance
_episode_memory = None

def get_episode_memory() -> EpisodeMemory:
    """Get the EpisodeMemory singleton instance."""
    global _episode_memory
    
    if _episode_memory is None:
        _episode_memory = EpisodeMemory()
    
    return _episode_memory

def extract_memories(episode_id: str) -> Dict[str, List[Dict[str, Any]]]:
    """Extract and store memories from an episode.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        Dictionary of memory entries by category
    """
    memory_manager = get_episode_memory()
    return memory_manager.extract_memories_from_episode(episode_id)

def add_memory(content: str, category: str, episode_id: str,
              metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Add an episode memory entry.
    
    Args:
        content: Text content for the memory
        category: Category of memory
        episode_id: ID of the related episode
        metadata: Additional metadata
    
    Returns:
        Result of the memory addition
    """
    memory_manager = get_episode_memory()
    return memory_manager.add_memory(content, category, episode_id, metadata)

def search_memories(query: str, category: Optional[str] = None,
                  episode_id: Optional[str] = None, limit: int = 5) -> List[Dict[str, Any]]:
    """Search episode memories.
    
    Args:
        query: Search query
        category: Optional memory category to filter by
        episode_id: Optional episode ID to filter by
        limit: Maximum number of results to return
    
    Returns:
        List of matching memory entries
    """
    memory_manager = get_episode_memory()
    return memory_manager.search_memories(query, category, episode_id, limit)

def get_timeline() -> Dict[str, List[Dict[str, Any]]]:
    """Get a chronological timeline of key events.
    
    Returns:
        Dictionary of episode IDs mapped to key plot points
    """
    memory_manager = get_episode_memory()
    return memory_manager.get_timeline()

def get_character_history(character_name: str) -> List[Dict[str, Any]]:
    """Get the development history of a character.
    
    Args:
        character_name: Name of the character
    
    Returns:
        List of memory entries about the character
    """
    memory_manager = get_episode_memory()
    return memory_manager.get_character_memories(character_name)
</file>

<file path="episode_metadata.py">
#!/usr/bin/env python
"""
Episode Metadata Module for Stardock Podium.

This module handles the management, storage, and retrieval of episode
metadata, including tagging, categorization, and organizational features.
"""

import os
import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Set
import uuid

# Local imports
from story_structure import get_story_structure, get_episode
from mem0_client import get_mem0_client

# Setup logging
logger = logging.getLogger(__name__)

class EpisodeMetadata:
    """Manages episode metadata and organization."""
    
    def __init__(self, episodes_dir: str = "episodes", metadata_dir: str = "data/metadata"):
        """Initialize the episode metadata manager.
        
        Args:
            episodes_dir: Directory containing episode data
            metadata_dir: Directory to store metadata files
        """
        self.episodes_dir = Path(episodes_dir)
        self.metadata_dir = Path(metadata_dir)
        self.metadata_dir.mkdir(exist_ok=True, parents=True)
        
        # Initialize story structure and mem0 client
        self.story_structure = get_story_structure()
        self.mem0_client = get_mem0_client()
        
        # Load series registry
        self._series_registry = self._load_series_registry()
        
        # Load tags registry
        self._tags_registry = self._load_tags_registry()
    
    def _load_series_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load the series registry.
        
        Returns:
            Dictionary of series with metadata
        """
        series_file = self.metadata_dir / "series_registry.json"
        
        if series_file.exists():
            try:
                with open(series_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading series registry: {e}")
        
        # Return empty registry if file doesn't exist or loading fails
        return {}
    
    def _save_series_registry(self) -> None:
        """Save the series registry to file."""
        series_file = self.metadata_dir / "series_registry.json"
        
        try:
            with open(series_file, 'w') as f:
                json.dump(self._series_registry, f, indent=2)
            
            logger.debug("Series registry saved")
        except Exception as e:
            logger.error(f"Error saving series registry: {e}")
    
    def _load_tags_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load the tags registry.
        
        Returns:
            Dictionary of tags with metadata
        """
        tags_file = self.metadata_dir / "tags_registry.json"
        
        if tags_file.exists():
            try:
                with open(tags_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading tags registry: {e}")
        
        # Return empty registry if file doesn't exist or loading fails
        return {}
    
    def _save_tags_registry(self) -> None:
        """Save the tags registry to file."""
        tags_file = self.metadata_dir / "tags_registry.json"
        
        try:
            with open(tags_file, 'w') as f:
                json.dump(self._tags_registry, f, indent=2)
            
            logger.debug("Tags registry saved")
        except Exception as e:
            logger.error(f"Error saving tags registry: {e}")
    
    def register_series(self, series_data: Dict[str, Any]) -> Dict[str, Any]:
        """Register a new series or update existing one.
        
        Args:
            series_data: Dictionary with series information
        
        Returns:
            Updated series data with status
        """
        # Ensure series has a name
        series_name = series_data.get('name')
        if not series_name:
            error_msg = "Series must have a name"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Normalize series name for ID
        series_id = series_data.get('series_id') or self._normalize_id(series_name)
        
        # Check if series already exists
        is_update = series_id in self._series_registry
        
        # Create or update series entry
        self._series_registry[series_id] = {
            "series_id": series_id,
            "name": series_name,
            "description": series_data.get('description', ''),
            "created_at": self._series_registry.get(series_id, {}).get('created_at', time.time()),
            "updated_at": time.time(),
            "tags": series_data.get('tags', []),
            "metadata": series_data.get('metadata', {})
        }
        
        # Save registry
        self._save_series_registry()
        
        return {
            "series_id": series_id,
            "action": "updated" if is_update else "created",
            "series_data": self._series_registry[series_id]
        }
    
    def get_series(self, series_id: str) -> Optional[Dict[str, Any]]:
        """Get series information by ID.
        
        Args:
            series_id: ID of the series
        
        Returns:
            Series data or None if not found
        """
        return self._series_registry.get(series_id)
    
    def list_series(self) -> List[Dict[str, Any]]:
        """List all registered series.
        
        Returns:
            List of series data
        """
        return list(self._series_registry.values())
    
    def delete_series(self, series_id: str) -> Dict[str, Any]:
        """Delete a series from the registry.
        
        Args:
            series_id: ID of the series to delete
        
        Returns:
            Status information
        """
        if series_id not in self._series_registry:
            return {"success": False, "error": f"Series not found: {series_id}"}
        
        # Check if there are episodes in this series
        episodes = self.list_episodes(filters={"series": series_id})
        if episodes:
            return {
                "success": False, 
                "error": f"Cannot delete series with episodes. Found {len(episodes)} episodes."
            }
        
        # Delete from registry
        deleted_series = self._series_registry.pop(series_id)
        self._save_series_registry()
        
        return {"success": True, "deleted_series": deleted_series}
    
    def create_tag(self, tag_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create or update a tag in the registry.
        
        Args:
            tag_data: Dictionary with tag information
        
        Returns:
            Updated tag data with status
        """
        # Ensure tag has a name
        tag_name = tag_data.get('name')
        if not tag_name:
            error_msg = "Tag must have a name"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Normalize tag name for ID
        tag_id = tag_data.get('tag_id') or self._normalize_id(tag_name)
        
        # Check if tag already exists
        is_update = tag_id in self._tags_registry
        
        # Create or update tag entry
        self._tags_registry[tag_id] = {
            "tag_id": tag_id,
            "name": tag_name,
            "description": tag_data.get('description', ''),
            "color": tag_data.get('color', '#cccccc'),
            "created_at": self._tags_registry.get(tag_id, {}).get('created_at', time.time()),
            "updated_at": time.time(),
            "category": tag_data.get('category', 'general')
        }
        
        # Save registry
        self._save_tags_registry()
        
        return {
            "tag_id": tag_id,
            "action": "updated" if is_update else "created",
            "tag_data": self._tags_registry[tag_id]
        }
    
    def get_tag(self, tag_id: str) -> Optional[Dict[str, Any]]:
        """Get tag information by ID.
        
        Args:
            tag_id: ID of the tag
        
        Returns:
            Tag data or None if not found
        """
        return self._tags_registry.get(tag_id)
    
    def list_tags(self, category: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all registered tags, optionally filtered by category.
        
        Args:
            category: Optional category to filter by
        
        Returns:
            List of tag data
        """
        if category:
            return [tag for tag in self._tags_registry.values() 
                   if tag.get('category') == category]
        else:
            return list(self._tags_registry.values())
    
    def delete_tag(self, tag_id: str) -> Dict[str, Any]:
        """Delete a tag from the registry.
        
        Args:
            tag_id: ID of the tag to delete
        
        Returns:
            Status information
        """
        if tag_id not in self._tags_registry:
            return {"success": False, "error": f"Tag not found: {tag_id}"}
        
        # Delete from registry
        deleted_tag = self._tags_registry.pop(tag_id)
        self._save_tags_registry()
        
        # Remove this tag from all episodes
        self._remove_tag_from_all_episodes(tag_id)
        
        return {"success": True, "deleted_tag": deleted_tag}
    
    def _remove_tag_from_all_episodes(self, tag_id: str) -> None:
        """Remove a tag from all episodes.
        
        Args:
            tag_id: ID of the tag to remove
        """
        # Iterate through episodes directory
        for episode_dir in self.episodes_dir.iterdir():
            if not episode_dir.is_dir():
                continue
            
            # Check for metadata file
            metadata_file = episode_dir / "metadata.json"
            if not metadata_file.exists():
                continue
            
            try:
                # Load metadata
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                # Check if this tag is present
                if 'tags' in metadata and tag_id in metadata['tags']:
                    # Remove the tag
                    metadata['tags'].remove(tag_id)
                    
                    # Save updated metadata
                    with open(metadata_file, 'w') as f:
                        json.dump(metadata, f, indent=2)
            
            except Exception as e:
                logger.error(f"Error updating metadata file {metadata_file}: {e}")
    
    def update_episode_metadata(self, episode_id: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Update metadata for an episode.
        
        Args:
            episode_id: ID of the episode
            metadata: Updated metadata fields
        
        Returns:
            Updated episode metadata
        """
        # Get episode directory
        episode_dir = self.episodes_dir / episode_id
        if not episode_dir.exists() or not episode_dir.is_dir():
            error_msg = f"Episode directory not found: {episode_id}"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Check if episode structure exists
        structure_file = episode_dir / "structure.json"
        if not structure_file.exists():
            error_msg = f"Episode structure file not found: {episode_id}"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Load current metadata
        metadata_file = episode_dir / "metadata.json"
        current_metadata = {}
        
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r') as f:
                    current_metadata = json.load(f)
            except Exception as e:
                logger.error(f"Error loading current metadata: {e}")
        
        # Update metadata
        updated_metadata = current_metadata.copy()
        updated_metadata.update(metadata)
        
        # Ensure certain fields are always present
        if 'episode_id' not in updated_metadata:
            updated_metadata['episode_id'] = episode_id
        
        if 'tags' not in updated_metadata:
            updated_metadata['tags'] = []
        
        if 'updated_at' not in updated_metadata:
            updated_metadata['updated_at'] = time.time()
        
        # Save metadata
        try:
            with open(metadata_file, 'w') as f:
                json.dump(updated_metadata, f, indent=2)
            
            logger.info(f"Updated metadata for episode {episode_id}")
            return updated_metadata
        
        except Exception as e:
            logger.error(f"Error saving metadata: {e}")
            return {"error": f"Error saving metadata: {e}"}
    
    def get_episode_metadata(self, episode_id: str) -> Dict[str, Any]:
        """Get metadata for an episode.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Episode metadata
        """
        # Check episode directory
        episode_dir = self.episodes_dir / episode_id
        if not episode_dir.exists() or not episode_dir.is_dir():
            logger.error(f"Episode directory not found: {episode_id}")
            return {}
        
        # Check metadata file
        metadata_file = episode_dir / "metadata.json"
        if not metadata_file.exists():
            # If no metadata file, return basic info from structure
            structure_file = episode_dir / "structure.json"
            if structure_file.exists():
                try:
                    with open(structure_file, 'r') as f:
                        structure = json.load(f)
                    
                    # Create basic metadata
                    return {
                        "episode_id": episode_id,
                        "title": structure.get("title", ""),
                        "series": structure.get("series", ""),
                        "episode_number": structure.get("episode_number", 0),
                        "created_at": structure.get("created_at", 0),
                        "tags": []
                    }
                except Exception as e:
                    logger.error(f"Error reading structure file: {e}")
                    return {}
            else:
                logger.error(f"Episode structure file not found: {episode_id}")
                return {}
        
        # Read metadata file
        try:
            with open(metadata_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error reading metadata file: {e}")
            return {}
    
    def add_tag_to_episode(self, episode_id: str, tag_id: str) -> Dict[str, Any]:
        """Add a tag to an episode.
        
        Args:
            episode_id: ID of the episode
            tag_id: ID of the tag to add
        
        Returns:
            Updated episode metadata
        """
        # Check if tag exists
        if tag_id not in self._tags_registry:
            error_msg = f"Tag not found: {tag_id}"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Get current metadata
        metadata = self.get_episode_metadata(episode_id)
        if not metadata:
            error_msg = f"Episode not found: {episode_id}"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Add tag if not already present
        if 'tags' not in metadata:
            metadata['tags'] = []
        
        if tag_id not in metadata['tags']:
            metadata['tags'].append(tag_id)
            
            # Update metadata
            return self.update_episode_metadata(episode_id, metadata)
        
        return metadata
    
    def remove_tag_from_episode(self, episode_id: str, tag_id: str) -> Dict[str, Any]:
        """Remove a tag from an episode.
        
        Args:
            episode_id: ID of the episode
            tag_id: ID of the tag to remove
        
        Returns:
            Updated episode metadata
        """
        # Get current metadata
        metadata = self.get_episode_metadata(episode_id)
        if not metadata:
            error_msg = f"Episode not found: {episode_id}"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Remove tag if present
        if 'tags' in metadata and tag_id in metadata['tags']:
            metadata['tags'].remove(tag_id)
            
            # Update metadata
            return self.update_episode_metadata(episode_id, metadata)
        
        return metadata
    
    def list_episodes(self, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """List all episodes with metadata, applying optional filters.
        
        Args:
            filters: Optional dictionary of filter criteria
        
        Returns:
            List of episode metadata
        """
        # Get all episodes from story structure
        all_episodes = self.story_structure.list_episodes()
        
        # Enhance with metadata
        enhanced_episodes = []
        
        for episode in all_episodes:
            episode_id = episode.get("episode_id")
            if not episode_id:
                continue
            
            # Get metadata
            metadata = self.get_episode_metadata(episode_id)
            
            # Create enhanced info
            enhanced = {**episode, **metadata}
            enhanced_episodes.append(enhanced)
        
        # Apply filters if provided
        if filters:
            filtered_episodes = []
            
            for episode in enhanced_episodes:
                include = True
                
                for key, value in filters.items():
                    if key == 'tags':
                        # Special handling for tags filter
                        episode_tags = episode.get('tags', [])
                        if not set(value).issubset(set(episode_tags)):
                            include = False
                            break
                    elif key == 'series':
                        # Check series name or ID
                        if value != episode.get('series'):
                            include = False
                            break
                    elif key == 'status':
                        if value != episode.get('status'):
                            include = False
                            break
                    elif key == 'search':
                        # Search in title or description
                        title = episode.get('title', '').lower()
                        description = episode.get('description', '').lower()
                        if value.lower() not in title and value.lower() not in description:
                            include = False
                            break
                    elif key == 'date_range':
                        # Check if episode created_at is within range
                        if not self._is_in_date_range(
                            episode.get('created_at', 0), 
                            value.get('start'), 
                            value.get('end')
                        ):
                            include = False
                            break
                    elif key not in episode or episode[key] != value:
                        include = False
                        break
                
                if include:
                    filtered_episodes.append(episode)
            
            return filtered_episodes
        
        return enhanced_episodes
    
    def _is_in_date_range(self, timestamp: float, start: Optional[float] = None, 
                         end: Optional[float] = None) -> bool:
        """Check if a timestamp is within a date range.
        
        Args:
            timestamp: The timestamp to check
            start: Optional start timestamp
            end: Optional end timestamp
        
        Returns:
            True if timestamp is within range, False otherwise
        """
        if start is not None and timestamp < start:
            return False
        
        if end is not None and timestamp > end:
            return False
        
        return True
    
    def _normalize_id(self, name: str) -> str:
        """Normalize a name to create a valid ID.
        
        Args:
            name: Name to normalize
        
        Returns:
            Normalized ID
        """
        # Remove non-alphanumeric characters and replace spaces with underscores
        normalized = re.sub(r'[^\w\s]', '', name).strip().lower().replace(' ', '_')
        
        # If empty after normalization, use a random ID
        if not normalized:
            normalized = f"id_{uuid.uuid4().hex[:8]}"
        
        return normalized
    
    def generate_episode_feed(self, format: str = "json", 
                            filters: Optional[Dict[str, Any]] = None) -> str:
        """Generate an episode feed in the specified format.
        
        Args:
            format: Output format (json, rss, etc.)
            filters: Optional filters to apply
        
        Returns:
            Formatted feed string
        """
        # Get filtered episodes
        episodes = self.list_episodes(filters=filters)
        
        # Sort by episode number and series
        episodes.sort(key=lambda ep: (ep.get("series", ""), ep.get("episode_number", 0)))
        
        if format.lower() == "json":
            return json.dumps({
                "episodes": episodes,
                "generated_at": time.time(),
                "count": len(episodes)
            }, indent=2)
        elif format.lower() == "rss":
            # Simple RSS generation for podcast feed
            rss = '<?xml version="1.0" encoding="UTF-8"?>\n'
            rss += '<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd">\n'
            rss += '  <channel>\n'
            rss += '    <title>Stardock Podium AI Podcast</title>\n'
            rss += '    <description>AI generated Star Trek-style podcast episodes</description>\n'
            rss += f'    <lastBuildDate>{time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.gmtime())}</lastBuildDate>\n'
            
            for episode in episodes:
                # Skip episodes without audio
                if not episode.get("has_audio"):
                    continue
                
                episode_id = episode.get("episode_id")
                title = episode.get("title", "Unknown Episode")
                description = episode.get("description", "")
                series = episode.get("series", "Main Series")
                
                rss += '    <item>\n'
                rss += f'      <title>{title}</title>\n'
                rss += f'      <description>{description}</description>\n'
                rss += f'      <guid>{episode_id}</guid>\n'
                
                # Add enclosure if audio file path is known
                audio_file = self.episodes_dir / episode_id / "audio" / "full_episode.mp3"
                if audio_file.exists():
                    file_size = audio_file.stat().st_size
                    rss += f'      <enclosure url="episodes/{episode_id}/audio/full_episode.mp3" length="{file_size}" type="audio/mpeg" />\n'
                
                rss += '    </item>\n'
            
            rss += '  </channel>\n'
            rss += '</rss>\n'
            
            return rss
        else:
            error_msg = f"Unsupported feed format: {format}"
            logger.error(error_msg)
            return json.dumps({"error": error_msg})
    
    def analyze_episode_stats(self) -> Dict[str, Any]:
        """Analyze statistics about episodes.
        
        Returns:
            Dictionary with episode statistics
        """
        episodes = self.list_episodes()
        
        # Count by series
        series_counts = {}
        for episode in episodes:
            series = episode.get("series", "Uncategorized")
            if series not in series_counts:
                series_counts[series] = 0
            series_counts[series] += 1
        
        # Count by status
        status_counts = {}
        for episode in episodes:
            status = episode.get("status", "draft")
            if status not in status_counts:
                status_counts[status] = 0
            status_counts[status] += 1
        
        # Count by tag
        tag_counts = {}
        for episode in episodes:
            for tag_id in episode.get("tags", []):
                if tag_id not in tag_counts:
                    tag_counts[tag_id] = 0
                tag_counts[tag_id] += 1
        
        # Get tag names
        tag_stats = []
        for tag_id, count in tag_counts.items():
            tag_info = self.get_tag(tag_id)
            if tag_info:
                tag_stats.append({
                    "tag_id": tag_id,
                    "name": tag_info.get("name", "Unknown"),
                    "count": count
                })
        
        # Get series info
        series_stats = []
        for series_id, count in series_counts.items():
            series_info = self.get_series(series_id)
            if series_info:
                series_stats.append({
                    "series_id": series_id,
                    "name": series_info.get("name", series_id),
                    "count": count
                })
            else:
                series_stats.append({
                    "series_id": series_id,
                    "name": series_id,
                    "count": count
                })
        
        # Calculate other stats
        total_episodes = len(episodes)
        episodes_with_audio = sum(1 for ep in episodes if ep.get("has_audio"))
        
        return {
            "total_episodes": total_episodes,
            "episodes_with_audio": episodes_with_audio,
            "series_stats": series_stats,
            "status_counts": status_counts,
            "tag_stats": tag_stats,
            "generated_at": time.time()
        }

# Singleton instance
_episode_metadata = None

def get_episode_metadata_manager() -> EpisodeMetadata:
    """Get the EpisodeMetadata singleton instance."""
    global _episode_metadata
    
    if _episode_metadata is None:
        _episode_metadata = EpisodeMetadata()
    
    return _episode_metadata

def update_metadata(episode_id: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
    """Update metadata for an episode.
    
    Args:
        episode_id: ID of the episode
        metadata: Updated metadata fields
    
    Returns:
        Updated episode metadata
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.update_episode_metadata(episode_id, metadata)

def get_metadata(episode_id: str) -> Dict[str, Any]:
    """Get metadata for an episode.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        Episode metadata
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.get_episode_metadata(episode_id)

def list_episodes(filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
    """List all episodes with metadata, applying optional filters.
    
    Args:
        filters: Optional dictionary of filter criteria
    
    Returns:
        List of episode metadata
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.list_episodes(filters)

def register_series(series_data: Dict[str, Any]) -> Dict[str, Any]:
    """Register a new series or update existing one.
    
    Args:
        series_data: Dictionary with series information
    
    Returns:
        Updated series data with status
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.register_series(series_data)

def list_series() -> List[Dict[str, Any]]:
    """List all registered series.
    
    Returns:
        List of series data
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.list_series()

def create_tag(tag_data: Dict[str, Any]) -> Dict[str, Any]:
    """Create or update a tag in the registry.
    
    Args:
        tag_data: Dictionary with tag information
    
    Returns:
        Updated tag data with status
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.create_tag(tag_data)

def list_tags(category: Optional[str] = None) -> List[Dict[str, Any]]:
    """List all registered tags, optionally filtered by category.
    
    Args:
        category: Optional category to filter by
    
    Returns:
        List of tag data
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.list_tags(category)

def add_tag_to_episode(episode_id: str, tag_id: str) -> Dict[str, Any]:
    """Add a tag to an episode.
    
    Args:
        episode_id: ID of the episode
        tag_id: ID of the tag to add
    
    Returns:
        Updated episode metadata
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.add_tag_to_episode(episode_id, tag_id)

def generate_feed(format: str = "json", 
                filters: Optional[Dict[str, Any]] = None) -> str:
    """Generate an episode feed in the specified format.
    
    Args:
        format: Output format (json, rss, etc.)
        filters: Optional filters to apply
    
    Returns:
        Formatted feed string
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.generate_episode_feed(format, filters)

def get_episode_stats() -> Dict[str, Any]:
    """Get statistics about episodes.
    
    Returns:
        Dictionary with episode statistics
    """
    metadata_manager = get_episode_metadata_manager()
    return metadata_manager.analyze_episode_stats()
</file>

<file path="epub_processor.py">
#!/usr/bin/env python
"""
EPUB Processor Module for Stardock Podium.

This module handles processing EPUB files, extracting content, metadata,
and preparing it for analysis and memory storage.
"""

import os
import logging
import json
import uuid
import re
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Iterator
import html2text

try:
    import ebooklib
    from ebooklib import epub
except ImportError:
    logging.error("EbookLib not found. Please install it with: pip install ebooklib")
    raise

# Setup logging
logger = logging.getLogger(__name__)

class EPUBProcessor:
    """Handler for processing EPUB files for reference ingestion."""
    
    def __init__(self, books_dir: str = "books", analysis_dir: str = "analysis"):
        """Initialize the EPUB processor.
        
        Args:
            books_dir: Directory to store processed book files
            analysis_dir: Directory to store analysis files
        """
        self.books_dir = Path(books_dir)
        self.analysis_dir = Path(analysis_dir)
        
        # Create directories if they don't exist
        self.books_dir.mkdir(exist_ok=True)
        self.analysis_dir.mkdir(exist_ok=True)
        
        # HTML to text converter
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = True
        self.html_converter.ignore_images = True
        self.html_converter.ignore_tables = False
        self.html_converter.body_width = 0  # No wrapping
    
    def process_epub(self, file_path: str) -> Dict[str, Any]:
        """Process an EPUB file, extracting content and metadata.
        
        Args:
            file_path: Path to the EPUB file
        
        Returns:
            Dictionary with book metadata and processing information
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return {}
        
        try:
            # Generate a book ID
            book_id = f"book_{uuid.uuid4().hex[:8]}"
            
            # Read the EPUB file
            book = epub.read_epub(file_path)
            
            # Extract metadata
            metadata = self._extract_metadata(book)
            metadata['book_id'] = book_id
            metadata['file_path'] = str(file_path)
            metadata['processed_at'] = time.time()
            
            # Extract content
            chapters = self._extract_chapters(book)
            
            # Save book data
            book_dir = self.books_dir / book_id
            book_dir.mkdir(exist_ok=True)
            
            # Save metadata
            with open(book_dir / "metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2)
            
            # Save chapters
            for i, chapter in enumerate(chapters):
                chapter_file = book_dir / f"chapter_{i:03d}.json"
                with open(chapter_file, 'w') as f:
                    json.dump(chapter, f, indent=2)
            
            # Save chapter index
            chapter_index = {
                "book_id": book_id,
                "title": metadata.get('title', 'Unknown'),
                "num_chapters": len(chapters),
                "chapters": [{"index": i, "title": ch.get('title', f"Chapter {i+1}")} 
                            for i, ch in enumerate(chapters)]
            }
            
            with open(book_dir / "chapter_index.json", 'w') as f:
                json.dump(chapter_index, f, indent=2)
            
            # Create document sections for more fine-grained reference
            sections = self._create_sections(chapters)
            
            with open(book_dir / "sections.json", 'w') as f:
                json.dump(sections, f, indent=2)
            
            logger.info(f"Successfully processed EPUB: {metadata.get('title', 'Unknown')} (ID: {book_id})")
            
            result = {
                "book_id": book_id,
                "title": metadata.get('title', 'Unknown'),
                "author": metadata.get('creator', 'Unknown'),
                "num_chapters": len(chapters),
                "num_sections": len(sections['sections']),
                "size_bytes": file_path.stat().st_size
            }
            
            return result
        
        except Exception as e:
            logger.exception(f"Error processing EPUB: {e}")
            return {}
    
    def _extract_metadata(self, book: epub.EpubBook) -> Dict[str, str]:
        """Extract metadata from an EPUB book.
        
        Args:
            book: The EpubBook object
        
        Returns:
            Dictionary of metadata
        """
        metadata = {}
        
        # Extract standard Dublin Core metadata
        for key in [
            'title', 'language', 'creator', 'contributor', 'publisher', 
            'identifier', 'source', 'rights', 'date', 'description'
        ]:
            value = book.get_metadata('DC', key)
            if value:
                # Extract value and attributes from the metadata tuple
                metadata[key] = value[0][0]
                
                # Some metadata items may have attributes
                if value[0][1]:
                    for attr_name, attr_value in value[0][1].items():
                        metadata[f"{key}_{attr_name}"] = attr_value
        
        return metadata
    
    def _extract_chapters(self, book: epub.EpubBook) -> List[Dict[str, Any]]:
        """Extract chapters from an EPUB book.
        
        Args:
            book: The EpubBook object
        
        Returns:
            List of chapters with text content
        """
        chapters = []
        
        # Get spine items (the reading order)
        spine_items = book.spine
        
        for item_id in spine_items:
            # Skip if it's the navigation item ('nav' or 'ncx')
            if item_id in ('nav', 'ncx'):
                continue
            
            item = book.get_item_with_id(item_id)
            
            # Skip if item is None or not a document
            if item is None or item.get_type() != ebooklib.ITEM_DOCUMENT:
                continue
            
            # Get content
            content = item.get_content().decode('utf-8', errors='ignore')
            
            # Convert HTML to plain text
            text = self.html_converter.handle(content)
            
            # Clean up extra whitespace
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = text.strip()
            
            # Skip if no meaningful content
            if not text or len(text) < 10:
                continue
            
            # Try to determine chapter title
            title = self._extract_title(content, text)
            
            chapter = {
                "id": item_id,
                "title": title,
                "content": text,
                "html_size": len(content),
                "text_size": len(text)
            }
            
            chapters.append(chapter)
        
        return chapters
    
    def _extract_title(self, html_content: str, text_content: str) -> str:
        """Extract the title from chapter content.
        
        Args:
            html_content: HTML content
            text_content: Plain text content
        
        Returns:
            Extracted title or empty string if not found
        """
        # Try to find h1 tag
        h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', html_content, re.IGNORECASE | re.DOTALL)
        if h1_match:
            title = re.sub(r'<[^>]+>', '', h1_match.group(1))
            return title.strip()
        
        # Try to find first non-empty line
        lines = text_content.split('\n')
        for line in lines:
            line = line.strip()
            if line and len(line) < 100:  # Assume titles aren't too long
                return line
        
        return ""
    
    def _create_sections(self, chapters: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Create smaller sections from chapters for more granular indexing.
        
        Args:
            chapters: List of chapter dictionaries
        
        Returns:
            Dictionary with sections information
        """
        sections = []
        section_size = 1000  # Target size in characters
        
        for chapter_idx, chapter in enumerate(chapters):
            content = chapter['content']
            chapter_title = chapter.get('title', f"Chapter {chapter_idx+1}")
            
            # Split content into paragraphs
            paragraphs = re.split(r'\n{2,}', content)
            
            current_section = []
            current_size = 0
            section_idx = 0
            
            for para in paragraphs:
                para_size = len(para)
                
                # If adding this paragraph would exceed target size and we already have content,
                # finalize the current section and start a new one
                if current_size > 0 and current_size + para_size > section_size:
                    section_text = '\n\n'.join(current_section)
                    section = {
                        "chapter_idx": chapter_idx,
                        "section_idx": section_idx,
                        "chapter_title": chapter_title,
                        "section_title": f"{chapter_title} - Section {section_idx+1}",
                        "content": section_text,
                        "size": current_size
                    }
                    sections.append(section)
                    
                    # Reset for next section
                    current_section = []
                    current_size = 0
                    section_idx += 1
                
                # Add paragraph to current section
                current_section.append(para)
                current_size += para_size
            
            # Don't forget the last section
            if current_section:
                section_text = '\n\n'.join(current_section)
                section = {
                    "chapter_idx": chapter_idx,
                    "section_idx": section_idx,
                    "chapter_title": chapter_title,
                    "section_title": f"{chapter_title} - Section {section_idx+1}",
                    "content": section_text,
                    "size": current_size
                }
                sections.append(section)
        
        return {
            "total_sections": len(sections),
            "target_size": section_size,
            "sections": sections
        }
    
    def list_ingested_books(self) -> List[Dict[str, Any]]:
        """List all ingested books.
        
        Returns:
            List of dictionaries with book information
        """
        book_list = []
        
        for book_dir in self.books_dir.iterdir():
            if not book_dir.is_dir():
                continue
            
            metadata_file = book_dir / "metadata.json"
            if not metadata_file.exists():
                continue
            
            try:
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                
                book_info = {
                    "book_id": metadata.get('book_id', book_dir.name),
                    "title": metadata.get('title', 'Unknown'),
                    "author": metadata.get('creator', 'Unknown'),
                    "processed_at": metadata.get('processed_at'),
                }
                
                # Check for chapter data
                chapter_index_file = book_dir / "chapter_index.json"
                if chapter_index_file.exists():
                    with open(chapter_index_file, 'r') as f:
                        chapter_index = json.load(f)
                        book_info["num_chapters"] = chapter_index.get('num_chapters', 0)
                
                book_list.append(book_info)
            
            except Exception as e:
                logger.error(f"Error reading book metadata from {metadata_file}: {e}")
        
        return book_list
    
    def get_book_metadata(self, book_id: str) -> Dict[str, Any]:
        """Get metadata for a specific book.
        
        Args:
            book_id: The book ID
        
        Returns:
            Dictionary with book metadata
        """
        book_dir = self.books_dir / book_id
        metadata_file = book_dir / "metadata.json"
        
        if not metadata_file.exists():
            logger.error(f"Metadata file not found for book ID: {book_id}")
            return {}
        
        try:
            with open(metadata_file, 'r') as f:
                return json.load(f)
        
        except Exception as e:
            logger.error(f"Error reading book metadata: {e}")
            return {}
    
    def get_book_chapters(self, book_id: str) -> List[Dict[str, Any]]:
        """Get all chapters for a specific book.
        
        Args:
            book_id: The book ID
        
        Returns:
            List of chapter dictionaries
        """
        book_dir = self.books_dir / book_id
        chapter_index_file = book_dir / "chapter_index.json"
        
        if not chapter_index_file.exists():
            logger.error(f"Chapter index file not found for book ID: {book_id}")
            return []
        
        try:
            with open(chapter_index_file, 'r') as f:
                chapter_index = json.load(f)
            
            num_chapters = chapter_index.get('num_chapters', 0)
            chapters = []
            
            for i in range(num_chapters):
                chapter_file = book_dir / f"chapter_{i:03d}.json"
                
                if chapter_file.exists():
                    with open(chapter_file, 'r') as f:
                        chapter = json.load(f)
                        chapters.append(chapter)
            
            return chapters
        
        except Exception as e:
            logger.error(f"Error reading book chapters: {e}")
            return []
    
    def get_book_sections(self, book_id: str) -> Dict[str, Any]:
        """Get all sections for a specific book.
        
        Args:
            book_id: The book ID
        
        Returns:
            Dictionary with sections information
        """
        book_dir = self.books_dir / book_id
        sections_file = book_dir / "sections.json"
        
        if not sections_file.exists():
            logger.error(f"Sections file not found for book ID: {book_id}")
            return {"total_sections": 0, "sections": []}
        
        try:
            with open(sections_file, 'r') as f:
                return json.load(f)
        
        except Exception as e:
            logger.error(f"Error reading book sections: {e}")
            return {"total_sections": 0, "sections": []}
    
    def get_chapter(self, book_id: str, chapter_idx: int) -> Dict[str, Any]:
        """Get a specific chapter from a book.
        
        Args:
            book_id: The book ID
            chapter_idx: The chapter index
        
        Returns:
            Chapter dictionary or empty dict if not found
        """
        book_dir = self.books_dir / book_id
        chapter_file = book_dir / f"chapter_{chapter_idx:03d}.json"
        
        if not chapter_file.exists():
            logger.error(f"Chapter file not found: {chapter_file}")
            return {}
        
        try:
            with open(chapter_file, 'r') as f:
                return json.load(f)
        
        except Exception as e:
            logger.error(f"Error reading chapter: {e}")
            return {}
    
    def get_section(self, book_id: str, chapter_idx: int, section_idx: int) -> Dict[str, Any]:
        """Get a specific section from a book.
        
        Args:
            book_id: The book ID
            chapter_idx: The chapter index
            section_idx: The section index
        
        Returns:
            Section dictionary or empty dict if not found
        """
        sections = self.get_book_sections(book_id)
        
        for section in sections.get('sections', []):
            if (section.get('chapter_idx') == chapter_idx and 
                section.get('section_idx') == section_idx):
                return section
        
        logger.error(f"Section not found: book_id={book_id}, chapter_idx={chapter_idx}, section_idx={section_idx}")
        return {}
    
    def get_book_content_generator(self, book_id: str) -> Iterator[Dict[str, Any]]:
        """Get a generator that yields sections from a book.
        
        Args:
            book_id: The book ID
        
        Yields:
            Section dictionaries
        """
        sections = self.get_book_sections(book_id)
        
        for section in sections.get('sections', []):
            yield section

# Singleton instance
_processor = None

def get_processor() -> EPUBProcessor:
    """Get the EPUBProcessor singleton instance."""
    global _processor
    
    if _processor is None:
        _processor = EPUBProcessor()
    
    return _processor

def process_epub(file_path: str) -> Dict[str, Any]:
    """Process an EPUB file, extracting content and metadata.
    
    Args:
        file_path: Path to the EPUB file
    
    Returns:
        Dictionary with book metadata and processing information
    """
    processor = get_processor()
    return processor.process_epub(file_path)

def list_books() -> List[Dict[str, Any]]:
    """List all ingested books.
    
    Returns:
        List of dictionaries with book information
    """
    processor = get_processor()
    return processor.list_ingested_books()

def get_book_content(book_id: str) -> Dict[str, Any]:
    """Get all content for a book.
    
    Args:
        book_id: The book ID
    
    Returns:
        Dictionary with book metadata, chapters, and sections
    """
    processor = get_processor()
    
    metadata = processor.get_book_metadata(book_id)
    chapters = processor.get_book_chapters(book_id)
    sections = processor.get_book_sections(book_id)
    
    return {
        "metadata": metadata,
        "chapters": chapters,
        "sections": sections
    }
</file>

<file path="main.py">
#!/usr/bin/env python
"""
Stardock Podium - AI Star Trek Podcast Generator

This module serves as the main entry point for the Stardock Podium system. It performs
environment checks, initializes all required components, and launches the CLI interface.

The system is designed to run natively on Windows 10 without requiring Docker or WSL,
while providing a complete pipeline for generating Star Trek-style podcast episodes
from reference materials.
"""

import os
import sys
import platform
import logging
import importlib.util
import subprocess
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("stardock_podium.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Required modules
REQUIRED_MODULES = [
    ('mem0', 'Mem0 Vector Database Client'),
    ('ebooklib', 'EPUB Processing Library'),
    ('elevenlabs', 'ElevenLabs Voice Synthesis API'),
    ('openai', 'OpenAI API Client'),
    ('nltk', 'Natural Language Toolkit'),
    ('ffmpeg', 'FFmpeg Python Bindings')
]

def check_environment():
    """
    Check if the environment is suitable for running the application.
    Returns True if all checks pass, False otherwise.
    """
    checks_passed = True
    
    # Check platform
    logger.info(f"Detected platform: {platform.system()} {platform.release()}")
    if platform.system() != "Windows":
        logger.warning("This application is optimized for Windows 10. Some features may not work as expected.")
    
    # Check Python version
    python_version = sys.version_info
    logger.info(f"Python version: {python_version.major}.{python_version.minor}.{python_version.micro}")
    if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 8):
        logger.error("Python 3.8+ is required to run this application.")
        checks_passed = False
    
    # Check required modules
    for module_name, module_desc in REQUIRED_MODULES:
        if importlib.util.find_spec(module_name) is None:
            logger.error(f"Required module not found: {module_name} ({module_desc})")
            checks_passed = False
        else:
            logger.info(f"Module found: {module_name}")
    
    # Check FFmpeg availability
    try:
        result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)
        if result.returncode == 0:
            ffmpeg_version = result.stdout.split('\n')[0]
            logger.info(f"FFmpeg found: {ffmpeg_version}")
        else:
            logger.error("FFmpeg executable not found or not working properly.")
            checks_passed = False
    except FileNotFoundError:
        logger.error("FFmpeg executable not found in PATH. Please install FFmpeg.")
        checks_passed = False
    
    # Check for API keys in environment variables
    api_keys = {
        'OPENAI_API_KEY': 'OpenAI API',
        'OPENROUTER_API_KEY': 'OpenRouter API',
        'ELEVENLABS_API_KEY': 'ElevenLabs API',
        'MEM0_API_KEY': 'Mem0 API'
    }
    
    for env_var, service_name in api_keys.items():
        if not os.getenv(env_var):
            logger.warning(f"Environment variable {env_var} for {service_name} not found.")
    
    return checks_passed

def create_default_directories():
    """Create all necessary directories for the application."""
    directories = [
        "books",      # For storing ingested books
        "analysis",   # For storing analysis results
        "episodes",   # For storing episode data
        "audio",      # For storing generated audio
        "voices",     # For voice registry data
        "temp",       # For temporary files
        "data",       # For application data
        "logs"        # For log files
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        logger.debug(f"Created directory: {directory}")

def check_nltk_data():
    """Ensure required NLTK data is downloaded."""
    try:
        import nltk
        required_packages = ['punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words']
        
        for package in required_packages:
            try:
                nltk.data.find(f'tokenizers/{package}')
                logger.debug(f"NLTK package found: {package}")
            except LookupError:
                logger.info(f"Downloading NLTK package: {package}")
                nltk.download(package, quiet=True)
    except ImportError:
        logger.error("NLTK not installed. Skipping NLTK data check.")

def display_welcome_message():
    """Display welcome message with application information."""
    welcome_text = """
    ╔════════════════════════════════════════════════════════════╗
    ║                   STARDOCK PODIUM v0.1.0                   ║
    ║      AI-Powered Star Trek Podcast Episode Generator        ║
    ╚════════════════════════════════════════════════════════════╝
    
    Type 'python main.py --help' for available commands.
    
    Key features:
    • Vector-based memory for reference materials
    • Save-the-Cat story structure implementation
    • High-quality voice generation with ElevenLabs
    • Quality checking and audio post-processing
    
    Get started:
    1. Ingest reference books: python main.py ingest <path-to-epub>
    2. Register character voices: python main.py register-voice <name> <voice-id>
    3. Generate an episode: python main.py generate-episode --title "Episode Title"
    4. Generate audio: python main.py generate-audio <episode-id>
    """
    print(welcome_text)

def init_modules():
    """Initialize required modules and verify they're working."""
    try:
        # Import and initialize modules
        from mem0_client import Mem0Client
        from epub_processor import EPUBProcessor
        from voice_registry import VoiceRegistry
        
        # Test mem0 connection
        mem0 = Mem0Client()
        if mem0.test_connection():
            logger.info("Successfully connected to Mem0 vector database")
        else:
            logger.warning("Could not connect to Mem0. Vector memory features may not work properly.")
        
        # Initialize voice registry
        voice_registry = VoiceRegistry()
        voice_count = len(voice_registry.list_voices())
        logger.info(f"Voice registry initialized with {voice_count} voices")
        
        return True
    except Exception as e:
        logger.exception(f"Error initializing modules: {e}")
        return False

def main():
    """Main entry point for the application."""
    # Display welcome message
    display_welcome_message()
    
    # Check environment
    logger.info("Checking environment...")
    if not check_environment():
        logger.error("Environment check failed. Please fix the issues and try again.")
        return 1
    
    # Create directories
    logger.info("Creating necessary directories...")
    create_default_directories()
    
    # Check NLTK data
    logger.info("Checking NLTK data...")
    check_nltk_data()
    
    # Initialize modules
    logger.info("Initializing modules...")
    if not init_modules():
        logger.warning("Some modules failed to initialize. Functionality may be limited.")
    
    # Import CLI entrypoint and run it
    try:
        from cli_entrypoint import main as cli_main
        logger.info("Starting CLI...")
        return cli_main()
    except Exception as e:
        logger.exception(f"Error executing CLI: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="mem0_client.py">
#!/usr/bin/env python
"""
Mem0 Client Module for Stardock Podium.

Provides a wrapper around the Mem0 vector database API for storing and retrieving
episode memory, character information, and reference materials.
"""

import os
import json
import logging
import time
from typing import Dict, List, Any, Optional, Union
from pathlib import Path
import uuid

# Import Mem0 SDK
try:
    from mem0 import Memory, MemoryClient
except ImportError:
    logging.error("Mem0 SDK not found. Please install it with: pip install mem0")
    raise

# Setup logging
logger = logging.getLogger(__name__)

class Mem0Client:
    """Client for interacting with Mem0 vector database.
    
    This class handles the connection to Mem0 and provides methods for
    storing and retrieving different types of data:
    - Episode memory (plots, characters, events)
    - Reference materials (from ingested books)
    - Character information
    - Voice metadata
    """
    
    # Constants for memory types/categories
    REFERENCE_MATERIAL = "reference_material"
    EPISODE_MEMORY = "episode_memory"
    CHARACTER_INFO = "character_info"
    VOICE_METADATA = "voice_metadata"
    STORY_STRUCTURE = "story_structure"
    
    def __init__(self, api_key: Optional[str] = None, config_path: Optional[str] = None):
        """Initialize the Mem0 client with API key or config.
        
        Args:
            api_key: Optional Mem0 API key (if not provided, will try to load from config or env)
            config_path: Optional path to a configuration file
        """
        self.api_key = api_key or os.environ.get("MEM0_API_KEY")
        self.config_path = config_path or "data/mem0_config.json"
        self.config = self._load_config()
        
        self._initialize_client()
        
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from file or create default."""
        config_file = Path(self.config_path)
        
        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load config: {e}")
        
        # Default configuration
        default_config = {
            "version": "v1.1",
            "embedder": {
                "provider": "openai",
                "config": {
                    "model": "text-embedding-3-large"
                }
            },
            "vector_store": {
                "provider": "qdrant",
                "config": {
                    "collection_name": "stardock_podium",
                    "embedding_model_dims": 3072,
                }
            },
            "llm": {
                "provider": "openai",
                "config": {
                    "model": "gpt-4o",
                    "temperature": 0.1,
                    "max_tokens": 2000,
                }
            }
        }
        
        # Ensure directory exists
        config_file.parent.mkdir(exist_ok=True, parents=True)
        
        # Save default config
        with open(config_file, 'w') as f:
            json.dump(default_config, f, indent=2)
        
        return default_config
    
    def _initialize_client(self):
        """Initialize the Mem0 client with the current configuration."""
        try:
            if self.api_key:
                # Use managed platform with API key
                os.environ["MEM0_API_KEY"] = self.api_key
                self.client = MemoryClient()
                logger.info("Initialized Mem0 client using API key")
            else:
                # Use local configuration
                self.memory = Memory.from_config(self.config)
                logger.info("Initialized Mem0 client using local configuration")
        except Exception as e:
            logger.error(f"Failed to initialize Mem0 client: {e}")
            raise
    
    def add_memory(self, content: str, user_id: str, memory_type: str, 
                   metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Add a memory to the database.
        
        Args:
            content: The text content to store
            user_id: The user ID (used for namespacing)
            memory_type: The type/category of memory
            metadata: Optional metadata to store with the memory
        
        Returns:
            Dict with memory ID and status
        """
        if not metadata:
            metadata = {}
        
        # Ensure memory_type is in metadata for filtering
        metadata["memory_type"] = memory_type
        
        try:
            if hasattr(self, 'client'):
                # Using managed platform
                result = self.client.add(content, user_id=user_id, metadata=metadata)
            else:
                # Using local memory
                result = self.memory.add(content, user_id=user_id, metadata=metadata)
            
            logger.debug(f"Added memory: {result}")
            return result
        except Exception as e:
            logger.error(f"Failed to add memory: {e}")
            raise
    
    def search_memory(self, query: str, user_id: str, memory_type: Optional[str] = None, 
                     limit: int = 5) -> List[Dict[str, Any]]:
        """Search memories based on a query string.
        
        Args:
            query: The search query
            user_id: The user ID to search within
            memory_type: Optional memory type to filter by
            limit: Maximum number of results to return
        
        Returns:
            List of memory objects matching the query
        """
        try:
            filters = None
            if memory_type:
                filters = {"memory_type": memory_type}
            
            if hasattr(self, 'client'):
                # Using managed platform
                results = self.client.search(query, user_id=user_id, 
                                           metadata=filters, limit=limit)
            else:
                # Using local memory
                search_results = self.memory.search(query, user_id=user_id, limit=limit)
                
                # Filter by memory_type if specified
                if memory_type and 'results' in search_results:
                    results = [r for r in search_results['results'] 
                              if r.get('metadata', {}).get('memory_type') == memory_type]
                else:
                    results = search_results.get('results', [])
            
            logger.debug(f"Search returned {len(results)} results")
            return results
        except Exception as e:
            logger.error(f"Failed to search memory: {e}")
            return []
    
    def get_all_memories(self, user_id: str, memory_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get all memories for a user, optionally filtered by type.
        
        Args:
            user_id: The user ID to retrieve memories for
            memory_type: Optional memory type to filter by
        
        Returns:
            List of memory objects
        """
        try:
            if hasattr(self, 'client'):
                # Using managed platform
                if memory_type:
                    filters = {
                        "AND": [
                            {"user_id": user_id},
                            {"metadata": {"memory_type": memory_type}}
                        ]
                    }
                    results = self.client.get_all(version="v2", filters=filters)
                else:
                    results = self.client.get_all(user_id=user_id)
            else:
                # Using local memory
                all_memories = self.memory.get_all(user_id=user_id)
                
                # Filter by memory_type if specified
                if memory_type and 'results' in all_memories:
                    results = [r for r in all_memories['results'] 
                              if r.get('metadata', {}).get('memory_type') == memory_type]
                else:
                    results = all_memories.get('results', [])
            
            logger.debug(f"Retrieved {len(results)} memories")
            return results
        except Exception as e:
            logger.error(f"Failed to get memories: {e}")
            return []
    
    def add_reference_material(self, content: str, source: str, 
                              metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Add reference material from ingested books.
        
        Args:
            content: The text content to store
            source: Source identifier (book ID, title, etc.)
            metadata: Additional metadata about the content
        
        Returns:
            Dict with memory ID and status
        """
        if not metadata:
            metadata = {}
        
        metadata.update({
            "source": source,
            "added_at": time.time()
        })
        
        return self.add_memory(
            content=content,
            user_id="reference_materials",
            memory_type=self.REFERENCE_MATERIAL,
            metadata=metadata
        )
    
    def add_episode_memory(self, content: str, episode_id: str, 
                          metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Add episode memory (plot points, events, character development).
        
        Args:
            content: The memory content
            episode_id: Episode identifier
            metadata: Additional metadata
        
        Returns:
            Dict with memory ID and status
        """
        if not metadata:
            metadata = {}
        
        metadata.update({
            "episode_id": episode_id,
            "added_at": time.time()
        })
        
        return self.add_memory(
            content=content,
            user_id="episodes",
            memory_type=self.EPISODE_MEMORY,
            metadata=metadata
        )
    
    def add_character_info(self, character_name: str, info: str, 
                          metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Add or update character information.
        
        Args:
            character_name: Name of the character
            info: Character information
            metadata: Additional metadata
        
        Returns:
            Dict with memory ID and status
        """
        if not metadata:
            metadata = {}
        
        metadata.update({
            "character_name": character_name,
            "updated_at": time.time()
        })
        
        return self.add_memory(
            content=info,
            user_id="characters",
            memory_type=self.CHARACTER_INFO,
            metadata=metadata
        )
    
    def search_reference_materials(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Search reference materials based on semantic similarity.
        
        Args:
            query: The search query
            limit: Maximum number of results to return
        
        Returns:
            List of matching reference materials
        """
        return self.search_memory(
            query=query,
            user_id="reference_materials",
            memory_type=self.REFERENCE_MATERIAL,
            limit=limit
        )
    
    def search_episode_memories(self, query: str, episode_id: Optional[str] = None, 
                              limit: int = 5) -> List[Dict[str, Any]]:
        """Search episode memories.
        
        Args:
            query: The search query
            episode_id: Optional episode ID to filter by
            limit: Maximum number of results to return
        
        Returns:
            List of matching episode memories
        """
        results = self.search_memory(
            query=query,
            user_id="episodes",
            memory_type=self.EPISODE_MEMORY,
            limit=limit
        )
        
        # Filter by episode ID if specified
        if episode_id:
            results = [r for r in results 
                      if r.get('metadata', {}).get('episode_id') == episode_id]
        
        return results
    
    def search_character_info(self, query: str, character_name: Optional[str] = None, 
                             limit: int = 5) -> List[Dict[str, Any]]:
        """Search character information.
        
        Args:
            query: The search query
            character_name: Optional character name to filter by
            limit: Maximum number of results to return
        
        Returns:
            List of matching character information
        """
        results = self.search_memory(
            query=query,
            user_id="characters",
            memory_type=self.CHARACTER_INFO,
            limit=limit
        )
        
        # Filter by character name if specified
        if character_name:
            results = [r for r in results 
                      if r.get('metadata', {}).get('character_name') == character_name]
        
        return results
    
    def get_character_info(self, character_name: str) -> Optional[Dict[str, Any]]:
        """Get information about a specific character.
        
        Args:
            character_name: Name of the character
        
        Returns:
            Character information or None if not found
        """
        results = self.get_all_memories(
            user_id="characters",
            memory_type=self.CHARACTER_INFO
        )
        
        for result in results:
            if result.get('metadata', {}).get('character_name') == character_name:
                return result
        
        return None
    
    def delete_memory(self, memory_id: str) -> bool:
        """Delete a specific memory by ID.
        
        Args:
            memory_id: ID of the memory to delete
        
        Returns:
            True if successful, False otherwise
        """
        try:
            if hasattr(self, 'client'):
                # Using managed platform
                self.client.delete(memory_id)
            else:
                # Using local memory
                self.memory.delete(memory_id)
            
            logger.debug(f"Deleted memory: {memory_id}")
            return True
        except Exception as e:
            logger.error(f"Failed to delete memory {memory_id}: {e}")
            return False
    
    def add_story_structure(self, structure_data: Dict[str, Any], 
                           episode_id: Optional[str] = None) -> Dict[str, Any]:
        """Add story structure information.
        
        Args:
            structure_data: Story structure data (dict converted to JSON)
            episode_id: Optional episode identifier
        
        Returns:
            Dict with memory ID and status
        """
        metadata = {
            "added_at": time.time()
        }
        
        if episode_id:
            metadata["episode_id"] = episode_id
        
        # Convert dict to JSON string for storage
        content = json.dumps(structure_data)
        
        return self.add_memory(
            content=content,
            user_id="story_structures",
            memory_type=self.STORY_STRUCTURE,
            metadata=metadata
        )
    
    def get_story_structure(self, episode_id: str) -> Optional[Dict[str, Any]]:
        """Get story structure for a specific episode.
        
        Args:
            episode_id: Episode identifier
        
        Returns:
            Story structure data or None if not found
        """
        results = self.get_all_memories(
            user_id="story_structures",
            memory_type=self.STORY_STRUCTURE
        )
        
        for result in results:
            if result.get('metadata', {}).get('episode_id') == episode_id:
                try:
                    # Parse JSON string back to dict
                    return json.loads(result['memory'])
                except Exception as e:
                    logger.error(f"Failed to parse story structure data: {e}")
                    return None
        
        return None
    
    def batch_add_memories(self, memories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add multiple memories in a batch operation.
        
        Args:
            memories: List of memory objects with fields:
                - content: The memory content
                - user_id: The user ID
                - memory_type: The type of memory
                - metadata: Optional metadata
        
        Returns:
            List of results for each memory added
        """
        results = []
        
        for memory in memories:
            try:
                result = self.add_memory(
                    content=memory['content'],
                    user_id=memory['user_id'],
                    memory_type=memory['memory_type'],
                    metadata=memory.get('metadata', {})
                )
                results.append(result)
            except Exception as e:
                logger.error(f"Failed to add memory in batch: {e}")
                results.append({"error": str(e)})
        
        return results

# Singleton instance
_mem0_client = None

def get_mem0_client() -> Mem0Client:
    """Get the Mem0Client singleton instance."""
    global _mem0_client
    
    if _mem0_client is None:
        _mem0_client = Mem0Client()
    
    return _mem0_client
</file>

<file path="quality_checker.py">
#!/usr/bin/env python
"""
Quality Checker Module for Stardock Podium.

This module verifies the quality of generated episodes, both in terms
of script content and audio production, flagging issues for improvement.
"""

import os
import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import uuid

# Try to import required libraries
try:
    import ffmpeg
except ImportError:
    logging.error("ffmpeg-python not found. Please install it with: pip install ffmpeg-python")
    raise

try:
    from openai import OpenAI
except ImportError:
    logging.error("OpenAI not found. Please install it with: pip install openai")
    raise

# Local imports
from story_structure import get_episode
from script_editor import load_episode_script
from episode_metadata import update_metadata
from episode_memory import get_episode_memory

# Setup logging
logger = logging.getLogger(__name__)

class QualityChecker:
    """Quality verification for episodes and audio."""
    
    def __init__(self, episodes_dir: str = "episodes"):
        """Initialize the quality checker.
        
        Args:
            episodes_dir: Directory containing episode data
        """
        self.episodes_dir = Path(episodes_dir)
        
        # Initialize OpenAI client for content checking
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not found in environment variables")
        
        self.client = OpenAI(api_key=api_key)
        
        # Initialize episode memory
        self.episode_memory = get_episode_memory()
    
    def check_episode_quality(self, episode_id: str, 
                            check_options: Dict[str, bool] = None) -> Dict[str, Any]:
        """Check the quality of an episode.
        
        Args:
            episode_id: ID of the episode
            check_options: Options for what to check
        
        Returns:
            Dictionary with quality check results
        """
        if check_options is None:
            check_options = {
                "check_script": True,
                "check_audio": True
            }
        
        # Get episode data
        episode = get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {"error": f"Episode not found: {episode_id}"}
        
        # Create results structure
        results = {
            "episode_id": episode_id,
            "title": episode.get("title", "Unknown"),
            "check_time": time.time(),
            "script_quality": None,
            "audio_quality": None,
            "overall_quality": None,
            "issues": [],
            "recommendations": []
        }
        
        # Check script quality if requested
        if check_options.get("check_script", True):
            script_results = self._check_script_quality(episode_id)
            results["script_quality"] = script_results
            
            # Add script issues to the main issues list
            if "issues" in script_results:
                for issue in script_results["issues"]:
                    results["issues"].append({
                        "type": "script",
                        "severity": issue.get("severity", "warning"),
                        "description": issue.get("description", "Unknown issue"),
                        "location": issue.get("location")
                    })
            
            # Add script recommendations
            if "recommendations" in script_results:
                results["recommendations"].extend(script_results["recommendations"])
        
        # Check audio quality if requested
        if check_options.get("check_audio", True):
            audio_results = self._check_audio_quality(episode_id)
            results["audio_quality"] = audio_results
            
            # Add audio issues to the main issues list
            if "issues" in audio_results:
                for issue in audio_results["issues"]:
                    results["issues"].append({
                        "type": "audio",
                        "severity": issue.get("severity", "warning"),
                        "description": issue.get("description", "Unknown issue"),
                        "location": issue.get("location")
                    })
            
            # Add audio recommendations
            if "recommendations" in audio_results:
                results["recommendations"].extend(audio_results["recommendations"])
        
        # Determine overall quality
        if results["script_quality"] and results["audio_quality"]:
            # Average of script and audio quality
            script_score = results["script_quality"].get("score", 0)
            audio_score = results["audio_quality"].get("score", 0)
            
            results["overall_quality"] = {
                "score": (script_score + audio_score) / 2,
                "grade": self._score_to_grade((script_score + audio_score) / 2)
            }
        elif results["script_quality"]:
            # Only script quality available
            results["overall_quality"] = {
                "score": results["script_quality"].get("score", 0),
                "grade": results["script_quality"].get("grade", "N/A")
            }
        elif results["audio_quality"]:
            # Only audio quality available
            results["overall_quality"] = {
                "score": results["audio_quality"].get("score", 0),
                "grade": results["audio_quality"].get("grade", "N/A")
            }
        
        # Save quality check results
        self._save_quality_check(episode_id, results)
        
        # Update episode metadata with quality info
        metadata_update = {
            "quality_check": {
                "checked_at": results["check_time"],
                "overall_grade": results["overall_quality"]["grade"] if results["overall_quality"] else "N/A",
                "issue_count": len(results["issues"])
            }
        }
        
        update_metadata(episode_id, metadata_update)
        
        return results
    
    def _check_script_quality(self, episode_id: str) -> Dict[str, Any]:
        """Check the quality of an episode script.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Dictionary with script quality check results
        """
        # Load episode and script
        episode = get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {"error": f"Episode not found: {episode_id}"}
        
        script = load_episode_script(episode_id)
        if not script:
            logger.error(f"Script not found for episode: {episode_id}")
            return {"error": f"Script not found: {episode_id}"}
        
        # Initialize results
        results = {
            "issues": [],
            "score": 0.0,
            "grade": "N/A",
            "recommendations": []
        }
        
        # Check overall script structure
        structure_issues = self._check_script_structure(script, episode)
        results["issues"].extend(structure_issues)
        
        # Check for continuity with previous episodes
        continuity_issues = self._check_continuity(episode_id, script)
        results["issues"].extend(continuity_issues)
        
        # Check dialogue quality
        dialogue_issues = self._check_dialogue_quality(script)
        results["issues"].extend(dialogue_issues)
        
        # Check pacing
        pacing_issues = self._check_pacing(script)
        results["issues"].extend(pacing_issues)
        
        # Generate AI evaluation for overall quality
        ai_evaluation = self._evaluate_script_with_ai(script, episode)
        
        if "score" in ai_evaluation:
            results["score"] = ai_evaluation["score"]
            results["grade"] = self._score_to_grade(ai_evaluation["score"])
        
        if "issues" in ai_evaluation:
            results["issues"].extend(ai_evaluation["issues"])
        
        if "recommendations" in ai_evaluation:
            results["recommendations"] = ai_evaluation["recommendations"]
        
        # Sort issues by severity
        results["issues"].sort(key=lambda x: self._severity_to_value(x.get("severity", "warning")), reverse=True)
        
        return results
    
    def _check_script_structure(self, script: Dict[str, Any], 
                              episode: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check the structure of a script against Save the Cat beats.
        
        Args:
            script: Script data
            episode: Episode data
        
        Returns:
            List of structure issues
        """
        issues = []
        
        # Get beat sheet
        beats = episode.get("beats", [])
        if not beats:
            issues.append({
                "severity": "warning",
                "description": "Episode is missing beat sheet structure",
                "location": None
            })
            return issues
        
        # Get scenes
        scenes = script.get("scenes", [])
        if not scenes:
            issues.append({
                "severity": "error",
                "description": "Script has no scenes",
                "location": None
            })
            return issues
        
        # Check that each beat has at least one scene
        beat_coverage = {beat["name"]: 0 for beat in beats}
        
        for scene in scenes:
            beat = scene.get("beat")
            if beat in beat_coverage:
                beat_coverage[beat] += 1
        
        # Report missing beats
        for beat, count in beat_coverage.items():
            if count == 0:
                issues.append({
                    "severity": "warning",
                    "description": f"Beat '{beat}' has no corresponding scenes",
                    "location": None
                })
        
        # Check for proper beat sequence
        beat_names = [beat["name"] for beat in beats]
        scene_beats = [scene.get("beat") for scene in scenes if scene.get("beat") in beat_names]
        
        if scene_beats:
            # Get the order of beats as they appear in scenes
            actual_beat_order = []
            for beat in scene_beats:
                if beat not in actual_beat_order:
                    actual_beat_order.append(beat)
            
            # Compare to expected order
            for i, beat in enumerate(actual_beat_order):
                expected_index = beat_names.index(beat)
                
                # Check if this beat appears out of order
                if i > 0 and expected_index < beat_names.index(actual_beat_order[i-1]):
                    issues.append({
                        "severity": "warning",
                        "description": f"Beat '{beat}' appears out of sequence in the script",
                        "location": f"After scene with beat '{actual_beat_order[i-1]}'"
                    })
        
        return issues
    
    def _check_continuity(self, episode_id: str, script: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check script for continuity with previous episodes.
        
        Args:
            episode_id: ID of the episode
            script: Script data
        
        Returns:
            List of continuity issues
        """
        issues = []
        
        # Get episode data
        episode = get_episode(episode_id)
        if not episode:
            return issues
        
        # Get episode number and series
        episode_number = episode.get("episode_number", 0)
        series = episode.get("series", "")
        
        # Skip continuity check for first episode in a series
        if episode_number <= 1:
            return issues
        
        # Get references to previous episodes
        timeline = self.episode_memory.get_timeline()
        
        # Find characters from previous episodes
        previous_characters = set()
        
        for ep_id, events in timeline.items():
            # Skip the current episode
            if ep_id == episode_id:
                continue
            
            # Check if this is from the same series
            ep_episode = get_episode(ep_id)
            if not ep_episode or ep_episode.get("series") != series:
                continue
            
            # Check if this is a previous episode
            if ep_episode.get("episode_number", 0) >= episode_number:
                continue
            
            # Get character names from events
            for event in events:
                if "character" in event.get("metadata", {}):
                    previous_characters.add(event["metadata"]["character"])
                
                # Also check for characters in relationships
                if "characters" in event.get("metadata", {}):
                    previous_characters.update(event["metadata"]["characters"])
        
        # Check if script has references to previous characters
        current_characters = set()
        character_references = {}
        
        for scene in script.get("scenes", []):
            for line in scene.get("lines", []):
                if line.get("type") == "dialogue":
                    character = line.get("character")
                    if character:
                        current_characters.add(character)
                
                # Check content for character references
                content = line.get("content", "")
                for character in previous_characters:
                    if re.search(r'\b' + re.escape(character) + r'\b', content):
                        if character not in character_references:
                            character_references[character] = 0
                        character_references[character] += 1
        
        # Check for previous significant characters not appearing in this episode
        missing_characters = previous_characters - current_characters
        
        # Only consider it an issue if the character was referenced but doesn't appear
        for character in missing_characters:
            if character in character_references and character_references[character] > 0:
                issues.append({
                    "severity": "info",
                    "description": f"Character '{character}' from previous episodes is referenced but doesn't appear",
                    "location": None
                })
        
        # Check for inconsistencies with previous episode memories
        # Get relevant memories
        memories = self.episode_memory.search_memories(
            query=episode.get("title", ""),
            category=self.episode_memory.CONTINUITY,
            limit=10
        )
        
        for memory in memories:
            memory_text = memory.get("memory", "")
            
            # Check for potential contradictions
            for scene in script.get("scenes", []):
                for line in scene.get("lines", []):
                    content = line.get("content", "")
                    
                    # This is a simplified check - would need NLP for better contradiction detection
                    if content and len(content) > 20 and self._might_contradict(content, memory_text):
                        issues.append({
                            "severity": "warning",
                            "description": f"Possible continuity contradiction with earlier episode",
                            "location": f"Scene {scene.get('scene_number')}, line type {line.get('type')}"
                        })
        
        return issues
    
    def _might_contradict(self, text_a: str, text_b: str) -> bool:
        """Simple check if two texts might contradict each other.
        
        Args:
            text_a: First text
            text_b: Second text
        
        Returns:
            True if contradiction is possible
        """
        # This is a very simplified check
        # Would need NLP/AI for better contradiction detection
        
        # Check if both texts contain the same named entities but with different verbs
        # This is prone to false positives, but it's a starting point
        
        # Extract potential entity names (capitalized words)
        entities_a = set(re.findall(r'\b[A-Z][a-z]+\b', text_a))
        entities_b = set(re.findall(r'\b[A-Z][a-z]+\b', text_b))
        
        # Find common entities
        common_entities = entities_a.intersection(entities_b)
        
        if not common_entities:
            return False
        
        # Check for negations around common entities
        negation_words = ['not', 'never', 'no', "didn't", "doesn't", "isn't", "wasn't", "couldn't"]
        
        for entity in common_entities:
            # Check for negations in context of this entity
            context_a = self._get_context(text_a, entity, window=3)
            context_b = self._get_context(text_b, entity, window=3)
            
            # If one has negation and the other doesn't for the same entity
            has_negation_a = any(neg in context_a for neg in negation_words)
            has_negation_b = any(neg in context_b for neg in negation_words)
            
            if has_negation_a != has_negation_b:
                return True
        
        return False
    
    def _get_context(self, text: str, keyword: str, window: int = 3) -> str:
        """Get context around a keyword in text.
        
        Args:
            text: Text to search
            keyword: Keyword to find
            window: Number of words for context on each side
        
        Returns:
            Context string
        """
        words = text.split()
        if keyword not in words:
            return ""
        
        idx = words.index(keyword)
        start = max(0, idx - window)
        end = min(len(words), idx + window + 1)
        
        return " ".join(words[start:end])
    
    def _check_dialogue_quality(self, script: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check the quality of dialogue in the script.
        
        Args:
            script: Script data
        
        Returns:
            List of dialogue issues
        """
        issues = []
        
        # Get scenes
        scenes = script.get("scenes", [])
        if not scenes:
            return issues
        
        # Track dialogue lines per character
        character_lines = {}
        
        # Track repetitive phrases
        phrase_counts = {}
        
        for scene_idx, scene in enumerate(scenes):
            scene_number = scene.get("scene_number", scene_idx + 1)
            
            # Check dialogue in this scene
            for line_idx, line in enumerate(scene.get("lines", [])):
                if line.get("type") != "dialogue":
                    continue
                
                character = line.get("character", "")
                content = line.get("content", "")
                
                # Count character lines
                if character not in character_lines:
                    character_lines[character] = 0
                character_lines[character] += 1
                
                # Check for very short or very long dialogue
                if len(content) < 10:
                    issues.append({
                        "severity": "info",
                        "description": f"Very short dialogue line for {character}",
                        "location": f"Scene {scene_number}, line {line_idx + 1}"
                    })
                elif len(content) > 200:
                    issues.append({
                        "severity": "warning",
                        "description": f"Very long dialogue line for {character}",
                        "location": f"Scene {scene_number}, line {line_idx + 1}"
                    })
                
                # Check for repetitive phrases
                words = content.split()
                if len(words) >= 3:
                    for i in range(len(words) - 2):
                        phrase = " ".join(words[i:i+3])
                        phrase = phrase.lower()
                        
                        if phrase not in phrase_counts:
                            phrase_counts[phrase] = []
                        
                        phrase_counts[phrase].append({
                            "scene_number": scene_number,
                            "line_index": line_idx,
                            "character": character
                        })
        
        # Check for disproportionate dialogue
        total_lines = sum(character_lines.values())
        if total_lines > 0:
            for character, count in character_lines.items():
                # If a character has more than 50% of all dialogue
                if count > total_lines * 0.5:
                    issues.append({
                        "severity": "warning",
                        "description": f"Character '{character}' has disproportionate dialogue ({count} lines, {count/total_lines*100:.1f}% of total)",
                        "location": None
                    })
                # If a character has only one line
                elif count == 1:
                    issues.append({
                        "severity": "info",
                        "description": f"Character '{character}' has only one line in the script",
                        "location": None
                    })
        
        # Check for repetitive phrases
        for phrase, occurrences in phrase_counts.items():
            if len(occurrences) >= 3:
                # Only report if same character uses the phrase multiple times
                character_counts = {}
                for occurrence in occurrences:
                    character = occurrence["character"]
                    if character not in character_counts:
                        character_counts[character] = 0
                    character_counts[character] += 1
                
                for character, count in character_counts.items():
                    if count >= 3:
                        issues.append({
                            "severity": "info",
                            "description": f"Character '{character}' repeats phrase '{phrase}' {count} times",
                            "location": f"First occurrence: Scene {occurrences[0]['scene_number']}, line {occurrences[0]['line_index'] + 1}"
                        })
        
        return issues
    
    def _check_pacing(self, script: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Check the pacing of the script.
        
        Args:
            script: Script data
        
        Returns:
            List of pacing issues
        """
        issues = []
        
        # Get scenes
        scenes = script.get("scenes", [])
        if not scenes:
            return issues
        
        # Check scene length distribution
        scene_lengths = [len(scene.get("lines", [])) for scene in scenes]
        
        if scene_lengths:
            avg_length = sum(scene_lengths) / len(scene_lengths)
            
            # Check for very short scenes
            for i, length in enumerate(scene_lengths):
                if length <= 2:
                    issues.append({
                        "severity": "info",
                        "description": f"Very short scene with only {length} lines",
                        "location": f"Scene {scenes[i].get('scene_number', i + 1)}"
                    })
            
            # Check for very long scenes
            for i, length in enumerate(scene_lengths):
                if length >= avg_length * 2:
                    issues.append({
                        "severity": "warning",
                        "description": f"Very long scene with {length} lines (average is {avg_length:.1f})",
                        "location": f"Scene {scenes[i].get('scene_number', i + 1)}"
                    })
        
        # Check for long dialogue stretches without action or sound effects
        for scene_idx, scene in enumerate(scenes):
            scene_number = scene.get("scene_number", scene_idx + 1)
            
            dialogue_stretch = 0
            last_non_dialogue = 0
            
            for line_idx, line in enumerate(scene.get("lines", [])):
                if line.get("type") == "dialogue":
                    dialogue_stretch += 1
                else:
                    # Reset counter if we hit a non-dialogue line
                    if dialogue_stretch >= 6:
                        issues.append({
                            "severity": "info",
                            "description": f"Long stretch of dialogue ({dialogue_stretch} lines) without action or sound effects",
                            "location": f"Scene {scene_number}, lines {last_non_dialogue + 1}-{line_idx}"
                        })
                    
                    dialogue_stretch = 0
                    last_non_dialogue = line_idx
            
            # Check end of scene
            if dialogue_stretch >= 6:
                issues.append({
                    "severity": "info",
                    "description": f"Long stretch of dialogue ({dialogue_stretch} lines) without action or sound effects",
                    "location": f"Scene {scene_number}, at end of scene"
                })
        
        return issues
    
    def _evaluate_script_with_ai(self, script: Dict[str, Any], 
                               episode: Dict[str, Any]) -> Dict[str, Any]:
        """Use AI to evaluate the script quality.
        
        Args:
            script: Script data
            episode: Episode data
        
        Returns:
            Dictionary with AI evaluation results
        """
        if not self.client:
            logger.error("OpenAI client not initialized")
            return {}
        
        try:
            # Create a simplified version of the script for evaluation
            simplified_script = self._simplify_script_for_evaluation(script)
            
            # Create prompt for evaluation
            prompt = f"""
            You are a professional script evaluator for podcast episodes. Please evaluate the following
            Star Trek-style podcast episode script and rate its quality.
            
            EPISODE INFORMATION:
            Title: {episode.get('title', 'Unknown')}
            Theme: {episode.get('theme', 'Not specified')}
            
            SCRIPT:
            {simplified_script}
            
            Please provide:
            1. A quality score from 0 to 10 (where 10 is excellent)
            2. A list of 1-3 significant issues with the script, if any
            3. 1-3 specific recommendations for improving the script
            
            Format your response as JSON with the following structure:
            {{
                "score": <score>,
                "issues": [
                    {{"severity": "<error|warning|info>", "description": "<issue description>", "location": "<location in script>"}}
                ],
                "recommendations": [
                    "<recommendation>"
                ]
            }}
            """
            
            # Query the AI
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a professional script evaluator. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=1000
            )
            
            # Parse the result
            try:
                result_json = json.loads(response.choices[0].message.content)
                return result_json
            except json.JSONDecodeError:
                logger.error("Failed to parse AI evaluation result as JSON")
                # Try to extract score from text
                text = response.choices[0].message.content
                score_match = re.search(r'score[\":\s]+(\d+(?:\.\d+)?)', text)
                
                if score_match:
                    score = float(score_match.group(1))
                    return {"score": score}
                
                return {}
        
        except Exception as e:
            logger.error(f"Error evaluating script with AI: {e}")
            return {}
    
    def _simplify_script_for_evaluation(self, script: Dict[str, Any]) -> str:
        """Create a simplified version of the script for AI evaluation.
        
        Args:
            script: Script data
        
        Returns:
            Simplified script text
        """
        simplified = ""
        
        for scene_idx, scene in enumerate(script.get("scenes", [])):
            scene_number = scene.get("scene_number", scene_idx + 1)
            beat = scene.get("beat", "")
            setting = scene.get("setting", "")
            
            simplified += f"SCENE {scene_number}: {beat}\n"
            simplified += f"SETTING: {setting}\n\n"
            
            for line in scene.get("lines", []):
                line_type = line.get("type", "")
                content = line.get("content", "")
                
                if line_type == "dialogue":
                    character = line.get("character", "")
                    simplified += f"{character}: {content}\n\n"
                elif line_type == "narration":
                    simplified += f"NARRATOR: {content}\n\n"
                elif line_type == "sound_effect":
                    simplified += f"(SOUND: {content})\n\n"
                elif line_type == "description":
                    simplified += f"[DESCRIPTION: {content}]\n\n"
            
            simplified += "---\n\n"
        
        return simplified
    
    def _check_audio_quality(self, episode_id: str) -> Dict[str, Any]:
        """Check the quality of episode audio.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Dictionary with audio quality check results
        """
        # Get episode data
        episode = get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {"error": f"Episode not found: {episode_id}"}
        
        # Check if episode has audio
        audio_path = episode.get("audio", {}).get("file_path")
        if not audio_path:
            logger.error(f"Episode has no audio: {episode_id}")
            return {"error": f"Episode has no audio: {episode_id}"}
        
        # Initialize results
        results = {
            "issues": [],
            "score": 0.0,
            "grade": "N/A",
            "recommendations": []
        }
        
        # Check audio file integrity
        integrity_issues = self._check_audio_integrity(audio_path)
        results["issues"].extend(integrity_issues)
        
        # Check audio properties
        property_issues = self._check_audio_properties(audio_path)
        results["issues"].extend(property_issues)
        
        # Analyze scene audio files
        audio_dir = Path(audio_path).parent
        scene_issues = self._check_scene_audio(audio_dir)
        results["issues"].extend(scene_issues)
        
        # Calculate score based on issues
        if results["issues"]:
            # Count by severity
            error_count = sum(1 for issue in results["issues"] if issue.get("severity") == "error")
            warning_count = sum(1 for issue in results["issues"] if issue.get("severity") == "warning")
            info_count = sum(1 for issue in results["issues"] if issue.get("severity") == "info")
            
            # Calculate weighted score
            total_issues = len(results["issues"])
            weighted_total = error_count * 5 + warning_count * 2 + info_count
            
            # Base score of 10, reduced by weighted issues
            raw_score = 10 - (weighted_total / max(1, total_issues * 2))
            results["score"] = max(0, min(10, raw_score))
        else:
            # No issues found
            results["score"] = 10.0
        
        results["grade"] = self._score_to_grade(results["score"])
        
        # Generate recommendations based on issues
        results["recommendations"] = self._generate_audio_recommendations(results["issues"])
        
        return results
    
    def _check_audio_integrity(self, audio_path: str) -> List[Dict[str, Any]]:
        """Check the integrity of an audio file.
        
        Args:
            audio_path: Path to the audio file
        
        Returns:
            List of integrity issues
        """
        issues = []
        
        # Check if file exists
        if not os.path.exists(audio_path):
            issues.append({
                "severity": "error",
                "description": "Audio file does not exist",
                "location": audio_path
            })
            return issues
        
        try:
            # Use ffmpeg to check file integrity
            probe = ffmpeg.probe(audio_path)
            
            # Check for audio streams
            audio_streams = [stream for stream in probe.get("streams", []) 
                           if stream.get("codec_type") == "audio"]
            
            if not audio_streams:
                issues.append({
                    "severity": "error",
                    "description": "Audio file contains no audio streams",
                    "location": audio_path
                })
            
            # Check duration
            duration = float(probe.get("format", {}).get("duration", 0))
            
            if duration < 30:
                issues.append({
                    "severity": "error",
                    "description": f"Audio file is too short: {duration:.1f} seconds",
                    "location": audio_path
                })
            
            # Check if file is empty
            size = int(probe.get("format", {}).get("size", 0))
            
            if size == 0:
                issues.append({
                    "severity": "error",
                    "description": "Audio file is empty (zero bytes)",
                    "location": audio_path
                })
            
        except Exception as e:
            issues.append({
                "severity": "error",
                "description": f"Error probing audio file: {str(e)}",
                "location": audio_path
            })
        
        return issues
    
    def _check_audio_properties(self, audio_path: str) -> List[Dict[str, Any]]:
        """Check the properties of an audio file.
        
        Args:
            audio_path: Path to the audio file
        
        Returns:
            List of property issues
        """
        issues = []
        
        # Skip if file doesn't exist
        if not os.path.exists(audio_path):
            return issues
        
        try:
            # Use ffmpeg to analyze audio properties
            probe = ffmpeg.probe(audio_path)
            
            # Get first audio stream
            audio_streams = [stream for stream in probe.get("streams", []) 
                           if stream.get("codec_type") == "audio"]
            
            if not audio_streams:
                return issues
            
            audio_stream = audio_streams[0]
            
            # Check codec
            codec = audio_stream.get("codec_name", "")
            if codec not in ["mp3", "aac", "opus"]:
                issues.append({
                    "severity": "warning",
                    "description": f"Non-standard audio codec: {codec}",
                    "location": audio_path
                })
            
            # Check sample rate
            sample_rate = int(audio_stream.get("sample_rate", 0))
            if sample_rate < 44100:
                issues.append({
                    "severity": "warning",
                    "description": f"Low sample rate: {sample_rate} Hz",
                    "location": audio_path
                })
            
            # Check channel count
            channels = int(audio_stream.get("channels", 0))
            if channels != 2:
                issues.append({
                    "severity": "info",
                    "description": f"Non-stereo audio: {channels} channels",
                    "location": audio_path
                })
            
            # Check bit rate
            bit_rate = int(probe.get("format", {}).get("bit_rate", 0))
            if bit_rate < 128000:
                issues.append({
                    "severity": "warning",
                    "description": f"Low bit rate: {bit_rate // 1000} kbps",
                    "location": audio_path
                })
            
            # Check for silent parts
            # This would require more complex analysis
            
        except Exception as e:
            issues.append({
                "severity": "warning",
                "description": f"Error analyzing audio properties: {str(e)}",
                "location": audio_path
            })
        
        return issues
    
    def _check_scene_audio(self, audio_dir: Path) -> List[Dict[str, Any]]:
        """Check audio files for individual scenes.
        
        Args:
            audio_dir: Directory containing audio files
        
        Returns:
            List of scene audio issues
        """
        issues = []
        
        # Look for scene directories
        scene_dirs = [d for d in audio_dir.glob("scene_*") if d.is_dir()]
        
        if not scene_dirs:
            issues.append({
                "severity": "info",
                "description": "No scene audio directories found",
                "location": str(audio_dir)
            })
            return issues
        
        # Check each scene directory
        for scene_dir in scene_dirs:
            scene_name = scene_dir.name
            scene_audio = scene_dir / "scene_audio.mp3"
            
            if not scene_audio.exists():
                issues.append({
                    "severity": "warning",
                    "description": f"Missing scene audio file for {scene_name}",
                    "location": str(scene_dir)
                })
                continue
            
            # Check scene audio file
            scene_issues = self._check_audio_integrity(str(scene_audio))
            
            for issue in scene_issues:
                issue["location"] = f"{scene_name}/{os.path.basename(issue['location'])}"
                issues.append(issue)
            
            # Check for temp directory with voice clips
            temp_dir = scene_dir / "temp"
            if temp_dir.exists() and temp_dir.is_dir():
                # Count voice clips
                voice_clips = list(temp_dir.glob("*.mp3"))
                
                if not voice_clips:
                    issues.append({
                        "severity": "info",
                        "description": f"No voice clips found for {scene_name}",
                        "location": str(temp_dir)
                    })
        
        return issues
    
    def _generate_audio_recommendations(self, issues: List[Dict[str, Any]]) -> List[str]:
        """Generate recommendations based on audio issues.
        
        Args:
            issues: List of audio issues
        
        Returns:
            List of recommendations
        """
        recommendations = []
        
        # Count issues by type
        integrity_issues = [i for i in issues if "integrity" in i.get("description", "").lower()]
        property_issues = [i for i in issues if any(term in i.get("description", "").lower() 
                                                 for term in ["sample rate", "bit rate", "codec", "channels"])]
        scene_issues = [i for i in issues if "scene" in i.get("location", "").lower()]
        
        # Recommendations for integrity issues
        if integrity_issues:
            recommendations.append(
                "Regenerate audio files that have integrity issues to ensure playability."
            )
        
        # Recommendations for property issues
        if property_issues:
            rate_issues = [i for i in property_issues if "rate" in i.get("description", "").lower()]
            if rate_issues:
                recommendations.append(
                    "Increase audio quality settings (sample rate, bit rate) for better sound fidelity."
                )
        
        # Recommendations for scene issues
        if scene_issues:
            missing_scenes = [i for i in scene_issues if "missing" in i.get("description", "").lower()]
            if missing_scenes:
                recommendations.append(
                    "Generate audio for all scenes to ensure complete episode coverage."
                )
        
        # Generic recommendation if none specific
        if not recommendations:
            recommendations.append(
                "Consider normalizing audio levels across all scenes for consistent volume."
            )
        
        return recommendations
    
    def _score_to_grade(self, score: float) -> str:
        """Convert a numerical score to a letter grade.
        
        Args:
            score: Numerical score (0-10)
        
        Returns:
            Letter grade
        """
        if score >= 9.5:
            return "A+"
        elif score >= 9.0:
            return "A"
        elif score >= 8.5:
            return "A-"
        elif score >= 8.0:
            return "B+"
        elif score >= 7.5:
            return "B"
        elif score >= 7.0:
            return "B-"
        elif score >= 6.5:
            return "C+"
        elif score >= 6.0:
            return "C"
        elif score >= 5.5:
            return "C-"
        elif score >= 5.0:
            return "D+"
        elif score >= 4.0:
            return "D"
        else:
            return "F"
    
    def _severity_to_value(self, severity: str) -> int:
        """Convert severity string to numerical value for sorting.
        
        Args:
            severity: Severity string
        
        Returns:
            Numerical value
        """
        if severity == "error":
            return 3
        elif severity == "warning":
            return 2
        elif severity == "info":
            return 1
        else:
            return 0
    
    def _save_quality_check(self, episode_id: str, results: Dict[str, Any]) -> None:
        """Save quality check results to file.
        
        Args:
            episode_id: ID of the episode
            results: Quality check results
        """
        episode_dir = self.episodes_dir / episode_id
        quality_file = episode_dir / "quality_check.json"
        
        try:
            with open(quality_file, 'w') as f:
                json.dump(results, f, indent=2)
            
            logger.info(f"Quality check results saved to {quality_file}")
        except Exception as e:
            logger.error(f"Error saving quality check results: {e}")

# Singleton instance
_quality_checker = None

def get_quality_checker() -> QualityChecker:
    """Get the QualityChecker singleton instance."""
    global _quality_checker
    
    if _quality_checker is None:
        _quality_checker = QualityChecker()
    
    return _quality_checker

def check_episode_quality(episode_id: str, check_options: Dict[str, bool] = None) -> Dict[str, Any]:
    """Check the quality of an episode.
    
    Args:
        episode_id: ID of the episode
        check_options: Options for what to check
    
    Returns:
        Dictionary with quality check results
    """
    checker = get_quality_checker()
    return checker.check_episode_quality(episode_id, check_options)
</file>

<file path="README.md">
# Stardock Podium

An AI-powered Star Trek podcast generator that creates original episodes in the style of classic Star Trek series.

## Overview

Stardock Podium ingests sci-fi reference materials and generates complete, Star Trek-style podcast episodes with consistent characters, continuity, and professional voice acting. The system uses advanced AI technologies to create original content that maintains the spirit and style of Star Trek while telling new stories.

## Features

- **Reference Ingestion**: Process EPUB sci-fi books to understand writing styles, character archetypes, and thematic elements
- **Vector Memory**: Store and retrieve reference materials using Mem0 vector database
- **Save-the-Cat Story Structure**: Generate well-structured episodes following proven storytelling beats
- **Character Continuity**: Maintain consistent characters across episodes with memory of prior developments
- **Voice Synthesis**: High-quality character voices using ElevenLabs API
- **Audio Production**: Complete audio pipeline with music, sound effects, and post-processing
- **Quality Checking**: Automated script and audio quality verification
- **Windows 10 Native**: Designed to run natively on Windows 10 without Docker or WSL

## Installation

### Prerequisites

- Windows 10
- Python 3.8+
- FFmpeg (added to PATH)
- API keys for:
  - OpenAI or OpenRouter
  - ElevenLabs
  - Mem0

### Setup

1. Clone this repository:
   ```
   git clone https://github.com/your-username/stardock-podium.git
   cd stardock-podium
   ```

2. Install required Python packages:
   ```
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   ```
   set OPENAI_API_KEY=your_openai_key
   set OPENROUTER_API_KEY=your_openrouter_key
   set ELEVENLABS_API_KEY=your_elevenlabs_key
   set MEM0_API_KEY=your_mem0_key
   ```
   
   For permanent setup, add these to your Windows environment variables.

## Usage

### Quick Start

1. Ingest a reference book:
   ```
   python main.py ingest path/to/scifi_book.epub --analyze
   ```

2. Sync the book to vector memory:
   ```
   python main.py sync-memory --all
   ```

3. Register character voices:
   ```
   python main.py register-voice "Captain Kirk" elevenlabs_voice_id --description "Commanding, charismatic"
   ```

4. Generate an episode:
   ```
   python main.py generate-episode --title "The Quantum Paradox" --theme "time travel" --duration 30
   ```

5. Generate audio for the episode:
   ```
   python main.py generate-audio episode_id
   ```

### Available Commands

- `ingest`: Process an EPUB file and extract content
- `analyze`: Analyze writing style and themes of ingested books
- `sync-memory`: Synchronize reference materials with vector memory
- `generate-episode`: Create a new podcast episode script
- `edit-script`: Edit an episode script
- `regenerate-scene`: Regenerate a specific scene in an episode
- `register-voice`: Add a new voice to the voice registry
- `list-voices`: Show all registered voices
- `generate-audio`: Create audio for an episode
- `check-quality`: Verify the quality of an episode's script and/or audio
- `list-episodes`: Display all generated episodes with filters

Run `python main.py --help` to see all available commands and options.

## Project Structure

- `main.py`: Main entry point and environment initialization
- `cli_entrypoint.py`: Command-line interface for all functionality
- `mem0_client.py`: Interface to Mem0 vector database
- `epub_processor.py`: EPUB file processing and content extraction
- `book_style_analysis.py`: Analysis of writing styles and themes
- `reference_memory_sync.py`: Sync reference materials to vector memory
- `story_structure.py`: Episode generation using Save-the-Cat structure
- `episode_memory.py`: Management of episode continuity and character development
- `voice_registry.py`: Voice management for characters
- `episode_metadata.py`: Episode tagging and organization
- `script_editor.py`: Script editing and scene regeneration
- `audio_pipeline.py`: Audio generation and assembly
- `quality_checker.py`: Script and audio quality verification

## Configuration

The system creates several directories for storing data:
- `books/`: Ingested reference materials
- `analysis/`: Style analysis results
- `episodes/`: Generated episode data
- `audio/`: Generated audio files
- `voices/`: Voice registry data
- `temp/`: Temporary files
- `data/`: Application data
- `logs/`: Log files

## Development

### Running Tests

```
python -m unittest discover -s tests
```

### Adding New Features

1. Extend the appropriate module based on the feature type
2. Register new commands in `cli_entrypoint.py` if needed
3. Update documentation

## Acknowledgments

- ElevenLabs for voice synthesis technology
- OpenAI and OpenRouter for text generation capabilities
- Mem0 for vector database functionality
- FFmpeg for audio processing

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="reference_memory_sync.py">
#!/usr/bin/env python
"""
Reference Memory Sync Module for Stardock Podium.

This module synchronizes processed reference materials (books, papers, etc.)
with the mem0 vector database for semantic search and retrieval.
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import concurrent.futures
from tqdm import tqdm

# Local imports
from epub_processor import get_processor, list_books
from mem0_client import get_mem0_client

# Setup logging
logger = logging.getLogger(__name__)

class ReferenceMemorySync:
    """Synchronizes reference materials with vector memory database."""
    
    def __init__(self):
        """Initialize the reference memory sync."""
        self.epub_processor = get_processor()
        self.mem0_client = get_mem0_client()
        
        # Create sync status directory
        self.sync_dir = Path("data/sync_status")
        self.sync_dir.mkdir(exist_ok=True, parents=True)
    
    def sync_book(self, book_id: str, force: bool = False) -> Dict[str, Any]:
        """Sync a specific book to memory.
        
        Args:
            book_id: ID of the book to sync
            force: Whether to force sync even if already synced
        
        Returns:
            Dictionary with sync results
        """
        # Check if book is already synced
        sync_file = self.sync_dir / f"{book_id}_sync.json"
        
        if sync_file.exists() and not force:
            try:
                with open(sync_file, 'r') as f:
                    sync_status = json.load(f)
                    if sync_status.get("completed", False):
                        logger.info(f"Book {book_id} already synced. Use force=True to resync.")
                        return sync_status
            except Exception as e:
                logger.error(f"Error reading sync status: {e}")
        
        # Get book metadata
        metadata = self.epub_processor.get_book_metadata(book_id)
        if not metadata:
            logger.error(f"Book metadata not found for ID: {book_id}")
            return {"error": "Book metadata not found"}
        
        title = metadata.get('title', 'Unknown Title')
        author = metadata.get('creator', 'Unknown Author')
        
        logger.info(f"Syncing book '{title}' by {author} (ID: {book_id}) to memory")
        
        # Get book sections
        sections = self.epub_processor.get_book_sections(book_id)
        if not sections or not sections.get('sections'):
            logger.error(f"No sections found for book {book_id}")
            return {"error": "No sections found"}
        
        # Initialize sync status
        sync_status = {
            "book_id": book_id,
            "title": title,
            "author": author,
            "started_at": time.time(),
            "completed": False,
            "total_sections": len(sections.get('sections', [])),
            "synced_sections": 0,
            "failed_sections": 0,
            "memory_ids": []
        }
        
        # Save initial sync status
        try:
            with open(sync_file, 'w') as f:
                json.dump(sync_status, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving sync status: {e}")
        
        # Process each section in parallel
        section_results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            for section in sections.get('sections', []):
                section_content = section.get('content', '')
                section_title = section.get('section_title', '')
                chapter_title = section.get('chapter_title', '')
                
                futures.append(
                    executor.submit(
                        self._add_section_to_memory,
                        book_id=book_id,
                        title=title,
                        author=author,
                        section_content=section_content,
                        section_title=section_title,
                        chapter_title=chapter_title
                    )
                )
            
            # Process results as they complete
            for future in tqdm(concurrent.futures.as_completed(futures),
                             total=len(futures),
                             desc=f"Syncing {title}"):
                try:
                    result = future.result()
                    section_results.append(result)
                    
                    # Update sync status
                    if result.get("success"):
                        sync_status["synced_sections"] += 1
                        sync_status["memory_ids"].append(result.get("memory_id"))
                    else:
                        sync_status["failed_sections"] += 1
                    
                    # Periodically save sync status
                    if (sync_status["synced_sections"] + 
                        sync_status["failed_sections"]) % 10 == 0:
                        with open(sync_file, 'w') as f:
                            json.dump(sync_status, f, indent=2)
                
                except Exception as e:
                    logger.error(f"Error processing section: {e}")
                    sync_status["failed_sections"] += 1
        
        # Update and save final sync status
        sync_status["completed"] = True
        sync_status["completed_at"] = time.time()
        sync_status["success_rate"] = (sync_status["synced_sections"] / 
                                      sync_status["total_sections"] 
                                      if sync_status["total_sections"] > 0 else 0)
        
        try:
            with open(sync_file, 'w') as f:
                json.dump(sync_status, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving final sync status: {e}")
        
        return sync_status
    
    def _add_section_to_memory(self, book_id: str, title: str, author: str,
                              section_content: str, section_title: str,
                              chapter_title: str) -> Dict[str, Any]:
        """Add a section to vector memory.
        
        Args:
            book_id: ID of the book
            title: Book title
            author: Book author
            section_content: Content of the section
            section_title: Title of the section
            chapter_title: Title of the chapter
        
        Returns:
            Dictionary with result information
        """
        try:
            # Prepare metadata
            metadata = {
                "book_id": book_id,
                "book_title": title,
                "author": author,
                "section_title": section_title,
                "chapter_title": chapter_title
            }
            
            # Add to memory
            result = self.mem0_client.add_reference_material(
                content=section_content,
                source=f"{title} by {author}",
                metadata=metadata
            )
            
            return {
                "success": True,
                "book_id": book_id,
                "section_title": section_title,
                "memory_id": result.get("id") if isinstance(result, dict) else None
            }
        
        except Exception as e:
            logger.error(f"Error adding section to memory: {e}")
            return {
                "success": False,
                "book_id": book_id,
                "section_title": section_title,
                "error": str(e)
            }
    
    def sync_all_books(self, force: bool = False) -> Dict[str, Any]:
        """Sync all available books to memory.
        
        Args:
            force: Whether to force sync even if already synced
        
        Returns:
            Dictionary with sync results
        """
        # Get all available books
        books = list_books()
        
        if not books:
            logger.warning("No books found to sync")
            return {"error": "No books found"}
        
        # Process each book
        results = {}
        
        for book in books:
            book_id = book.get("book_id")
            if not book_id:
                continue
            
            result = self.sync_book(book_id, force=force)
            results[book_id] = result
        
        # Create a summary
        summary = {
            "total_books": len(books),
            "successful_syncs": sum(1 for result in results.values() 
                                   if result.get("completed", False)),
            "failed_syncs": sum(1 for result in results.values() 
                               if not result.get("completed", False)),
            "total_sections_synced": sum(result.get("synced_sections", 0) 
                                        for result in results.values()),
            "completed_at": time.time()
        }
        
        # Save overall sync status
        try:
            with open(self.sync_dir / "all_books_sync.json", 'w') as f:
                json.dump({
                    "summary": summary,
                    "book_results": results
                }, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving all books sync status: {e}")
        
        return {
            "summary": summary,
            "book_results": results
        }
    
    def get_sync_status(self, book_id: Optional[str] = None) -> Dict[str, Any]:
        """Get sync status for a book or all books.
        
        Args:
            book_id: Optional ID of the book to check
        
        Returns:
            Dictionary with sync status
        """
        if book_id:
            # Get status for a specific book
            sync_file = self.sync_dir / f"{book_id}_sync.json"
            
            if not sync_file.exists():
                return {"book_id": book_id, "synced": False, "error": "Not synced"}
            
            try:
                with open(sync_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error reading sync status: {e}")
                return {"book_id": book_id, "error": f"Error reading status: {e}"}
        else:
            # Get status for all books
            all_sync_file = self.sync_dir / "all_books_sync.json"
            
            if all_sync_file.exists():
                try:
                    with open(all_sync_file, 'r') as f:
                        return json.load(f)
                except Exception as e:
                    logger.error(f"Error reading all books sync status: {e}")
            
            # If no overall status, collect individual statuses
            statuses = {}
            for sync_file in self.sync_dir.glob("*_sync.json"):
                if sync_file.name == "all_books_sync.json":
                    continue
                
                try:
                    with open(sync_file, 'r') as f:
                        book_status = json.load(f)
                        book_id = book_status.get("book_id")
                        if book_id:
                            statuses[book_id] = book_status
                except Exception as e:
                    logger.error(f"Error reading sync file {sync_file}: {e}")
            
            return {
                "individual_statuses": statuses,
                "total_books_synced": len(statuses)
            }
    
    def search_references(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Search reference materials based on semantic similarity.
        
        Args:
            query: The search query
            limit: Maximum number of results to return
        
        Returns:
            List of relevant reference materials
        """
        return self.mem0_client.search_reference_materials(query, limit=limit)

# Create singleton instance
_memory_sync = None

def get_memory_sync() -> ReferenceMemorySync:
    """Get the ReferenceMemorySync singleton instance."""
    global _memory_sync
    
    if _memory_sync is None:
        _memory_sync = ReferenceMemorySync()
    
    return _memory_sync

def sync_references(book_id: Optional[str] = None, force: bool = False) -> Dict[str, Any]:
    """Sync reference materials to memory.
    
    Args:
        book_id: Optional ID of a specific book to sync
        force: Whether to force sync even if already synced
    
    Returns:
        Dictionary with sync results
    """
    memory_sync = get_memory_sync()
    
    if book_id:
        return memory_sync.sync_book(book_id, force=force)
    else:
        return memory_sync.sync_all_books(force=force)

def get_sync_status(book_id: Optional[str] = None) -> Dict[str, Any]:
    """Get sync status for a book or all books.
    
    Args:
        book_id: Optional ID of the book to check
    
    Returns:
        Dictionary with sync status
    """
    memory_sync = get_memory_sync()
    return memory_sync.get_sync_status(book_id)

def search_references(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """Search reference materials based on semantic similarity.
    
    Args:
        query: The search query
        limit: Maximum number of results to return
    
    Returns:
        List of relevant reference materials
    """
    memory_sync = get_memory_sync()
    return memory_sync.search_references(query, limit=limit)
</file>

<file path="requirements.txt">
# Core dependencies
openai>=1.0.0
ebooklib>=0.18.0
elevenlabs>=0.2.24
ffmpeg-python>=0.2.0
nltk>=3.8.1
mem0>=0.1.0  # Vector database client

# Utilities
requests>=2.31.0
aiohttp>=3.8.5
python-dotenv>=1.0.0
tqdm>=4.66.1
colorama>=0.4.6

# Data processing
numpy>=1.24.0
pandas>=2.0.0

# Testing
pytest>=7.3.1
pytest-asyncio>=0.21.0
</file>

<file path="script_editor.py">
#!/usr/bin/env python
"""
Script Editor Module for Stardock Podium.

This module allows manual editing, regeneration flagging, and revision history
of AI-generated scripts before audio production.
"""

import os
import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import uuid
import tempfile
import subprocess
import shutil

# Try to import OpenAI
try:
    from openai import OpenAI
except ImportError:
    logging.error("OpenAI not found. Please install it with: pip install openai")
    raise

# Local imports
from story_structure import get_story_structure, get_episode

# Setup logging
logger = logging.getLogger(__name__)

class ScriptEditor:
    """Editor for episode scripts with revision history and scene regeneration."""
    
    def __init__(self, episodes_dir: str = "episodes"):
        """Initialize the script editor.
        
        Args:
            episodes_dir: Directory containing episode data
        """
        self.episodes_dir = Path(episodes_dir)
        
        # Initialize OpenAI client for regeneration
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not found in environment variables")
        
        self.client = OpenAI(api_key=api_key)
        
        # Initialize story structure
        self.story_structure = get_story_structure()
    
    def load_episode_script(self, episode_id: str) -> Dict[str, Any]:
        """Load the script for an episode.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Dictionary with script data
        """
        episode_dir = self.episodes_dir / episode_id
        script_file = episode_dir / "script.json"
        
        if not script_file.exists():
            logger.error(f"Script file not found: {script_file}")
            return {}
        
        try:
            with open(script_file, 'r') as f:
                script = json.load(f)
            
            return script
        except Exception as e:
            logger.error(f"Error loading script: {e}")
            return {}
    
    def save_script(self, script: Dict[str, Any]) -> bool:
        """Save a script to file, with revision history.
        
        Args:
            script: Updated script data
        
        Returns:
            Success status
        """
        if not script or 'episode_id' not in script:
            logger.error("Invalid script data: missing episode_id")
            return False
        
        episode_id = script['episode_id']
        episode_dir = self.episodes_dir / episode_id
        
        if not episode_dir.exists():
            logger.error(f"Episode directory not found: {episode_dir}")
            return False
        
        # Create revisions directory if it doesn't exist
        revisions_dir = episode_dir / "revisions"
        revisions_dir.mkdir(exist_ok=True)
        
        # Check if current script exists to create a revision
        script_file = episode_dir / "script.json"
        if script_file.exists():
            try:
                # Create revision of current script
                with open(script_file, 'r') as f:
                    current_script = json.load(f)
                
                # Generate revision ID and timestamp
                revision = {
                    "revision_id": f"rev_{uuid.uuid4().hex[:8]}",
                    "timestamp": time.time(),
                    "script": current_script
                }
                
                # Save revision
                revision_file = revisions_dir / f"{revision['revision_id']}.json"
                with open(revision_file, 'w') as f:
                    json.dump(revision, f, indent=2)
                
                logger.info(f"Created script revision: {revision['revision_id']}")
            
            except Exception as e:
                logger.error(f"Error creating script revision: {e}")
        
        # Save new script
        try:
            # Update modified timestamp
            script['updated_at'] = time.time()
            
            with open(script_file, 'w') as f:
                json.dump(script, f, indent=2)
            
            logger.info(f"Saved script for episode {episode_id}")
            return True
        
        except Exception as e:
            logger.error(f"Error saving script: {e}")
            return False
    
    def preview_scene_flow(self, script: Dict[str, Any]) -> List[str]:
        """Generate a preview of the scene flow in the script.
        
        Args:
            script: Script data
        
        Returns:
            List of scene summaries
        """
        if not script or 'scenes' not in script:
            logger.error("Invalid script data: missing scenes")
            return []
        
        scene_summaries = []
        
        for i, scene in enumerate(script['scenes']):
            # Extract basic info
            scene_number = scene.get('scene_number', i + 1)
            beat = scene.get('beat', 'Unknown beat')
            setting = scene.get('setting', 'Unknown setting')
            
            # Count dialogue lines by character
            character_lines = {}
            for line in scene.get('lines', []):
                if line.get('type') == 'dialogue':
                    character = line.get('character', 'Unknown')
                    if character not in character_lines:
                        character_lines[character] = 0
                    character_lines[character] += 1
            
            # Create character summary
            character_summary = ", ".join([f"{char} ({count} lines)" 
                                         for char, count in character_lines.items()])
            
            # Create scene summary
            summary = f"Scene {scene_number}: {beat} - {setting}"
            if character_summary:
                summary += f" - Characters: {character_summary}"
            
            scene_summaries.append(summary)
        
        return scene_summaries
    
    def update_line(self, script: Dict[str, Any], scene_index: int, 
                   line_index: int, new_text: str) -> Dict[str, Any]:
        """Update a specific line in the script.
        
        Args:
            script: Script data
            scene_index: Index of the scene
            line_index: Index of the line within the scene
            new_text: New content for the line
        
        Returns:
            Updated script
        """
        # Validate inputs
        if not script or 'scenes' not in script:
            logger.error("Invalid script data: missing scenes")
            return script
        
        if scene_index < 0 or scene_index >= len(script['scenes']):
            logger.error(f"Invalid scene index: {scene_index}")
            return script
        
        scene = script['scenes'][scene_index]
        
        if 'lines' not in scene:
            logger.error(f"Scene has no lines: {scene_index}")
            return script
        
        if line_index < 0 or line_index >= len(scene['lines']):
            logger.error(f"Invalid line index: {line_index}")
            return script
        
        # Update the line
        line = scene['lines'][line_index]
        line['content'] = new_text
        
        # Mark as manually edited
        if 'edit_history' not in line:
            line['edit_history'] = []
        
        line['edit_history'].append({
            "timestamp": time.time(),
            "type": "manual_edit"
        })
        
        return script
    
    def mark_scene_for_regeneration(self, script: Dict[str, Any], scene_index: int) -> Dict[str, Any]:
        """Mark a scene for regeneration.
        
        Args:
            script: Script data
            scene_index: Index of the scene
        
        Returns:
            Updated script
        """
        # Validate inputs
        if not script or 'scenes' not in script:
            logger.error("Invalid script data: missing scenes")
            return script
        
        if scene_index < 0 or scene_index >= len(script['scenes']):
            logger.error(f"Invalid scene index: {scene_index}")
            return script
        
        # Mark the scene
        scene = script['scenes'][scene_index]
        scene['needs_regeneration'] = True
        
        if 'edit_history' not in scene:
            scene['edit_history'] = []
        
        scene['edit_history'].append({
            "timestamp": time.time(),
            "type": "marked_for_regeneration"
        })
        
        return script
    
    def regenerate_scene(self, episode_id: str, scene_index: int,
                        instructions: Optional[str] = None) -> Dict[str, Any]:
        """Regenerate a specific scene with optional instructions.
        
        Args:
            episode_id: ID of the episode
            scene_index: Index of the scene to regenerate
            instructions: Optional special instructions for regeneration
        
        Returns:
            Updated script with regenerated scene
        """
        # Load episode and script
        episode = get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {}
        
        script = self.load_episode_script(episode_id)
        if not script:
            logger.error(f"Script not found for episode: {episode_id}")
            return {}
        
        # Validate scene index
        if 'scenes' not in script:
            logger.error("Invalid script data: missing scenes")
            return script
        
        if scene_index < 0 or scene_index >= len(script['scenes']):
            logger.error(f"Invalid scene index: {scene_index}")
            return script
        
        # Get the scene to regenerate
        scene = script['scenes'][scene_index]
        
        # Get character information
        character_info = ""
        for char in episode.get('characters', []):
            character_info += f"{char.get('name', '')}: {char.get('species', '')} - {char.get('role', '')}\n"
        
        # Create context for regeneration
        context = (
            f"Title: {episode.get('title', '')}\n"
            f"Theme: {episode.get('theme', '')}\n"
            f"Beat: {scene.get('beat', '')}\n"
            f"Setting: {scene.get('setting', '')}\n"
            f"Scene Number: {scene.get('scene_number', scene_index + 1)}\n\n"
            f"Character Information:\n{character_info}\n"
        )
        
        # Add special instructions if provided
        special_instructions = ""
        if instructions:
            special_instructions = f"Special Instructions: {instructions}\n\n"
        
        # Construct prompt
        prompt = f"""
        You are tasked with regenerating a scene for a Star Trek-style podcast episode.
        
        CONTEXT:
        {context}
        
        {special_instructions}
        
        Please write a detailed scene script that maintains the same setting and beat as the original scene,
        but potentially improves the dialogue, pacing, and dramatic elements.
        
        Format the scene script as follows:
        1. Brief setting descriptions in [brackets]
        2. Character names in ALL CAPS, followed by their dialogue
        3. Sound effects in (parentheses)
        4. Narrator sections marked as NARRATOR
        """
        
        try:
            # Generate new scene content
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are an expert screenwriter for audio dramas."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            # Parse the generated content
            new_content = response.choices[0].message.content
            new_lines = self._parse_script_lines(new_content)
            
            # Update the scene
            scene['lines'] = new_lines
            scene.pop('needs_regeneration', None)  # Remove regeneration flag
            
            # Add to edit history
            if 'edit_history' not in scene:
                scene['edit_history'] = []
            
            scene['edit_history'].append({
                "timestamp": time.time(),
                "type": "regenerated",
                "instructions": instructions
            })
            
            # Save the updated script
            self.save_script(script)
            
            return script
        
        except Exception as e:
            logger.error(f"Error regenerating scene: {e}")
            return script
    
    def _parse_script_lines(self, script_content: str) -> List[Dict[str, Any]]:
        """Parse script content into structured lines.
        
        Args:
            script_content: Generated script content
        
        Returns:
            List of line dictionaries
        """
        lines = []
        
        # Split script into paragraphs
        paragraphs = re.split(r'\n{2,}', script_content)
        
        scene_description = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # Check for scene description in brackets
            description_match = re.search(r'\[(.*?)\]', paragraph)
            if description_match:
                scene_description = description_match.group(1).strip()
                # Check if there's content after the description
                remaining = re.sub(r'\[(.*?)\]', '', paragraph).strip()
                if not remaining:
                    lines.append({
                        "type": "description",
                        "content": scene_description
                    })
                    continue
                paragraph = remaining
            
            # Check for sound effect in parentheses
            sound_effect_match = re.search(r'\((.*?)\)', paragraph)
            if sound_effect_match and len(sound_effect_match.group(0)) > len(paragraph) * 0.7:
                lines.append({
                    "type": "sound_effect",
                    "content": sound_effect_match.group(1).strip()
                })
                continue
            
            # Check for character dialogue
            dialogue_match = re.search(r'^([A-Z][A-Z\s]+)(?:\s*\(.*?\))?\:\s*(.*)', paragraph)
            if dialogue_match:
                character = dialogue_match.group(1).strip()
                dialogue = dialogue_match.group(2).strip()
                
                # Check if it's the narrator
                if character.upper() == "NARRATOR":
                    lines.append({
                        "type": "narration",
                        "content": dialogue
                    })
                else:
                    lines.append({
                        "type": "dialogue",
                        "character": character,
                        "content": dialogue
                    })
                continue
            
            # If no specific format matched, treat as description
            lines.append({
                "type": "description",
                "content": paragraph
            })
        
        return lines
    
    def get_revisions(self, episode_id: str) -> List[Dict[str, Any]]:
        """Get all revisions for an episode script.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            List of revision metadata
        """
        episode_dir = self.episodes_dir / episode_id
        revisions_dir = episode_dir / "revisions"
        
        if not revisions_dir.exists():
            return []
        
        revisions = []
        
        for revision_file in revisions_dir.glob("*.json"):
            try:
                with open(revision_file, 'r') as f:
                    revision = json.load(f)
                
                # Extract metadata only
                revisions.append({
                    "revision_id": revision.get("revision_id"),
                    "timestamp": revision.get("timestamp"),
                    "file": revision_file.name
                })
            
            except Exception as e:
                logger.error(f"Error reading revision file {revision_file}: {e}")
        
        # Sort by timestamp
        revisions.sort(key=lambda r: r.get("timestamp", 0), reverse=True)
        
        return revisions
    
    def load_revision(self, episode_id: str, revision_id: str) -> Dict[str, Any]:
        """Load a specific revision of a script.
        
        Args:
            episode_id: ID of the episode
            revision_id: ID of the revision
        
        Returns:
            Revision script data
        """
        episode_dir = self.episodes_dir / episode_id
        revision_file = episode_dir / "revisions" / f"{revision_id}.json"
        
        if not revision_file.exists():
            logger.error(f"Revision file not found: {revision_file}")
            return {}
        
        try:
            with open(revision_file, 'r') as f:
                revision = json.load(f)
            
            return revision.get("script", {})
        
        except Exception as e:
            logger.error(f"Error loading revision: {e}")
            return {}
    
    def restore_revision(self, episode_id: str, revision_id: str) -> Dict[str, Any]:
        """Restore a script to a previous revision.
        
        Args:
            episode_id: ID of the episode
            revision_id: ID of the revision to restore
        
        Returns:
            Restored script
        """
        # Load the revision
        revision_script = self.load_revision(episode_id, revision_id)
        if not revision_script:
            logger.error(f"Failed to load revision: {revision_id}")
            return {}
        
        # Save as current script
        if self.save_script(revision_script):
            logger.info(f"Restored script to revision: {revision_id}")
            return revision_script
        else:
            logger.error(f"Failed to restore script to revision: {revision_id}")
            return {}
    
    def compare_revisions(self, episode_id: str, revision_id_a: str, 
                         revision_id_b: Optional[str] = None) -> Dict[str, Any]:
        """Compare two script revisions.
        
        Args:
            episode_id: ID of the episode
            revision_id_a: ID of the first revision
            revision_id_b: Optional ID of the second revision (current if None)
        
        Returns:
            Dictionary with comparison results
        """
        # Load first revision
        script_a = self.load_revision(episode_id, revision_id_a)
        if not script_a:
            logger.error(f"Failed to load revision A: {revision_id_a}")
            return {"error": f"Failed to load revision A: {revision_id_a}"}
        
        # Load second revision or current script
        if revision_id_b:
            script_b = self.load_revision(episode_id, revision_id_b)
            if not script_b:
                logger.error(f"Failed to load revision B: {revision_id_b}")
                return {"error": f"Failed to load revision B: {revision_id_b}"}
        else:
            script_b = self.load_episode_script(episode_id)
            if not script_b:
                logger.error(f"Failed to load current script for episode: {episode_id}")
                return {"error": f"Failed to load current script for episode: {episode_id}"}
        
        # Compare scenes
        comparison = {
            "revision_a": revision_id_a,
            "revision_b": revision_id_b or "current",
            "scene_changes": [],
            "overall_similarity": 0.0
        }
        
        # Check that both scripts have scenes
        if 'scenes' not in script_a or 'scenes' not in script_b:
            return {"error": "One or both scripts are missing scenes"}
        
        # Compare each scene
        a_scenes = script_a['scenes']
        b_scenes = script_b['scenes']
        
        # Count total scenes in both scripts
        total_scenes = max(len(a_scenes), len(b_scenes))
        matched_scenes = 0
        
        for i in range(total_scenes):
            if i < len(a_scenes) and i < len(b_scenes):
                # Both scripts have this scene
                scene_a = a_scenes[i]
                scene_b = b_scenes[i]
                
                # Compare scene attributes
                scene_comparison = self._compare_scenes(scene_a, scene_b)
                comparison["scene_changes"].append(scene_comparison)
                
                # Update matched scenes count
                if scene_comparison.get("similarity", 0) > 0.7:
                    matched_scenes += 1
            
            elif i < len(a_scenes):
                # Scene exists in A but not in B
                comparison["scene_changes"].append({
                    "scene_number": i + 1,
                    "action": "removed",
                    "similarity": 0.0,
                    "details": f"Scene {i + 1} from revision A is not present in revision B"
                })
            
            else:
                # Scene exists in B but not in A
                comparison["scene_changes"].append({
                    "scene_number": i + 1,
                    "action": "added",
                    "similarity": 0.0,
                    "details": f"Scene {i + 1} is new in revision B"
                })
        
        # Calculate overall similarity
        comparison["overall_similarity"] = matched_scenes / total_scenes if total_scenes > 0 else 1.0
        
        return comparison
    
    def _compare_scenes(self, scene_a: Dict[str, Any], scene_b: Dict[str, Any]) -> Dict[str, Any]:
        """Compare two scenes for differences.
        
        Args:
            scene_a: First scene
            scene_b: Second scene
        
        Returns:
            Dictionary with comparison results
        """
        scene_number = scene_a.get('scene_number', 0)
        
        # Check for basic attribute changes
        attribute_changes = []
        for attr in ['beat', 'setting']:
            if scene_a.get(attr) != scene_b.get(attr):
                attribute_changes.append({
                    "attribute": attr,
                    "previous": scene_a.get(attr),
                    "current": scene_b.get(attr)
                })
        
        # Compare lines
        lines_a = scene_a.get('lines', [])
        lines_b = scene_b.get('lines', [])
        
        line_changes = []
        
        # Count total lines in both scenes
        total_lines = max(len(lines_a), len(lines_b))
        matched_lines = 0
        
        for i in range(total_lines):
            if i < len(lines_a) and i < len(lines_b):
                # Both scenes have this line
                line_a = lines_a[i]
                line_b = lines_b[i]
                
                # Check if line type changed
                type_changed = line_a.get('type') != line_b.get('type')
                
                # Check if content changed
                content_changed = line_a.get('content') != line_b.get('content')
                
                # Check if character changed (for dialogue)
                character_changed = False
                if line_a.get('type') == 'dialogue' and line_b.get('type') == 'dialogue':
                    character_changed = line_a.get('character') != line_b.get('character')
                
                if type_changed or content_changed or character_changed:
                    line_changes.append({
                        "line_number": i + 1,
                        "type_changed": type_changed,
                        "content_changed": content_changed,
                        "character_changed": character_changed,
                        "previous": {
                            "type": line_a.get('type'),
                            "content": line_a.get('content')[:50] + "..." if len(line_a.get('content', '')) > 50 else line_a.get('content', ''),
                            "character": line_a.get('character') if line_a.get('type') == 'dialogue' else None
                        },
                        "current": {
                            "type": line_b.get('type'),
                            "content": line_b.get('content')[:50] + "..." if len(line_b.get('content', '')) > 50 else line_b.get('content', ''),
                            "character": line_b.get('character') if line_b.get('type') == 'dialogue' else None
                        }
                    })
                else:
                    # Lines are the same
                    matched_lines += 1
            
            elif i < len(lines_a):
                # Line exists in A but not in B
                line_changes.append({
                    "line_number": i + 1,
                    "action": "removed",
                    "previous": {
                        "type": lines_a[i].get('type'),
                        "content": lines_a[i].get('content')[:50] + "..." if len(lines_a[i].get('content', '')) > 50 else lines_a[i].get('content', ''),
                        "character": lines_a[i].get('character') if lines_a[i].get('type') == 'dialogue' else None
                    }
                })
            
            else:
                # Line exists in B but not in A
                line_changes.append({
                    "line_number": i + 1,
                    "action": "added",
                    "current": {
                        "type": lines_b[i].get('type'),
                        "content": lines_b[i].get('content')[:50] + "..." if len(lines_b[i].get('content', '')) > 50 else lines_b[i].get('content', ''),
                        "character": lines_b[i].get('character') if lines_b[i].get('type') == 'dialogue' else None
                    }
                })
        
        # Calculate line similarity
        line_similarity = matched_lines / total_lines if total_lines > 0 else 1.0
        
        # Calculate overall scene similarity
        attribute_similarity = 1.0 - (len(attribute_changes) / 2)  # 2 possible attributes
        scene_similarity = (line_similarity * 0.8) + (attribute_similarity * 0.2)
        
        # Determine overall action
        if scene_similarity > 0.9:
            action = "unchanged"
        elif scene_similarity > 0.7:
            action = "modified"
        elif scene_similarity > 0.4:
            action = "heavily_modified"
        else:
            action = "completely_different"
        
        return {
            "scene_number": scene_number,
            "action": action,
            "similarity": scene_similarity,
            "attribute_changes": attribute_changes,
            "line_changes": line_changes
        }
    
    def edit_episode_script(self, episode_id: str) -> bool:
        """Open the script in a text editor for manual editing.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Success status
        """
        # Load script
        script = self.load_episode_script(episode_id)
        if not script:
            logger.error(f"Script not found for episode: {episode_id}")
            return False
        
        # Create a human-readable version of the script
        readable_script = self._create_readable_script(script)
        
        # Create a temporary file
        with tempfile.NamedTemporaryFile(suffix=".txt", mode="w+", delete=False) as tmp:
            tmp_path = tmp.name
            tmp.write(readable_script)
        
        try:
            # Determine editor
            editor = os.environ.get("EDITOR", "notepad" if os.name == "nt" else "nano")
            
            # Open the editor
            subprocess.run([editor, tmp_path], check=True)
            
            # Wait for editor to close, then read the file
            with open(tmp_path, 'r') as f:
                edited_script = f.read()
            
            # Convert back to structured script
            new_script = self._parse_readable_script(edited_script, script)
            
            # Save the new script
            if self.save_script(new_script):
                logger.info(f"Script for episode {episode_id} updated successfully")
                return True
            else:
                logger.error(f"Failed to save updated script for episode {episode_id}")
                return False
        
        except Exception as e:
            logger.error(f"Error editing script: {e}")
            return False
        
        finally:
            # Clean up temporary file
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)
    
    def _create_readable_script(self, script: Dict[str, Any]) -> str:
        """Create a human-readable version of the script.
        
        Args:
            script: Script data
        
        Returns:
            Human-readable script string
        """
        readable = f"TITLE: {script.get('title', 'Untitled')}\n"
        readable += f"EPISODE ID: {script.get('episode_id', 'unknown')}\n\n"
        readable += "=== SCRIPT START (DO NOT EDIT THIS LINE) ===\n\n"
        
        for i, scene in enumerate(script.get('scenes', [])):
            scene_number = scene.get('scene_number', i + 1)
            beat = scene.get('beat', 'Unknown beat')
            setting = scene.get('setting', 'Unknown setting')
            
            readable += f"### SCENE {scene_number}: {beat} ###\n"
            readable += f"SETTING: {setting}\n\n"
            
            for line in scene.get('lines', []):
                line_type = line.get('type', 'unknown')
                content = line.get('content', '')
                
                if line_type == 'description':
                    readable += f"[DESCRIPTION] {content}\n\n"
                elif line_type == 'dialogue':
                    character = line.get('character', 'UNKNOWN')
                    readable += f"{character}: {content}\n\n"
                elif line_type == 'sound_effect':
                    readable += f"(SOUND) {content}\n\n"
                elif line_type == 'narration':
                    readable += f"NARRATOR: {content}\n\n"
                else:
                    readable += f"[{line_type.upper()}] {content}\n\n"
            
            readable += "### END SCENE ###\n\n"
        
        readable += "=== SCRIPT END (DO NOT EDIT THIS LINE) ===\n"
        
        return readable
    
    def _parse_readable_script(self, readable_script: str, original_script: Dict[str, Any]) -> Dict[str, Any]:
        """Parse a human-readable script back to structured format.
        
        Args:
            readable_script: Human-readable script string
            original_script: Original script data for reference
        
        Returns:
            Updated script data
        """
        # Create a copy of the original script
        new_script = {
            "title": original_script.get('title', 'Untitled'),
            "episode_id": original_script.get('episode_id', 'unknown'),
            "created_at": original_script.get('created_at', time.time()),
            "updated_at": time.time(),
            "scenes": []
        }
        
        # Extract content between start and end markers
        pattern = r"=== SCRIPT START \(DO NOT EDIT THIS LINE\) ===\n(.*?)\n=== SCRIPT END \(DO NOT EDIT THIS LINE\) ==="
        match = re.search(pattern, readable_script, re.DOTALL)
        
        if not match:
            logger.error("Failed to find script content markers")
            return original_script
        
        content = match.group(1)
        
        # Split into scenes
        scene_pattern = r"### SCENE (\d+): (.*?) ###\n(.*?)(?=### END SCENE ###)"
        scene_matches = re.finditer(scene_pattern, content, re.DOTALL)
        
        for scene_match in scene_matches:
            scene_number = int(scene_match.group(1))
            beat = scene_match.group(2).strip()
            scene_content = scene_match.group(3)
            
            # Extract setting
            setting_match = re.search(r"SETTING: (.*?)(?:\n\n|\Z)", scene_content)
            setting = setting_match.group(1).strip() if setting_match else ""
            
            # Remove setting line from content
            if setting_match:
                scene_content = scene_content.replace(setting_match.group(0), "", 1)
            
            # Parse lines
            lines = []
            
            # Split content into lines/paragraphs
            paragraphs = re.split(r'\n\n', scene_content.strip())
            
            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue
                
                # Check line type
                if paragraph.startswith('[DESCRIPTION]'):
                    content = paragraph[len('[DESCRIPTION]'):].strip()
                    lines.append({
                        "type": "description",
                        "content": content
                    })
                
                elif paragraph.startswith('(SOUND)'):
                    content = paragraph[len('(SOUND)'):].strip()
                    lines.append({
                        "type": "sound_effect",
                        "content": content
                    })
                
                elif paragraph.startswith('NARRATOR:'):
                    content = paragraph[len('NARRATOR:'):].strip()
                    lines.append({
                        "type": "narration",
                        "content": content
                    })
                
                elif ':' in paragraph:
                    # Dialogue
                    parts = paragraph.split(':', 1)
                    character = parts[0].strip()
                    content = parts[1].strip()
                    
                    lines.append({
                        "type": "dialogue",
                        "character": character,
                        "content": content
                    })
                
                else:
                    # Default to description
                    lines.append({
                        "type": "description",
                        "content": paragraph
                    })
            
            # Create scene
            scene = {
                "scene_id": f"scene_{uuid.uuid4().hex[:8]}",
                "scene_number": scene_number,
                "beat": beat,
                "setting": setting,
                "lines": lines
            }
            
            # Check if this scene has an ID in the original script
            if 'scenes' in original_script:
                for original_scene in original_script['scenes']:
                    if original_scene.get('scene_number') == scene_number:
                        scene['scene_id'] = original_scene.get('scene_id', scene['scene_id'])
                        break
            
            new_script['scenes'].append(scene)
        
        # Sort scenes by scene number
        new_script['scenes'].sort(key=lambda s: s.get('scene_number', 0))
        
        return new_script

# Singleton instance
_script_editor = None

def get_script_editor() -> ScriptEditor:
    """Get the ScriptEditor singleton instance."""
    global _script_editor
    
    if _script_editor is None:
        _script_editor = ScriptEditor()
    
    return _script_editor

def load_episode_script(episode_id: str) -> Dict[str, Any]:
    """Load the script for an episode.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        Dictionary with script data
    """
    editor = get_script_editor()
    return editor.load_episode_script(episode_id)

def preview_scene_flow(script: Dict[str, Any]) -> List[str]:
    """Generate a preview of the scene flow in the script.
    
    Args:
        script: Script data
    
    Returns:
        List of scene summaries
    """
    editor = get_script_editor()
    return editor.preview_scene_flow(script)

def update_line(scene_index: int, line_index: int, new_text: str, 
               episode_id: str) -> Dict[str, Any]:
    """Update a specific line in the script.
    
    Args:
        scene_index: Index of the scene
        line_index: Index of the line within the scene
        new_text: New content for the line
        episode_id: ID of the episode
    
    Returns:
        Updated script
    """
    editor = get_script_editor()
    script = editor.load_episode_script(episode_id)
    
    if not script:
        logger.error(f"Failed to load script for episode: {episode_id}")
        return {}
    
    updated_script = editor.update_line(script, scene_index, line_index, new_text)
    
    if editor.save_script(updated_script):
        return updated_script
    else:
        logger.error(f"Failed to save updated script for episode: {episode_id}")
        return script

def mark_scene_for_regeneration(scene_index: int, episode_id: str) -> Dict[str, Any]:
    """Mark a scene for regeneration.
    
    Args:
        scene_index: Index of the scene
        episode_id: ID of the episode
    
    Returns:
        Updated script
    """
    editor = get_script_editor()
    script = editor.load_episode_script(episode_id)
    
    if not script:
        logger.error(f"Failed to load script for episode: {episode_id}")
        return {}
    
    updated_script = editor.mark_scene_for_regeneration(script, scene_index)
    
    if editor.save_script(updated_script):
        return updated_script
    else:
        logger.error(f"Failed to save updated script for episode: {episode_id}")
        return script

def regenerate_scene(episode_id: str, scene_index: int, 
                    instructions: Optional[str] = None) -> Dict[str, Any]:
    """Regenerate a specific scene with optional instructions.
    
    Args:
        episode_id: ID of the episode
        scene_index: Index of the scene to regenerate
        instructions: Optional special instructions for regeneration
    
    Returns:
        Updated script with regenerated scene
    """
    editor = get_script_editor()
    return editor.regenerate_scene(episode_id, scene_index, instructions)

def save_script(script: Dict[str, Any]) -> bool:
    """Save a script to file, with revision history.
    
    Args:
        script: Updated script data
    
    Returns:
        Success status
    """
    editor = get_script_editor()
    return editor.save_script(script)

def edit_episode_script(episode_id: str) -> bool:
    """Open the script in a text editor for manual editing.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        Success status
    """
    editor = get_script_editor()
    return editor.edit_episode_script(episode_id)
</file>

<file path="SETUP_GUIDE.md">
# Super Simple Setup Guide

## Step 1: Install Python
Make sure you have Python 3.8 or newer installed on your Windows 10 computer.

## Step 2: Get the Code
1. Download all these files to a folder on your computer

## Step 3: Install Requirements
1. Open Command Prompt 
2. Navigate to the folder with the code
3. Run: `pip install -r requirements.txt`

## Step 4: Install FFmpeg
1. Download FFmpeg from: https://ffmpeg.org/download.html
2. Extract the files
3. Add the bin folder to your Windows PATH

## Step 5: Set Up API Keys
Create a file named `.env` in the same folder with this content:
```
OPENAI_API_KEY=your_openai_key_here
ELEVENLABS_API_KEY=your_elevenlabs_key_here
MEM0_API_KEY=your_mem0_key_here
OPENROUTER_API_KEY=your_openrouter_key_here
```

Replace the "your_xxx_key_here" parts with your actual API keys.

## Step 6: First Run
1. Open Command Prompt
2. Navigate to the folder with the code
3. Run: `python main.py`

## Step 7: Start Creating
1. Add a sci-fi book: `python main.py ingest path/to/book.epub`
2. Register voices: `python main.py register-voice "Captain Kirk" your_elevenlabs_voice_id`
3. Create an episode: `python main.py generate-episode --title "Space Adventure"`
4. Make audio: `python main.py generate-audio your_episode_id`

## Folder Structure
The program will automatically create these folders:
- `books/` - Your uploaded books
- `analysis/` - Analysis of book styles
- `episodes/` - Generated episode scripts
- `audio/` - Generated audio files
- `voices/` - Saved voice information
- `data/` - Program data
- `temp/` - Temporary files

## Get Help
Type `python main.py --help` to see all available commands
</file>

<file path="story_structure.py">
#!/usr/bin/env python
"""
Story Structure Module for Stardock Podium.

This module implements Save the Cat story structure for podcast episode generation,
ensuring consistent narrative arcs across all generated content.
"""

import os
import json
import logging
import time
import uuid
import random
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import asyncio

# Try to import required libraries
try:
    from openai import OpenAI, AsyncOpenAI
except ImportError:
    logging.error("OpenAI not found. Please install it with: pip install openai")
    raise

# Local imports
from mem0_client import get_mem0_client
from reference_memory_sync import search_references

# Setup logging
logger = logging.getLogger(__name__)

class StoryStructure:
    """Handles story structure using Save the Cat beat sheet approach."""
    
    # Save the Cat beat sheet structure
    BEAT_SHEET = [
        {
            "name": "Opening Image",
            "description": "Sets the tone, mood, and style. Gives a snapshot of the starting world and its problems.",
            "percentage": 0.01,
            "duration_factor": 0.01
        },
        {
            "name": "Theme Stated",
            "description": "The message or thematic premise - what the story is really about.",
            "percentage": 0.05,
            "duration_factor": 0.02
        },
        {
            "name": "Setup",
            "description": "Introduces the main characters, their habits, and their world.",
            "percentage": 0.10,
            "duration_factor": 0.08
        },
        {
            "name": "Catalyst",
            "description": "The inciting incident or call to adventure that disrupts the status quo.",
            "percentage": 0.15,
            "duration_factor": 0.02
        },
        {
            "name": "Debate",
            "description": "The protagonist questions whether to pursue the journey or goal.",
            "percentage": 0.20,
            "duration_factor": 0.08
        },
        {
            "name": "Break into Two",
            "description": "The protagonist makes the decision to take on the journey.",
            "percentage": 0.25,
            "duration_factor": 0.02
        },
        {
            "name": "B Story",
            "description": "A secondary story or relationship that carries the theme of the story.",
            "percentage": 0.30,
            "duration_factor": 0.03
        },
        {
            "name": "Fun and Games",
            "description": "The promise of the premise is explored. The enjoyable part of the story.",
            "percentage": 0.40,
            "duration_factor": 0.15
        },
        {
            "name": "Midpoint",
            "description": "A false victory or false defeat. Stakes are raised, and the goal is less attainable.",
            "percentage": 0.50,
            "duration_factor": 0.02
        },
        {
            "name": "Bad Guys Close In",
            "description": "Antagonistic forces regroup and close in on the protagonist.",
            "percentage": 0.60,
            "duration_factor": 0.08
        },
        {
            "name": "All Is Lost",
            "description": "The lowest point where it seems the goal is impossible to achieve.",
            "percentage": 0.70,
            "duration_factor": 0.02
        },
        {
            "name": "Dark Night of the Soul",
            "description": "The protagonist must make a final decision based on what they've learned.",
            "percentage": 0.75,
            "duration_factor": 0.05
        },
        {
            "name": "Break into Three",
            "description": "The protagonist figures out the solution and commits to the final push.",
            "percentage": 0.80,
            "duration_factor": 0.02
        },
        {
            "name": "Finale",
            "description": "The protagonist proves they've changed and succeeds (or fails tragically).",
            "percentage": 0.85,
            "duration_factor": 0.12
        },
        {
            "name": "Final Image",
            "description": "Shows how the world has changed, often mirroring the opening image.",
            "percentage": 0.98,
            "duration_factor": 0.02
        }
    ]
    
    def __init__(self, episodes_dir: str = "episodes"):
        """Initialize the story structure module.
        
        Args:
            episodes_dir: Directory to store episode data
        """
        self.episodes_dir = Path(episodes_dir)
        self.episodes_dir.mkdir(exist_ok=True)
        
        # Get mem0 client
        self.mem0_client = get_mem0_client()
        
        # Initialize OpenAI client
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not found in environment variables")
        
        self.client = OpenAI(api_key=api_key)
        self.async_client = AsyncOpenAI(api_key=api_key)
        
        # Initialize OpenRouter client (fallback)
        openrouter_key = os.environ.get("OPENROUTER_API_KEY")
        if openrouter_key:
            self.using_openrouter = True
            self.openrouter_client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=openrouter_key
            )
            self.async_openrouter_client = AsyncOpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=openrouter_key
            )
        else:
            self.using_openrouter = False
            logger.warning("OPENROUTER_API_KEY not found in environment variables")
    
    def generate_episode_structure(self, episode_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate the structure for a new podcast episode.
        
        Args:
            episode_data: Data for the episode (title, theme, etc.)
        
        Returns:
            Dictionary with complete episode structure
        """
        # Generate or use episode ID
        episode_id = episode_data.get("episode_id", f"ep_{uuid.uuid4().hex[:8]}")
        
        # Get episode number
        episode_number = episode_data.get("episode_number")
        if episode_number is None:
            # Auto-increment from existing episodes
            existing_episodes = self.list_episodes(series=episode_data.get("series"))
            episode_number = max([ep.get("episode_number", 0) for ep in existing_episodes], default=0) + 1
        
        # Get or generate title
        title = episode_data.get("title")
        if not title:
            title = self._generate_title(
                theme=episode_data.get("theme"),
                series=episode_data.get("series"),
                episode_number=episode_number
            )
        
        # Initialize episode structure
        episode = {
            "episode_id": episode_id,
            "title": title,
            "series": episode_data.get("series", "Main Series"),
            "episode_number": episode_number,
            "theme": episode_data.get("theme"),
            "created_at": time.time(),
            "target_duration_minutes": episode_data.get("target_duration", 30),
            "status": "draft",
            "beats": self._calculate_beat_durations(episode_data.get("target_duration", 30)),
            "characters": [],
            "scenes": [],
            "script": None,
            "audio": None,
            "metadata": {}
        }
        
        # Add episode to memory
        self._add_episode_to_memory(episode)
        
        # Save episode structure
        self._save_episode(episode)
        
        return episode
    
    def _calculate_beat_durations(self, target_duration: int) -> List[Dict[str, Any]]:
        """Calculate durations for each beat based on target episode length.
        
        Args:
            target_duration: Target duration in minutes
        
        Returns:
            List of beats with calculated durations
        """
        total_seconds = target_duration * 60
        
        beats = []
        for beat in self.BEAT_SHEET:
            beat_duration = int(total_seconds * beat["duration_factor"])
            
            # Calculate timepoints for the beat
            start_percent = beat["percentage"] - (beat["duration_factor"] / 2)
            end_percent = beat["percentage"] + (beat["duration_factor"] / 2)
            
            start_time = int(total_seconds * start_percent)
            end_time = int(total_seconds * end_percent)
            
            beats.append({
                "name": beat["name"],
                "description": beat["description"],
                "duration_seconds": beat_duration,
                "start_time": start_time,
                "end_time": end_time
            })
        
        return beats
    
    def _generate_title(self, theme: Optional[str] = None, 
                      series: Optional[str] = None,
                      episode_number: Optional[int] = None) -> str:
        """Generate a title for the episode.
        
        Args:
            theme: Optional theme for the episode
            series: Optional series name
            episode_number: Optional episode number
        
        Returns:
            Generated title
        """
        try:
            # Prepare prompt for title generation
            prompt = "Generate a Star Trek-style podcast episode title"
            
            if theme:
                prompt += f" with the theme of '{theme}'"
            
            if series:
                prompt += f" for the series '{series}'"
            
            if episode_number:
                prompt += f", episode number {episode_number}"
            
            prompt += ". The title should be catchy, intriguing, and reference sci-fi concepts."
            
            # Query the AI
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a professional sci-fi writer specializing in Star Trek."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=50
            )
            
            # Extract and clean the title
            title = response.choices[0].message.content.strip()
            
            # Remove quotes if present
            if title.startswith('"') and title.endswith('"'):
                title = title[1:-1]
            
            # If title is too long, truncate with ellipsis
            if len(title) > 80:
                title = title[:77] + "..."
            
            return title
        
        except Exception as e:
            logger.error(f"Error generating title: {e}")
            
            # Fallback title
            fallback = f"Episode {episode_number or 'X'}"
            if theme:
                fallback += f": {theme}"
            
            return fallback
    
    def _add_episode_to_memory(self, episode: Dict[str, Any]) -> None:
        """Add episode structure to memory for future reference.
        
        Args:
            episode: Episode structure dictionary
        """
        try:
            # Convert to string for storage
            episode_str = json.dumps(episode)
            
            metadata = {
                "episode_id": episode["episode_id"],
                "title": episode["title"],
                "series": episode["series"],
                "episode_number": episode["episode_number"]
            }
            
            # Add to memory
            self.mem0_client.add_story_structure(
                structure_data=episode,
                episode_id=episode["episode_id"]
            )
            
            logger.debug(f"Added episode structure to memory: {episode['episode_id']}")
        
        except Exception as e:
            logger.error(f"Error adding episode to memory: {e}")
    
    def _save_episode(self, episode: Dict[str, Any]) -> None:
        """Save episode data to file.
        
        Args:
            episode: Episode data
        """
        episode_dir = self.episodes_dir / episode["episode_id"]
        episode_dir.mkdir(exist_ok=True)
        
        episode_file = episode_dir / "structure.json"
        
        try:
            with open(episode_file, 'w') as f:
                json.dump(episode, f, indent=2)
            
            logger.info(f"Saved episode structure to {episode_file}")
        
        except Exception as e:
            logger.error(f"Error saving episode structure: {e}")
    
    def generate_character_cast(self, episode_id: str) -> List[Dict[str, Any]]:
        """Generate a cast of characters for the episode.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            List of character dictionaries
        """
        # Load episode data
        episode = self.get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return []
        
        try:
            # Search for existing character archetypes in memory
            character_archetypes = self.mem0_client.search_memory(
                "Star Trek character archetypes",
                user_id="reference_materials",
                memory_type=self.mem0_client.REFERENCE_MATERIAL,
                limit=3
            )
            
            # Create context from episode data
            context = {
                "title": episode["title"],
                "theme": episode["theme"],
                "series": episode["series"]
            }
            
            # Create prompt for character generation
            existing_archetypes = "\n".join([arch["memory"] for arch in character_archetypes])
            
            prompt = f"""
            Generate a cast of 4-6 main characters for a Star Trek-style podcast episode.
            
            Episode Title: {context['title']}
            Series: {context['series']}
            Theme: {context['theme'] or 'Not specified'}
            
            Character Information Reference:
            {existing_archetypes}
            
            For each character, provide:
            1. Name
            2. Species
            3. Role on the ship/station
            4. Personality traits
            5. Key backstory elements
            6. Voice description (for voice acting)
            
            The cast should be diverse and should typically include:
            - A commanding officer
            - A science or technical specialist
            - A security or tactical officer
            - A medical or counselor role
            - 1-2 additional specialists or guest characters
            
            Format each character as a detailed profile that can be used for voice casting and character development.
            """
            
            # Query the AI
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a Star Trek universe expert and character creator."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=2000
            )
            
            # Extract character data from response
            character_text = response.choices[0].message.content
            
            # Parse characters from the text
            characters = self._parse_characters(character_text)
            
            # Update episode with characters
            episode["characters"] = characters
            self._save_episode(episode)
            
            return characters
        
        except Exception as e:
            logger.error(f"Error generating characters: {e}")
            return []
    
    def _parse_characters(self, character_text: str) -> List[Dict[str, Any]]:
        """Parse character descriptions from generated text.
        
        Args:
            character_text: Generated character descriptions
        
        Returns:
            List of parsed character dictionaries
        """
        characters = []
        
        # Split by double newlines or numbered sections
        sections = re.split(r'\n\s*\n|\n\d+\.\s+', character_text)
        
        for section in sections:
            if not section.strip():
                continue
            
            # Extract character data
            char = {}
            
            # Extract name - usually at the beginning of the section
            name_match = re.search(r'^[*#]*\s*(?:Name:?\s*)?([A-Za-z\s\'\"]+)', section, re.MULTILINE)
            if name_match:
                char["name"] = name_match.group(1).strip()
            else:
                # Try to find a capitalized name at the beginning
                name_match = re.search(r'^([A-Z][A-Za-z\'\s]+)(?:\n|\:)', section)
                if name_match:
                    char["name"] = name_match.group(1).strip()
                else:
                    # Skip if no name found
                    continue
            
            # Extract species
            species_match = re.search(r'Species:?\s*([A-Za-z\s\-]+)', section, re.IGNORECASE)
            if species_match:
                char["species"] = species_match.group(1).strip()
            
            # Extract role
            role_match = re.search(r'Role:?\s*([^\n]+)', section, re.IGNORECASE)
            if not role_match:
                role_match = re.search(r'Position:?\s*([^\n]+)', section, re.IGNORECASE)
            
            if role_match:
                char["role"] = role_match.group(1).strip()
            
            # Extract personality
            personality_match = re.search(r'Personality:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                         section, re.IGNORECASE)
            if personality_match:
                char["personality"] = personality_match.group(1).strip()
            
            # Extract backstory
            backstory_match = re.search(r'Backstory:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                       section, re.IGNORECASE)
            if not backstory_match:
                backstory_match = re.search(r'Background:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                           section, re.IGNORECASE)
            
            if backstory_match:
                char["backstory"] = backstory_match.group(1).strip()
            
            # Extract voice description
            voice_match = re.search(r'Voice:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                   section, re.IGNORECASE)
            if voice_match:
                char["voice_description"] = voice_match.group(1).strip()
            
            # Add character ID
            char["character_id"] = f"char_{uuid.uuid4().hex[:8]}"
            
            # Add to characters list if we have the minimum info
            if "name" in char and ("role" in char or "personality" in char):
                characters.append(char)
        
        return characters
    
    async def generate_scenes(self, episode_id: str) -> List[Dict[str, Any]]:
        """Generate scenes for an episode based on its structure and characters.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            List of scene dictionaries
        """
        # Load episode data
        episode = self.get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return []
        
        # Ensure characters exist
        if not episode.get("characters"):
            logger.warning(f"No characters found for episode {episode_id}. Generating characters first.")
            characters = self.generate_character_cast(episode_id)
            episode["characters"] = characters
        
        try:
            # Calculate number of scenes based on beats and target duration
            target_seconds = episode.get("target_duration_minutes", 30) * 60
            beats = episode.get("beats", [])
            
            # Aim for scenes averaging 2-3 minutes
            target_scenes = max(5, min(15, target_seconds // 150))
            
            # Determine number of scenes per beat based on duration ratios
            scenes_per_beat = {}
            for beat in beats:
                # Calculate scenes proportional to beat duration
                scene_count = max(1, int((beat["duration_seconds"] / target_seconds) * target_scenes))
                scenes_per_beat[beat["name"]] = scene_count
            
            # Search for relevant reference materials
            query = f"Star Trek {episode.get('theme', '')}"
            references = search_references(query, limit=3)
            
            # Extract reference text
            reference_text = "\n".join([ref.get("memory", "") for ref in references])
            
            # Prepare character information
            character_info = "\n".join([
                f"{char.get('name', 'Unknown')}: {char.get('species', 'Unknown')} - {char.get('role', 'Unknown')}"
                for char in episode.get("characters", [])
            ])
            
            # Generate scene outlines for each beat
            all_scenes = []
            tasks = []
            
            for beat in beats:
                num_scenes = scenes_per_beat.get(beat["name"], 1)
                for i in range(num_scenes):
                    task = self._generate_scene_outline(
                        episode=episode,
                        beat=beat,
                        scene_number=len(tasks) + 1,
                        total_scenes=sum(scenes_per_beat.values()),
                        reference_text=reference_text,
                        character_info=character_info
                    )
                    tasks.append(task)
            
            # Run scene generation concurrently
            scene_results = await asyncio.gather(*tasks)
            all_scenes = [scene for scene in scene_results if scene]
            
            # Update episode with scenes
            episode["scenes"] = all_scenes
            self._save_episode(episode)
            
            return all_scenes
        
        except Exception as e:
            logger.error(f"Error generating scenes: {e}")
            return []
    
    async def _generate_scene_outline(self, episode: Dict[str, Any], beat: Dict[str, Any],
                                    scene_number: int, total_scenes: int,
                                    reference_text: str, character_info: str) -> Dict[str, Any]:
        """Generate a single scene outline.
        
        Args:
            episode: Episode data
            beat: The story beat this scene belongs to
            scene_number: Number of this scene in the overall sequence
            total_scenes: Total number of scenes in the episode
            reference_text: Relevant reference material text
            character_info: Character information string
        
        Returns:
            Scene dictionary
        """
        try:
            # Calculate approximate duration for this scene
            target_seconds = episode.get("target_duration_minutes", 30) * 60
            scene_duration = int(target_seconds / total_scenes)
            
            # Determine approximate position
            progress = scene_number / total_scenes
            
            # Create prompt for scene generation
            prompt = f"""
            Create a detailed scene outline for a Star Trek-style podcast episode.
            
            EPISODE INFORMATION:
            Title: {episode.get('title')}
            Theme: {episode.get('theme', 'Not specified')}
            
            STORY BEAT: {beat.get('name')}
            Beat Description: {beat.get('description')}
            Scene Number: {scene_number} of {total_scenes}
            Progress: {progress*100:.0f}% through the story
            
            CHARACTERS:
            {character_info}
            
            REFERENCE MATERIAL:
            {reference_text[:500] if reference_text else "No specific reference material."}
            
            Create a detailed scene outline with:
            1. Setting (where the scene takes place)
            2. Character participants (who is in this scene)
            3. Plot (what happens in this scene)
            4. Dialogue suggestions (key lines or exchanges)
            5. Atmosphere/mood
            6. Sound effects/music suggestions
            
            The scene should be appropriate for the beat it's in, advancing the story in a compelling way.
            Target scene length: {scene_duration//60} minutes {scene_duration%60} seconds.
            """
            
            # Decide whether to use OpenAI or OpenRouter
            use_openrouter = self.using_openrouter and random.random() < 0.3  # 30% chance to use OpenRouter if available
            
            # Query the AI
            if use_openrouter:
                response = await self.async_openrouter_client.chat.completions.create(
                    model="anthropic/claude-3-opus",
                    messages=[
                        {"role": "system", "content": "You are an expert screenwriter specializing in science fiction and Star Trek."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )
            else:
                response = await self.async_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "You are an expert screenwriter specializing in science fiction and Star Trek."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )
            
            # Extract scene content
            scene_content = response.choices[0].message.content
            
            # Parse scene into structured format
            scene = self._parse_scene(scene_content)
            
            # Add scene metadata
            scene["scene_id"] = f"scene_{uuid.uuid4().hex[:8]}"
            scene["scene_number"] = scene_number
            scene["beat"] = beat["name"]
            scene["duration_seconds"] = scene_duration
            
            return scene
        
        except Exception as e:
            logger.error(f"Error generating scene outline: {e}")
            return {}
    
    def _parse_scene(self, scene_content: str) -> Dict[str, Any]:
        """Parse scene description from generated text.
        
        Args:
            scene_content: Generated scene description
        
        Returns:
            Parsed scene dictionary
        """
        scene = {}
        
        # Extract setting
        setting_match = re.search(r'Setting:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                scene_content, re.IGNORECASE)
        if setting_match:
            scene["setting"] = setting_match.group(1).strip()
        
        # Extract characters
        characters_match = re.search(r'Characters?:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                   scene_content, re.IGNORECASE)
        if characters_match:
            characters_text = characters_match.group(1).strip()
            scene["characters"] = [char.strip() for char in re.split(r',|\n', characters_text) if char.strip()]
        
        # Extract plot
        plot_match = re.search(r'Plot:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                             scene_content, re.IGNORECASE)
        if plot_match:
            scene["plot"] = plot_match.group(1).strip()
        
        # Extract dialogue
        dialogue_match = re.search(r'Dialogue:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                 scene_content, re.IGNORECASE)
        if dialogue_match:
            scene["dialogue"] = dialogue_match.group(1).strip()
        
        # Extract atmosphere
        atmosphere_match = re.search(r'Atmosphere:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                   scene_content, re.IGNORECASE)
        if not atmosphere_match:
            atmosphere_match = re.search(r'Mood:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                       scene_content, re.IGNORECASE)
        
        if atmosphere_match:
            scene["atmosphere"] = atmosphere_match.group(1).strip()
        
        # Extract sound effects
        sound_match = re.search(r'Sound Effects:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                              scene_content, re.IGNORECASE)
        if not sound_match:
            sound_match = re.search(r'Sound:?\s*([^\n]+(?:\n[^\n]+)*?)(?:\n\s*[A-Za-z]+:|\Z)', 
                                  scene_content, re.IGNORECASE)
        
        if sound_match:
            scene["sound_effects"] = sound_match.group(1).strip()
        
        # If we couldn't parse structured data, save the whole content
        if len(scene) <= 1:
            scene["content"] = scene_content
        
        return scene
    
    def generate_episode_script(self, episode_id: str) -> Dict[str, Any]:
        """Generate a complete script for an episode.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Dictionary with complete script
        """
        # Load episode data
        episode = self.get_episode(episode_id)
        if not episode:
            logger.error(f"Episode not found: {episode_id}")
            return {}
        
        # Ensure scenes exist
        if not episode.get("scenes"):
            logger.warning(f"No scenes found for episode {episode_id}. Generate scenes first.")
            return {}
        
        try:
            # Prepare script format
            script = {
                "title": episode.get("title"),
                "episode_id": episode_id,
                "created_at": time.time(),
                "scenes": []
            }
            
            # Process each scene to generate detailed script
            for scene in episode.get("scenes", []):
                # Generate script for this scene
                scene_script = self._generate_scene_script(episode, scene)
                script["scenes"].append(scene_script)
            
            # Update episode with script
            episode["script"] = script
            self._save_episode(episode)
            
            # Save script to separate file for easier editing
            self._save_script(episode_id, script)
            
            return script
        
        except Exception as e:
            logger.error(f"Error generating episode script: {e}")
            return {}
    
    def _generate_scene_script(self, episode: Dict[str, Any], scene: Dict[str, Any]) -> Dict[str, Any]:
        """Generate detailed script for a scene.
        
        Args:
            episode: Episode data
            scene: Scene outline
        
        Returns:
            Scene script dictionary
        """
        try:
            # Prepare character information
            character_profiles = {}
            for char in episode.get("characters", []):
                char_name = char.get("name", "")
                if char_name:
                    character_profiles[char_name] = {
                        "role": char.get("role", ""),
                        "personality": char.get("personality", ""),
                        "species": char.get("species", "")
                    }
            
            # Create prompt for scene script generation
            prompt = f"""
            Write a detailed script for a scene in a Star Trek-style podcast episode.
            
            EPISODE INFORMATION:
            Title: {episode.get('title')}
            
            SCENE INFORMATION:
            Setting: {scene.get('setting', 'Not specified')}
            Plot: {scene.get('plot', 'Not specified')}
            Atmosphere: {scene.get('atmosphere', 'Not specified')}
            Beat: {scene.get('beat', 'Not specified')}
            
            CHARACTERS IN SCENE:
            """
            
            # Add character information
            for char_name in scene.get("characters", []):
                profile = character_profiles.get(char_name, {})
                prompt += f"{char_name} - {profile.get('role', '')} - {profile.get('species', '')}\n"
            
            prompt += f"""
            DIALOGUE SUGGESTIONS:
            {scene.get('dialogue', 'No specific suggestions.')}
            
            Format the script with:
            1. Brief setting descriptions in [brackets]
            2. Character names in ALL CAPS, followed by their dialogue
            3. Sound effects in (parentheses)
            4. Narrator sections marked as NARRATOR
            
            Write a complete, detailed script for this scene with realistic dialogue and pacing.
            Approximate scene length: {scene.get('duration_seconds', 120)//60} minutes {scene.get('duration_seconds', 120)%60} seconds.
            """
            
            # Query the AI
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are an expert screenwriter specializing in science fiction audio dramas."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            # Extract script content
            script_content = response.choices[0].message.content
            
            # Parse script into lines
            script_lines = self._parse_script_lines(script_content)
            
            # Create scene script structure
            scene_script = {
                "scene_id": scene.get("scene_id", f"scene_{uuid.uuid4().hex[:8]}"),
                "scene_number": scene.get("scene_number", 0),
                "beat": scene.get("beat", ""),
                "setting": scene.get("setting", ""),
                "lines": script_lines
            }
            
            return scene_script
        
        except Exception as e:
            logger.error(f"Error generating scene script: {e}")
            return {
                "scene_id": scene.get("scene_id", f"scene_{uuid.uuid4().hex[:8]}"),
                "scene_number": scene.get("scene_number", 0),
                "beat": scene.get("beat", ""),
                "setting": scene.get("setting", ""),
                "lines": []
            }
    
    def _parse_script_lines(self, script_content: str) -> List[Dict[str, Any]]:
        """Parse script content into structured lines.
        
        Args:
            script_content: Generated script content
        
        Returns:
            List of line dictionaries
        """
        lines = []
        
        # Split script into paragraphs
        paragraphs = re.split(r'\n{2,}', script_content)
        
        scene_description = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # Check for scene description in brackets
            description_match = re.search(r'\[(.*?)\]', paragraph)
            if description_match:
                scene_description = description_match.group(1).strip()
                # Check if there's content after the description
                remaining = re.sub(r'\[(.*?)\]', '', paragraph).strip()
                if not remaining:
                    lines.append({
                        "type": "description",
                        "content": scene_description
                    })
                    continue
                paragraph = remaining
            
            # Check for sound effect in parentheses
            sound_effect_match = re.search(r'\((.*?)\)', paragraph)
            if sound_effect_match and len(sound_effect_match.group(0)) > len(paragraph) * 0.7:
                lines.append({
                    "type": "sound_effect",
                    "content": sound_effect_match.group(1).strip()
                })
                continue
            
            # Check for character dialogue
            dialogue_match = re.search(r'^([A-Z][A-Z\s]+)(?:\s*\(.*?\))?\:\s*(.*)', paragraph)
            if dialogue_match:
                character = dialogue_match.group(1).strip()
                dialogue = dialogue_match.group(2).strip()
                
                # Check if it's the narrator
                if character.upper() == "NARRATOR":
                    lines.append({
                        "type": "narration",
                        "content": dialogue
                    })
                else:
                    lines.append({
                        "type": "dialogue",
                        "character": character,
                        "content": dialogue
                    })
                continue
            
            # If no specific format matched, treat as description
            lines.append({
                "type": "description",
                "content": paragraph
            })
        
        return lines
    
    def _save_script(self, episode_id: str, script: Dict[str, Any]) -> None:
        """Save script to a separate file.
        
        Args:
            episode_id: Episode ID
            script: Script data
        """
        episode_dir = self.episodes_dir / episode_id
        episode_dir.mkdir(exist_ok=True)
        
        script_file = episode_dir / "script.json"
        
        try:
            with open(script_file, 'w') as f:
                json.dump(script, f, indent=2)
            
            logger.info(f"Saved script to {script_file}")
        
        except Exception as e:
            logger.error(f"Error saving script: {e}")
    
    def get_episode(self, episode_id: str) -> Optional[Dict[str, Any]]:
        """Get episode data by ID.
        
        Args:
            episode_id: ID of the episode
        
        Returns:
            Episode dictionary or None if not found
        """
        episode_dir = self.episodes_dir / episode_id
        structure_file = episode_dir / "structure.json"
        
        if not structure_file.exists():
            logger.error(f"Episode structure file not found: {structure_file}")
            return None
        
        try:
            with open(structure_file, 'r') as f:
                return json.load(f)
        
        except Exception as e:
            logger.error(f"Error reading episode structure: {e}")
            return None
    
    def list_episodes(self, series: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all episodes, optionally filtered by series.
        
        Args:
            series: Optional series name to filter by
        
        Returns:
            List of episode summary dictionaries
        """
        episodes = []
        
        for episode_dir in self.episodes_dir.iterdir():
            if not episode_dir.is_dir():
                continue
            
            structure_file = episode_dir / "structure.json"
            if not structure_file.exists():
                continue
            
            try:
                with open(structure_file, 'r') as f:
                    episode = json.load(f)
                
                # Apply series filter if specified
                if series and episode.get("series") != series:
                    continue
                
                # Create summary
                summary = {
                    "episode_id": episode.get("episode_id"),
                    "title": episode.get("title"),
                    "series": episode.get("series"),
                    "episode_number": episode.get("episode_number"),
                    "status": episode.get("status", "draft"),
                    "created_at": episode.get("created_at"),
                    "has_script": bool(episode.get("script")),
                    "has_audio": bool(episode.get("audio"))
                }
                
                episodes.append(summary)
            
            except Exception as e:
                logger.error(f"Error reading episode structure from {structure_file}: {e}")
        
        # Sort by series and episode number
        episodes.sort(key=lambda ep: (ep.get("series", ""), ep.get("episode_number", 0)))
        
        return episodes

# Singleton instance
_story_structure = None

def get_story_structure() -> StoryStructure:
    """Get the StoryStructure singleton instance."""
    global _story_structure
    
    if _story_structure is None:
        _story_structure = StoryStructure()
    
    return _story_structure

def generate_episode(episode_data: Dict[str, Any]) -> Dict[str, Any]:
    """Generate a new podcast episode structure.
    
    Args:
        episode_data: Data for the episode (title, theme, etc.)
    
    Returns:
        Dictionary with complete episode structure
    """
    story_structure = get_story_structure()
    return story_structure.generate_episode_structure(episode_data)

def generate_characters(episode_id: str) -> List[Dict[str, Any]]:
    """Generate characters for an episode.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        List of character dictionaries
    """
    story_structure = get_story_structure()
    return story_structure.generate_character_cast(episode_id)

async def generate_scenes(episode_id: str) -> List[Dict[str, Any]]:
    """Generate scenes for an episode.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        List of scene dictionaries
    """
    story_structure = get_story_structure()
    return await story_structure.generate_scenes(episode_id)

def generate_script(episode_id: str) -> Dict[str, Any]:
    """Generate script for an episode.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        Dictionary with complete script
    """
    story_structure = get_story_structure()
    return story_structure.generate_episode_script(episode_id)

def get_episode(episode_id: str) -> Optional[Dict[str, Any]]:
    """Get episode data by ID.
    
    Args:
        episode_id: ID of the episode
    
    Returns:
        Episode dictionary or None if not found
    """
    story_structure = get_story_structure()
    return story_structure.get_episode(episode_id)

def list_episodes(series: Optional[str] = None) -> List[Dict[str, Any]]:
    """List all episodes, optionally filtered by series.
    
    Args:
        series: Optional series name to filter by
    
    Returns:
        List of episode summary dictionaries
    """
    story_structure = get_story_structure()
    return story_structure.list_episodes(series)
</file>

<file path="tests/test_story_structure.py">
#!/usr/bin/env python
"""
Tests for story_structure module.

These tests verify the functionality of the story structure module,
including beat sheet calculations, episode generation, and script generation.
"""

import os
import sys
import json
import unittest
from unittest.mock import patch, MagicMock
from pathlib import Path
import tempfile
import shutil

# Add parent directory to path to import modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import module to test
from story_structure import (
    StoryStructure, 
    generate_episode, 
    get_episode,
    list_episodes
)

class TestStoryStructure(unittest.TestCase):
    """Test cases for story_structure module."""
    
    def setUp(self):
        """Set up test environment."""
        # Create temporary directory for episodes
        self.temp_dir = tempfile.mkdtemp()
        
        # Mock environment variables
        self.env_patcher = patch.dict('os.environ', {
            'OPENAI_API_KEY': 'fake_key',
            'MEM0_API_KEY': 'fake_key'
        })
        self.env_patcher.start()
        
        # Create patchers for external dependencies
        self.mem0_patcher = patch('story_structure.get_mem0_client')
        self.openai_patcher = patch('story_structure.OpenAI')
        self.async_openai_patcher = patch('story_structure.AsyncOpenAI')
        self.search_refs_patcher = patch('story_structure.search_references')
        
        # Start patchers
        self.mock_mem0 = self.mem0_patcher.start()
        self.mock_openai = self.openai_patcher.start()
        self.mock_async_openai = self.async_openai_patcher.start()
        self.mock_search_refs = self.search_refs_patcher.start()
        
        # Configure mocks
        self.mock_mem0_client = MagicMock()
        self.mock_mem0_client.add_story_structure.return_value = True
        self.mock_mem0_client.search_memory.return_value = []
        self.mock_mem0.return_value = self.mock_mem0_client
        
        self.mock_openai_client = MagicMock()
        self.mock_openai.return_value = self.mock_openai_client
        
        self.mock_async_openai_client = MagicMock()
        self.mock_async_openai.return_value = self.mock_async_openai_client
        
        # Configure chat completions response
        mock_response = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "Test Episode Title"
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        self.mock_openai_client.chat.completions.create.return_value = mock_response
        
        # Set up story structure with test directory
        self.story_structure = StoryStructure(episodes_dir=self.temp_dir)
    
    def tearDown(self):
        """Clean up after tests."""
        # Stop patchers
        self.env_patcher.stop()
        self.mem0_patcher.stop()
        self.openai_patcher.stop()
        self.async_openai_patcher.stop()
        self.search_refs_patcher.stop()
        
        # Remove temporary directory
        shutil.rmtree(self.temp_dir)
    
    def test_beat_sheet_structure(self):
        """Test that the beat sheet has the correct structure."""
        # The beat sheet should be a list of dictionaries
        self.assertIsInstance(self.story_structure.BEAT_SHEET, list)
        
        # Each beat should have the required keys
        required_keys = ['name', 'description', 'percentage', 'duration_factor']
        for beat in self.story_structure.BEAT_SHEET:
            for key in required_keys:
                self.assertIn(key, beat)
        
        # The beat sheet should cover the whole story
        total_duration = sum(beat['duration_factor'] for beat in self.story_structure.BEAT_SHEET)
        self.assertAlmostEqual(total_duration, 1.0, places=2)
    
    def test_calculate_beat_durations(self):
        """Test calculating beat durations based on target length."""
        # Test with 30 minute episode
        beats = self.story_structure._calculate_beat_durations(30)
        
        # Should have same number of beats as the beat sheet
        self.assertEqual(len(beats), len(self.story_structure.BEAT_SHEET))
        
        # Each beat should have duration_seconds, start_time, and end_time
        for beat in beats:
            self.assertIn('duration_seconds', beat)
            self.assertIn('start_time', beat)
            self.assertIn('end_time', beat)
            
            # Duration should be positive
            self.assertGreater(beat['duration_seconds'], 0)
            
            # End time should be after start time
            self.assertGreater(beat['end_time'], beat['start_time'])
        
        # Total duration should approximately match target
        total_seconds = sum(beat['duration_seconds'] for beat in beats)
        self.assertAlmostEqual(total_seconds, 30 * 60, delta=30)  # Allow 30 seconds margin
    
    def test_generate_episode_structure(self):
        """Test generating an episode structure."""
        # Test data
        episode_data = {
            'title': 'Test Episode',
            'theme': 'time travel',
            'series': 'Test Series',
            'target_duration': 30
        }
        
        # Generate episode
        episode = self.story_structure.generate_episode_structure(episode_data)
        
        # Check basic structure
        self.assertIn('episode_id', episode)
        self.assertEqual(episode['title'], 'Test Episode')
        self.assertEqual(episode['series'], 'Test Series')
        self.assertEqual(episode['theme'], 'time travel')
        self.assertEqual(episode['target_duration_minutes'], 30)
        self.assertEqual(episode['status'], 'draft')
        
        # Check beats
        self.assertIn('beats', episode)
        self.assertGreater(len(episode['beats']), 0)
        
        # Check file was saved
        episode_file = Path(self.temp_dir) / episode['episode_id'] / "structure.json"
        self.assertTrue(episode_file.exists())
        
        # Check mem0 was called
        self.mock_mem0_client.add_story_structure.assert_called_once()
    
    def test_generate_title(self):
        """Test generating an episode title."""
        # Mock response
        mock_response = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "The Temporal Paradox"
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        self.mock_openai_client.chat.completions.create.return_value = mock_response
        
        # Generate title
        title = self.story_structure._generate_title(
            theme="time travel",
            series="Test Series",
            episode_number=1
        )
        
        # Check title
        self.assertEqual(title, "The Temporal Paradox")
        
        # Check OpenAI was called
        self.mock_openai_client.chat.completions.create.assert_called_once()
        
        # Test fallback when API fails
        self.mock_openai_client.chat.completions.create.side_effect = Exception("API Error")
        
        # Generate title - should use fallback
        title = self.story_structure._generate_title(
            theme="time travel",
            series="Test Series",
            episode_number=1
        )
        
        # Check fallback title format
        self.assertEqual(title, "Episode 1: time travel")
    
    def test_save_and_get_episode(self):
        """Test saving and retrieving an episode."""
        # Create test episode
        episode = {
            'episode_id': 'test_episode',
            'title': 'Test Episode',
            'series': 'Test Series',
            'theme': 'test theme',
            'created_at': 1234567890,
            'target_duration_minutes': 30,
            'status': 'draft',
            'beats': [{'name': 'Opening Image', 'duration_seconds': 30}],
            'characters': [],
            'scenes': [],
            'script': None,
            'audio': None,
            'metadata': {}
        }
        
        # Save episode
        self.story_structure._save_episode(episode)
        
        # Retrieve episode
        retrieved = self.story_structure.get_episode('test_episode')
        
        # Check retrieved episode matches original
        self.assertEqual(retrieved['episode_id'], episode['episode_id'])
        self.assertEqual(retrieved['title'], episode['title'])
        self.assertEqual(retrieved['series'], episode['series'])
        
        # Test retrieving non-existent episode
        retrieved = self.story_structure.get_episode('nonexistent_episode')
        self.assertIsNone(retrieved)
    
    def test_list_episodes(self):
        """Test listing episodes."""
        # Create test episodes
        episodes = [
            {
                'episode_id': 'test_episode_1',
                'title': 'Test Episode 1',
                'series': 'Test Series A',
                'episode_number': 1,
                'created_at': 1234567890,
                'status': 'draft'
            },
            {
                'episode_id': 'test_episode_2',
                'title': 'Test Episode 2',
                'series': 'Test Series A',
                'episode_number': 2,
                'created_at': 1234567891,
                'status': 'complete'
            },
            {
                'episode_id': 'test_episode_3',
                'title': 'Test Episode 3',
                'series': 'Test Series B',
                'episode_number': 1,
                'created_at': 1234567892,
                'status': 'draft'
            }
        ]
        
        # Save episodes
        for episode in episodes:
            episode_dir = Path(self.temp_dir) / episode['episode_id']
            episode_dir.mkdir(exist_ok=True)
            
            with open(episode_dir / "structure.json", 'w') as f:
                json.dump(episode, f)
        
        # List all episodes
        results = self.story_structure.list_episodes()
        self.assertEqual(len(results), 3)
        
        # List episodes filtered by series
        results = self.story_structure.list_episodes(series="Test Series A")
        self.assertEqual(len(results), 2)
        
        results = self.story_structure.list_episodes(series="Test Series B")
        self.assertEqual(len(results), 1)
        
        # Test non-existent series
        results = self.story_structure.list_episodes(series="Nonexistent Series")
        self.assertEqual(len(results), 0)

if __name__ == '__main__':
    unittest.main()
</file>

<file path="voice_registry.py">
#!/usr/bin/env python
"""
Voice Registry Module for Stardock Podium.

This module manages the registration, retrieval, and mapping of character
voices using the ElevenLabs API. It maintains persistent voice metadata
to ensure character voice consistency across episodes.
"""

import os
import json
import logging
import time
import uuid
from pathlib import Path
from typing import Dict, List, Any, Optional, Union

# Try to import ElevenLabs
try:
    from elevenlabs import ElevenLabs, VoiceSettings
    from elevenlabs.client import ElevenLabs as ElevenLabsClient
except ImportError:
    logging.error("ElevenLabs not found. Please install it with: pip install elevenlabs")
    raise

# Local imports
from mem0_client import get_mem0_client

# Setup logging
logger = logging.getLogger(__name__)

class VoiceRegistry:
    """Manages voice registration and retrieval for characters."""
    
    def __init__(self, voices_dir: str = "voices"):
        """Initialize the voice registry.
        
        Args:
            voices_dir: Directory to store voice metadata
        """
        self.voices_dir = Path(voices_dir)
        self.voices_dir.mkdir(exist_ok=True)
        
        # Load ElevenLabs API key from environment
        self.api_key = os.environ.get("ELEVENLABS_API_KEY")
        if not self.api_key:
            logger.warning("ELEVENLABS_API_KEY not found in environment variables")
        
        # Initialize ElevenLabs client if API key is available
        if self.api_key:
            self.client = ElevenLabsClient(api_key=self.api_key)
            self.elevenlabs = ElevenLabs(api_key=self.api_key)
        else:
            self.client = None
            self.elevenlabs = None
        
        # Initialize mem0 client
        self.mem0_client = get_mem0_client()
        
        # Load registry
        self.registry = self._load_registry()
    
    def _load_registry(self) -> Dict[str, Dict[str, Any]]:
        """Load the voice registry from file.
        
        Returns:
            Dictionary of voice registry entries
        """
        registry_file = self.voices_dir / "registry.json"
        
        if registry_file.exists():
            try:
                with open(registry_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading voice registry: {e}")
        
        # Return empty registry if file doesn't exist or loading fails
        return {}
    
    def _save_registry(self) -> None:
        """Save the voice registry to file."""
        registry_file = self.voices_dir / "registry.json"
        
        try:
            with open(registry_file, 'w') as f:
                json.dump(self.registry, f, indent=2)
            
            logger.info("Voice registry saved successfully")
        except Exception as e:
            logger.error(f"Error saving voice registry: {e}")
    
    def register_voice(self, voice_data: Dict[str, Any]) -> Dict[str, Any]:
        """Register a new voice in the registry.
        
        Args:
            voice_data: Voice data including name, voice_id, and description
        
        Returns:
            Registered voice data with ID
        """
        # Ensure required fields are present
        required_fields = ['name', 'voice_id']
        for field in required_fields:
            if field not in voice_data:
                error_msg = f"Missing required field: {field}"
                logger.error(error_msg)
                return {"error": error_msg}
        
        # Check if voice exists with ElevenLabs if client is available
        if self.client and 'voice_id' in voice_data:
            try:
                # Check if voice ID exists
                voices = self.client.voices.get_all()
                voice_ids = [voice.voice_id for voice in voices.voices]
                
                if voice_data['voice_id'] not in voice_ids:
                    error_msg = f"Voice ID not found in ElevenLabs: {voice_data['voice_id']}"
                    logger.error(error_msg)
                    return {"error": error_msg}
            except Exception as e:
                logger.warning(f"Couldn't verify voice ID with ElevenLabs: {e}")
        
        # Generate a unique voice registry ID
        voice_registry_id = voice_data.get('voice_registry_id', f"voice_{uuid.uuid4().hex[:8]}")
        
        # Prepare voice entry
        voice_entry = {
            "voice_registry_id": voice_registry_id,
            "name": voice_data['name'],
            "voice_id": voice_data['voice_id'],
            "description": voice_data.get('description', ''),
            "character_bio": voice_data.get('character_bio', ''),
            "created_at": time.time(),
            "updated_at": time.time(),
            "settings": voice_data.get('settings', {})
        }
        
        # Add to registry
        self.registry[voice_registry_id] = voice_entry
        
        # Save registry
        self._save_registry()
        
        # Add to memory
        self._add_voice_to_memory(voice_entry)
        
        return voice_entry
    
    def get_voice(self, identifier: str) -> Optional[Dict[str, Any]]:
        """Get a voice by ID or character name.
        
        Args:
            identifier: Voice registry ID or character name
        
        Returns:
            Voice data if found, None otherwise
        """
        # Check if it's a direct ID match
        if identifier in self.registry:
            return self.registry[identifier]
        
        # Check for character name match
        for voice_id, voice in self.registry.items():
            if voice.get('name', '').lower() == identifier.lower():
                return voice
        
        # No match found
        return None
    
    def update_voice(self, voice_registry_id: str, updates: Dict[str, Any]) -> Dict[str, Any]:
        """Update an existing voice entry.
        
        Args:
            voice_registry_id: ID of the voice in the registry
            updates: Dictionary of fields to update
        
        Returns:
            Updated voice data
        """
        # Check if voice exists
        if voice_registry_id not in self.registry:
            error_msg = f"Voice not found in registry: {voice_registry_id}"
            logger.error(error_msg)
            return {"error": error_msg}
        
        # Create a copy of the current entry
        voice_entry = self.registry[voice_registry_id].copy()
        
        # Update fields
        for key, value in updates.items():
            if key != 'voice_registry_id':  # Don't allow changing the ID
                voice_entry[key] = value
        
        # Update timestamp
        voice_entry['updated_at'] = time.time()
        
        # Save to registry
        self.registry[voice_registry_id] = voice_entry
        self._save_registry()
        
        # Update in memory
        self._add_voice_to_memory(voice_entry)
        
        return voice_entry
    
    def delete_voice(self, voice_registry_id: str) -> Dict[str, Any]:
        """Delete a voice from the registry.
        
        Args:
            voice_registry_id: ID of the voice in the registry
        
        Returns:
            Status of the delete operation
        """
        # Check if voice exists
        if voice_registry_id not in self.registry:
            error_msg = f"Voice not found in registry: {voice_registry_id}"
            logger.error(error_msg)
            return {"error": error_msg, "success": False}
        
        # Remove from registry
        deleted_voice = self.registry.pop(voice_registry_id)
        
        # Save registry
        self._save_registry()
        
        return {"success": True, "deleted": deleted_voice}
    
    def list_voices(self) -> List[Dict[str, Any]]:
        """List all registered voices.
        
        Returns:
            List of all voice entries
        """
        return list(self.registry.values())
    
    def _add_voice_to_memory(self, voice_entry: Dict[str, Any]) -> None:
        """Add voice entry to memory for searchability.
        
        Args:
            voice_entry: Voice entry to add to memory
        """
        try:
            # Create memory-friendly representation
            voice_info = (
                f"Voice Registry Entry - Character: {voice_entry.get('name', '')}\n"
                f"Voice ID: {voice_entry.get('voice_id', '')}\n"
                f"Description: {voice_entry.get('description', '')}\n"
                f"Character Bio: {voice_entry.get('character_bio', '')}"
            )
            
            # Add to memory
            self.mem0_client.add_memory(
                content=voice_info,
                user_id="voice_registry",
                memory_type=self.mem0_client.VOICE_METADATA,
                metadata={
                    "voice_registry_id": voice_entry.get("voice_registry_id"),
                    "name": voice_entry.get("name"),
                    "voice_id": voice_entry.get("voice_id")
                }
            )
            
            logger.debug(f"Added voice to memory: {voice_entry.get('name')}")
        except Exception as e:
            logger.error(f"Error adding voice to memory: {e}")
    
    def find_voices_by_description(self, description: str, limit: int = 5) -> List[Dict[str, Any]]:
        """Find voices that match a description using semantic search.
        
        Args:
            description: Voice or character description
            limit: Maximum number of results
        
        Returns:
            List of matching voice entries
        """
        # Search memory for matching voices
        results = self.mem0_client.search_memory(
            query=description,
            user_id="voice_registry",
            memory_type=self.mem0_client.VOICE_METADATA,
            limit=limit
        )
        
        # Convert to voice entries
        voices = []
        for result in results:
            metadata = result.get('metadata', {})
            voice_id = metadata.get('voice_registry_id')
            
            if voice_id and voice_id in self.registry:
                voices.append(self.registry[voice_id])
        
        return voices
    
    def create_voice_from_description(self, name: str, description: str) -> Dict[str, Any]:
        """Create a new ElevenLabs voice from a text description.
        
        Args:
            name: Name of the character/voice
            description: Detailed voice description
        
        Returns:
            Created voice data
        """
        if not self.client:
            error_msg = "ElevenLabs client not initialized (no API key)"
            logger.error(error_msg)
            return {"error": error_msg}
        
        try:
            # Generate voice samples
            voice_preview = self.client.text_to_voice.create_previews(
                voice_description=description,
                text="Welcome to Stardock Podium, the Star Trek podcast generator. My name is " + name + ". I'll be your guide through this adventure.",
            )
            
            if not voice_preview.previews:
                error_msg = "No voice previews generated"
                logger.error(error_msg)
                return {"error": error_msg}
            
            # Create voice from preview
            voice = self.client.text_to_voice.create_voice_from_preview(
                voice_name=name,
                voice_description=description,
                generated_voice_id=voice_preview.previews[0].generated_voice_id,
            )
            
            # Register voice
            voice_data = {
                "name": name,
                "voice_id": voice.voice_id,
                "description": description,
                "settings": {
                    "stability": 0.5,
                    "similarity_boost": 0.75,
                    "style": 0.0,
                    "use_speaker_boost": True
                }
            }
            
            return self.register_voice(voice_data)
        
        except Exception as e:
            error_msg = f"Error creating voice: {e}"
            logger.error(error_msg)
            return {"error": error_msg}
    
    def generate_speech(self, text: str, voice_identifier: str, 
                      output_path: Optional[str] = None) -> bytes:
        """Generate speech audio for a given text and voice.
        
        Args:
            text: Text to convert to speech
            voice_identifier: Voice registry ID or character name
            output_path: Optional path to save the audio file
        
        Returns:
            Audio data as bytes
        """
        if not self.client:
            error_msg = "ElevenLabs client not initialized (no API key)"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
        
        # Get voice data
        voice_data = self.get_voice(voice_identifier)
        if not voice_data:
            error_msg = f"Voice not found: {voice_identifier}"
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        voice_id = voice_data['voice_id']
        
        # Extract voice settings
        settings = voice_data.get('settings', {})
        voice_settings = VoiceSettings(
            stability=settings.get('stability', 0.5),
            similarity_boost=settings.get('similarity_boost', 0.75),
            style=settings.get('style', 0.0),
            use_speaker_boost=settings.get('use_speaker_boost', True)
        )
        
        try:
            # Generate audio
            audio_data = self.elevenlabs.generate(
                text=text,
                voice=voice_id,
                model="eleven_monolingual_v1",
                voice_settings=voice_settings
            )
            
            # Save to file if output path is specified
            if output_path:
                with open(output_path, 'wb') as f:
                    f.write(audio_data)
                logger.info(f"Audio saved to {output_path}")
            
            return audio_data
        
        except Exception as e:
            error_msg = f"Error generating speech: {e}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)
    
    def check_voice_health(self, voice_registry_id: str) -> Dict[str, Any]:
        """Check if a voice is still available in ElevenLabs.
        
        Args:
            voice_registry_id: Voice registry ID
        
        Returns:
            Health status of the voice
        """
        if not self.client:
            return {"status": "unknown", "message": "ElevenLabs client not initialized"}
        
        # Get voice data
        voice_data = self.get_voice(voice_registry_id)
        if not voice_data:
            return {"status": "error", "message": f"Voice not found in registry: {voice_registry_id}"}
        
        voice_id = voice_data['voice_id']
        
        try:
            # Check if voice ID exists
            voices = self.client.voices.get_all()
            voice_ids = [voice.voice_id for voice in voices.voices]
            
            if voice_id in voice_ids:
                return {"status": "healthy", "message": "Voice available in ElevenLabs"}
            else:
                return {"status": "missing", "message": "Voice not found in ElevenLabs"}
        
        except Exception as e:
            return {"status": "error", "message": f"Error checking voice health: {e}"}
    
    def check_all_voices_health(self) -> Dict[str, Dict[str, Any]]:
        """Check health of all voices in the registry.
        
        Returns:
            Health status for all registered voices
        """
        health_status = {}
        
        for voice_id in self.registry:
            health_status[voice_id] = self.check_voice_health(voice_id)
        
        return health_status
    
    def map_characters_to_voices(self, characters: List[Dict[str, Any]]) -> Dict[str, str]:
        """Map character names to voice IDs based on descriptions.
        
        Args:
            characters: List of character dictionaries
        
        Returns:
            Dictionary mapping character names to voice registry IDs
        """
        character_voices = {}
        
        for character in characters:
            character_name = character.get('name', '')
            if not character_name:
                continue
            
            # Check if character already has a voice
            existing_voice = None
            for voice in self.registry.values():
                if voice.get('name', '').lower() == character_name.lower():
                    existing_voice = voice
                    break
            
            if existing_voice:
                character_voices[character_name] = existing_voice['voice_registry_id']
                continue
            
            # Try to find a matching voice based on description
            voice_description = character.get('voice_description', '')
            if voice_description:
                matching_voices = self.find_voices_by_description(voice_description, limit=1)
                
                if matching_voices:
                    # Use the best match
                    character_voices[character_name] = matching_voices[0]['voice_registry_id']
                    
                    # Update voice name to match character
                    self.update_voice(
                        matching_voices[0]['voice_registry_id'],
                        {'name': character_name}
                    )
                else:
                    # Create a new voice if possible
                    if self.client:
                        new_voice = self.create_voice_from_description(
                            name=character_name,
                            description=voice_description
                        )
                        
                        if 'voice_registry_id' in new_voice:
                            character_voices[character_name] = new_voice['voice_registry_id']
        
        return character_voices

# Singleton instance
_voice_registry = None

def get_voice_registry() -> VoiceRegistry:
    """Get the VoiceRegistry singleton instance."""
    global _voice_registry
    
    if _voice_registry is None:
        _voice_registry = VoiceRegistry()
    
    return _voice_registry

def register_voice(voice_data: Dict[str, Any]) -> Dict[str, Any]:
    """Register a new voice in the registry.
    
    Args:
        voice_data: Voice data including name, voice_id, and description
    
    Returns:
        Registered voice data with ID
    """
    registry = get_voice_registry()
    return registry.register_voice(voice_data)

def get_voice(identifier: str) -> Optional[Dict[str, Any]]:
    """Get a voice by ID or character name.
    
    Args:
        identifier: Voice registry ID or character name
    
    Returns:
        Voice data if found, None otherwise
    """
    registry = get_voice_registry()
    return registry.get_voice(identifier)

def list_voices() -> List[Dict[str, Any]]:
    """List all registered voices.
    
    Returns:
        List of all voice entries
    """
    registry = get_voice_registry()
    return registry.list_voices()

def generate_speech(text: str, voice_identifier: str, 
                   output_path: Optional[str] = None) -> bytes:
    """Generate speech audio for a given text and voice.
    
    Args:
        text: Text to convert to speech
        voice_identifier: Voice registry ID or character name
        output_path: Optional path to save the audio file
    
    Returns:
        Audio data as bytes
    """
    registry = get_voice_registry()
    return registry.generate_speech(text, voice_identifier, output_path)

def create_voice_from_description(name: str, description: str) -> Dict[str, Any]:
    """Create a new ElevenLabs voice from a text description.
    
    Args:
        name: Name of the character/voice
        description: Detailed voice description
    
    Returns:
        Created voice data
    """
    registry = get_voice_registry()
    return registry.create_voice_from_description(name, description)

def map_characters_to_voices(characters: List[Dict[str, Any]]) -> Dict[str, str]:
    """Map character names to voice IDs based on descriptions.
    
    Args:
        characters: List of character dictionaries
    
    Returns:
        Dictionary mapping character names to voice registry IDs
    """
    registry = get_voice_registry()
    return registry.map_characters_to_voices(characters)
</file>

</files>
